{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Markov and Chebyshev Inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 5.1 (Markov's Inequality)**.  Let $X$ be a non-negative random variable and suppose that $\\mathbb{E}(X)$ exists.  For any $t > 0$,\n",
    "\n",
    "$$ \\mathbb{P}(X > t) \\leq \\frac{\\mathbb{E}(X)}{t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}(X)\n",
    "=\\int_0^\\infty xf(x) dx\n",
    "=\\int_0^t xf(x) dx + \\int_t^\\infty xf(x) dx\n",
    "\\geq \\int_t^\\infty xf(x) dx\n",
    "\\geq t \\int_t^\\infty f(x) dx\n",
    "= t \\mathbb{P}(X > t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 5.2 (Chebyshev's Inequality)**.  Let $\\mu = \\mathbb{E}(X)$ and $\\sigma^2 = \\mathbb{V}(X)$.  Then,\n",
    "\n",
    "$$ \\mathbb{P}(|X - \\mu| \\geq t) \\leq \\frac{\\sigma^2}{t^2} \n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{P}(|Z| \\geq k) \\leq \\frac{1}{k^2} $$\n",
    "\n",
    "where $Z = (X - \\mu) / \\sigma$.  In particular, $\\mathbb{P}(|Z| > 2) \\leq 1/4$ and $\\mathbb{P}(|Z| > 3) \\leq 1/9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  We use Markov's inequality to conclude that\n",
    "\n",
    "$$ \\mathbb{P}(|X - \\mu| \\geq t) = \\mathbb{P}(|X - \\mu|^2 \\geq t^2) \\leq \\left(\\frac{\\mathbb{E}(X - \\mu)}{t}\\right)^2 = \\frac{\\sigma^2}{t^2} $$\n",
    "\n",
    "The second part follows by setting $t = k \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hoeffding's Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoeffding's inequality is similar in spirit to Markov's inequality but it is a sharper inequality.  We present the result here in two parts.  The proofs are in the technical appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 5.4**.  Let $Y_1, \\dots, Y_n$ be independent observations such that $\\mathbb{E}(Y_i) = 0$ and $a_i \\leq Y_i \\leq b_i$.  Let $\\epsilon > 0$.  Then, for any $t > 0$,\n",
    "\n",
    "$$ \\mathbb{P}\\left( \\sum_{i=1}^n Y_i \\geq \\epsilon \\right) \\leq e^{-t\\epsilon} \\prod_{i=1}^n e^{t^2(b_i - a_i)^2 / 8} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 5.5**.  Let $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$.  Then, for any $\\epsilon > 0$,\n",
    "\n",
    "$$ \\mathbb{P}(|\\overline{X}_n - p| > \\epsilon) \\leq 2e^{-2n\\epsilon^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoeffding's inequality gives us a simple way go create a **confidence interval** for a binomial parameter $p$.  We will discuss confidence intervals later but here is the basic idea.  Let $\\alpha > 0$ and let\n",
    "\n",
    "$$ \\epsilon_n = \\left\\{ \\frac{1}{2n} \\log \\left( \\frac{2}{\\alpha} \\right) \\right\\}^{1/2} $$\n",
    "\n",
    "By Hoeffding's inequality,\n",
    "\n",
    "$$ \\mathbb{P}(|\\overline{X}_n - p| > \\epsilon_n) \\leq 2e^{-2n\\epsilon_n^2} = \\alpha $$\n",
    "\n",
    "Let $C = (\\overline{X}_n - \\epsilon, \\overline{X}_n + \\epsilon)$. Then, $\\mathbb{P}(\\text{not } C \\in p) = \\mathbb{P}(|\\overline{X}_n - p| > \\epsilon) \\leq \\alpha$.  Hence, $\\mathbb{P}(p \\in C) \\geq 1 - \\alpha$, that is, the random interval $C$ traps the true parameter $p$ with probability $1 - \\alpha$; we call $C$ a $1 - \\alpha$ confidence interval.  More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cauchy-Schwartz and Jensen Inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains two inequalities on expected values that are often useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 5.7 (Cauchy-Schwartz Inequalities)**.  If $X$ and $Y$ have finite variances then\n",
    "\n",
    "$$ \\mathbb{E}|XY| \\leq \\sqrt{\\mathbb{E}\\left(X^2\\right) \\mathbb{E}\\left(Y^2\\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a function $g$ is **convex** if for each $x, y$ and each $\\alpha \\in [0, 1]$,\n",
    "\n",
    "$$ g(\\alpha x + (1 - \\alpha)y) \\leq \\alpha g(x) + (1 - \\alpha) g(y) $$\n",
    "\n",
    "If $g$ is twice differentiable, then the convexity reduces to checking that $g''(x) \\geq 0$ for all $x$.  It can be shown that if $g$ is convex then it lies above any line that touches $g$ at some point, called a tangent line.  A function $g$ is **concave** if $-g$ is convex.  Examples of convex functions are $g(x) = -x^2$ and $g(x) = \\log x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 5.8 (Jensen's Inequality)**.  If $g$ is convex then\n",
    "\n",
    "$$ \\mathbb{E}g(X) \\geq g(\\mathbb{E}X) $$\n",
    "\n",
    "If $g$ is concave then\n",
    "\n",
    "$$ \\mathbb{E}g(X) \\leq g(\\mathbb{E}X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  Let $L(x) = a + bx$ be a line, tangent to the $g(x)$ at the point $\\mathbb{E}(X)$.  Since $g$ is convex, it lies above the line $L(x)$. So,\n",
    "\n",
    "$$ \\mathbb{E}g(X) \\geq \\mathbb{E}L(X) = \\mathbb{E}(a + bX) = a + b\\mathbb{E}(X) = L(\\mathbb{E}(X)) = g(\\mathbb{E}(X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Jensen's inequality we see that $\\mathbb{E}X^2 \\geq (\\mathbb{E}X)^2$ and $\\mathbb{E}(1/X) \\geq 1 / \\mathbb{E}(X)$.  Since log is concave, $\\mathbb{E}(\\log X) \\leq \\log \\mathbb{E}(X)$. For example, suppose that $X \\sim N(3, 1)$.  Then $\\mathbb{E}(1 / X) \\geq 1/3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Technical Appendix: Proof of Hoeffding's Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the exact form of Taylor's theorem:  if $g$ is a smooth function, then there is a number $\\xi \\in (0, u)$ such that $g(u) = g(0) + u g'(0) + \\frac{u^2}{2}g''(\\xi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof of Theorem 5.4**.  For any $t > 0$, we have, from Markov's inequality, that\n",
    "\n",
    "$$ \\mathbb{P}\\left( \\sum_{i=1}^n Y_i \\geq \\epsilon \\right) = \\mathbb{P}\\left( t \\sum_{i=1}^n Y_i \\geq t \\epsilon \\right)\n",
    "= \\mathbb{P}\\left( e^{t \\sum_{i=1}^n Y_i} \\geq e^{t \\epsilon} \\right) \\leq e^{-t\\epsilon} \\mathbb{E}\\left( e^{t \\sum_{i=1}^n Y_i}\\right) = e^{-t\\epsilon} \\prod_i \\mathbb{E}\\left(e^{tY_i}\\right) $$\n",
    "\n",
    "Since $a_i \\leq Y_i \\leq b_i$, we can write $Y_i$ as a convex combination of $a_i$ and $b_i$, namely, $Y-i = \\alpha b_i + (1 - \\alpha) a_i$ where $\\alpha = (Y_i - a_i) / (b_i - a_i)$.  So, by the convexity of $e^{ty}$ we have\n",
    "\n",
    "$$ e^{tY_i} \\leq \\frac{Y_i - a_i}{b_i - a_i} e^{tb_i} + \\frac{b_i - Y_i}{b_i - a_i} e^{ta_i} $$\n",
    "\n",
    "Take expectations of both sides and use the fact that $\\mathbb{E}(Y_i) = 0$ to get\n",
    "\n",
    "$$ \\mathbb{E}e^{tY_i} \\leq - \\frac{a_i}{b_i - a_i} e^{tb_i} + \\frac{b_i}{b_i - a_i} e^{ta_i} = e^{g(u)} $$\n",
    "\n",
    "where $u = t(b_i - a_i)$, $g(u) = -\\gamma u + \\log (1 - \\gamma + \\gamma e^u)$ and $\\gamma = -a_i / (b_i - a_i)$.\n",
    "\n",
    "Note that $g(0) = g'(0) = 0$.  Also, $g''(u) \\leq 1/4$ for all $u > 0$.  By Taylor's theorem, there is a $\\xi \\in (0, u)$ such that\n",
    "\n",
    "$$ g(u) = g(0) + u g'(0) + \\frac{u^2}{2} g(\\xi) = \\frac{u^2}{2} g(\\xi) \\leq \\frac{u^2}{8} = \\frac{t^2(b_i - a_i)^2}{8} $$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$ \\mathbb{E}e^{tY_i} \\leq e^{g(u)} \\leq e^{t^2(b_i - a_i)^2/8} $$\n",
    "\n",
    "and the result follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof of Theorem 5.5**.  Let $Y_i = (1 / n)(X_i - p)$.  Then $\\mathbb{E}(Y_i) = 0$ and $a \\leq Y_i \\leq b$ where $a = -p/n$ and $b = (1 - p) / n$.  Also, $(b - a)^2 = 1/n^2$.  Applying Theorem 5.4 we get\n",
    "\n",
    "$$ \\mathbb{P}\\left(\\overline{X}_n - p > \\epsilon\\right) = \\mathbb{P}\\left( \\sum_i Y_i > \\epsilon \\right) \\leq e^{-t\\epsilon} e^{t^2/(8n)}$$\n",
    "\n",
    "The above holds for any $t > 0$.  In particular, take $t = 4n\\epsilon$ and we get $\\mathbb{P}\\left(\\overline{X}_n - p > \\epsilon\\right)  \\leq e^{-2n\\epsilon^2}$.  By a similar argument we can show that $\\mathbb{P}\\left(\\overline{X}_n - p < \\epsilon\\right)  \\leq e^{-2n\\epsilon^2}$.  Putting those together we get $\\mathbb{P}\\left(|\\overline{X}_n - p| >  \\epsilon\\right)  \\leq 2e^{-2n\\epsilon^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
