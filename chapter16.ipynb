{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Inference about Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter addresses two questions:\n",
    "\n",
    "1. How do we test if two random variables are independent?\n",
    "2. How do we estimate the strength of dependence between two random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall we write $Y \\text{ ⫫ } Z$ to mean that $Y$ and $Z$ are independent.\n",
    "\n",
    "When $Y$ and $Z$ are not independent, we say they are **dependent** or **associated** or **related**.\n",
    "\n",
    "Note that dependence does not mean causation:\n",
    "- Smoking is related to heart disease, and quitting smoking will reduce the chance of heart disease.\n",
    "- Owning a TV is related to lower starvation, but giving a starving person a TV does not make them not hungry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.1 Two Binary Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that both $Y$ and $Z$ are binary.  Consider a data set $(Y_1, Z_1), \\dots, (Y_n, Z_n)$.  Represent the data as a two-by-two table:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & Y = 0  & Y = 1 & \\\\\n",
    "\\hline\n",
    "Z = 0 & X_{00} & X_{01} & X_{0\\text{·}}\\\\\n",
    "Z = 1 & X_{10} & X_{11} & X_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & X_{\\text{·}0} & X_{\\text{·}1} & n = X_{\\text{··}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $X_{ij}$ represents the number of observations where $(Z_k, Y_k) = (i, j)$.  The dotted subscripts denote sums, e.g. $X_{i\\text{·}} = \\sum_j X_{ij}$.  Denote the corresponding probabilities by:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & Y = 0  & Y = 1 & \\\\\n",
    "\\hline\n",
    "Z = 0 & p_{00} & p_{01} & p_{0\\text{·}}\\\\\n",
    "Z = 1 & p_{10} & p_{11} & p_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & p_{\\text{·}0} & p_{\\text{·}1} & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $p_{ij} = \\mathbb{P}(Z = i, Y = j)$.  Let $X = (X_{00}, X_{01}, X_{10}, X_{11})$ denote the vector of counts.  Then $X \\sim \\text{Multinomial}(n, p)$ where $p = (p_{00}, p_{01}, p_{10}, p_{11})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **odds ratio** is defined to be\n",
    "\n",
    "$$ \\psi = \\frac{p_{00} p_{11}}{p_{01} p_{10}}$$\n",
    "\n",
    "The **log odds ratio** is defined to be\n",
    "\n",
    "$$ \\gamma = \\log \\psi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.2**.  The following statements are equivalent:\n",
    "\n",
    "1. $Y \\text{ ⫫ } Z$\n",
    "2. $\\psi = 1$\n",
    "3. $\\gamma = 0$\n",
    "4. For $i, j \\in \\{ 0, 1 \\}$, $p_{ij} = p_{i\\text{·}} p_{\\text{·}j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider testing\n",
    "\n",
    "$$\n",
    "H_0: Y \\text{ ⫫ } Z\n",
    "\\quad \\text{versus} \\quad\n",
    "H_1: \\text{not} (Y \\text{ ⫫ } Z)\n",
    "$$\n",
    "\n",
    "First consider the likelihood ratio test.  Under $H_1$, $X \\sim \\text{Multinomial}(n, p)$ and the MLE is $\\hat{p} = X / n$.  Under $H_0$, again $X \\sim \\text{Multinomial}(n, p)$ but $p$ is subjected to the constraint $p_{ij} = p_{i.} p_{.j}$.  This leads to the following test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.3 (Likelihood Ratio Test for Independence in a 2-by-2 table)**. \n",
    "Let\n",
    "\n",
    "$$ T = 2 \\sum_{i=0}^1 \\sum_{j=0}^1 X_{ij} \\log \\left( \\frac{X_{ij} X_{\\text{··}}}{X_{i\\text{·}} X_{\\text{·}j}} \\right)$$\n",
    "\n",
    "Under $H_0$, $T \\leadsto \\chi_1^2$.  Thus, an approximate level $\\alpha$ test is obtained by rejecting $H_0$ when $T > \\chi_{1, \\alpha}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.4 (Pearson's $\\chi^2$ test for Independence in a 2-by-2 table)**. Let\n",
    "\n",
    "$$ U = \\sum_{i=0}^1 \\sum_{j=0}^1 \\frac{(X_{ij} - E_{ij})^2}{E_{ij}} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ E_{ij} = \\frac{X_{i\\text{·}} X_{\\text{·}j}}{n}$$\n",
    "\n",
    "Under $H_0$, $U \\leadsto \\chi_1^2$.  Thus, an approximate level $\\alpha$ test is obtained by rejecting $H_0$ when $U > \\chi_{1, \\alpha}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the intuition for the Pearson test: Under $H_0$, $p_{ij} = p_{i\\text{·}} p_{\\text{·}j}$, so the MLE of $p_{ij}$ is $\\hat{p}_{ij} = \\hat{p}_{i\\text{·}} \\hat{p}_{\\text{·}j} = \\frac{X_{i\\text{·}}}{n} \\frac{X_{\\text{·}j}}{n}$.  Thus, the expected number of observations in the $(i, j)$ cell is $E_{ij} = n \\hat{p}_{ij} = \\frac{X_{i\\text{·}} X_{\\text{·}j}}{n}$.  The statistic $U$ compares the observed and expected counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.6**. The MLE's of $\\psi$ and $\\gamma$ are\n",
    "\n",
    "$$\n",
    "\\hat{\\psi} = \\frac{X_{00} X_{11}}{X_{01} X_{10}}\n",
    ", \\quad\n",
    "\\hat{\\gamma} = \\log \\hat{\\psi}\n",
    "$$\n",
    "\n",
    "The asymptotic standard errors (computed from the delta method) are\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\text{se}}(\\hat{\\psi}) &= \\sqrt{\\frac{1}{X_{00}} + \\frac{1}{X_{01}} + \\frac{1}{X_{10}} + \\frac{1}{X_{11}}}\\\\\n",
    "\\hat{\\text{se}}(\\hat{\\gamma}) &= \\hat{\\psi} \\hat{\\text{se}}(\\hat{\\gamma})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another test of independence is the Wald test for $\\gamma = 0$ given by $W = (\\hat{\\gamma} - 0) / \\hat{\\text{se}}(\\hat{\\gamma})$. \n",
    "\n",
    "A $1 - \\alpha$ confidence interval for $\\gamma$ is $\\hat{\\gamma} \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\gamma})$.\n",
    "\n",
    "A $1 - \\alpha$ confidence interval for $\\psi$ can be obtained in two ways.  First, we could use $\\hat{\\psi} \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\psi})$.  Second, since $\\psi = e^{\\gamma}$ we could use  $\\exp \\{\\hat{\\gamma} \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\gamma})\\}$.  This second method is usually more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.2 Interpreting the Odds Ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose event $A$ has probability $\\mathbb{P}(A)$.  The odds of $A$ are defined as\n",
    "\n",
    "$$\\text{odds}(A) = \\frac{\\mathbb{P}(A)}{1 - \\mathbb{P}(A)}$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\\mathbb{P}(A) = \\frac{\\text{odds}(A)}{1 + \\text{odds}(A)}$$\n",
    "\n",
    "Let $E$ be the event that someone is exposed to something (smoking, radiation, etc) and let $D$ be the event that they get a disease.  The odds of getting the disease given exposure are:\n",
    "\n",
    "$$\\text{odds}(D | E) = \\frac{\\mathbb{P}(D | E)}{1 - \\mathbb{P}(D | E)}$$\n",
    "\n",
    "and the odds of getting the disease given non-exposure are:\n",
    "\n",
    "$$\\text{odds}(D | E^c) = \\frac{\\mathbb{P}(D | E^c)}{1 - \\mathbb{P}(D | E^c)}$$\n",
    "\n",
    "The **odds ratio** is defined to be\n",
    "\n",
    "$$\\psi = \\frac{\\text{odds}(D | E)}{\\text{odds}(D | E^c)}$$\n",
    "\n",
    "If $\\psi = 1$ then the disease probability is the same for exposed and unexposed; this implies these events are independent.  Recall that the log-odds ratio is defined as $\\gamma = \\log \\psi$.  Independence corresponds to $\\gamma = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this table of probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & D^c    & D      & \\\\\n",
    "\\hline\n",
    "E^c   & p_{00} & p_{01} & p_{0\\text{·}}\\\\\n",
    "E     & p_{10} & p_{11} & p_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & p_{\\text{·}0} & p_{\\text{·}1} & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Denote the data by\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & D^c    & D      & \\\\\n",
    "\\hline\n",
    "E^c   & X_{00} & X_{01} & X_{0\\text{·}}\\\\\n",
    "E     & X_{10} & X_{11} & X_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & X_{\\text{·}0} & X_{\\text{·}1} & X_{\\text{··}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(D | E) = \\frac{p_{11}}{p_{10} + p_{11}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{P}(D | E^c) = \\frac{p_{01}}{p_{00} + p_{01}}\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\n",
    "\\text{odds}(D | E) = \\frac{p_{11}}{p_{10}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\text{odds}(D | E^c) = \\frac{p_{01}}{p_{00}}\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$ \\psi = \\frac{p_{11}p_{00}}{p_{01}p_{10}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the parameters, we have to consider how the data were collected.  There are three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Sampling**.  We draw a sample from the population and, for each sample, record their exposure and disease status.  In this case, $X = (X_{00}, X_{01}, X_{10}, X_{11}) \\sim \\text{Multinomial}(n, p)$.  We then estimates the probabilities in the table by $\\hat{p}_{ij$ = X_{ij} / n$ and\n",
    "\n",
    "$$ \\hat{\\psi} = \\frac{\\hat{p}_{11} \\hat{p}_{00}}{\\hat{p}_{01} \\hat{p}_{10}} = \\frac{X_{11} X_{00}}{X_{01} X_{10}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prospective Sampling (Cohort Sampling)**.  We get some exposed and unexposed people and count the number with disease within each group.  Thus,\n",
    "\n",
    "$$\n",
    "X_{01} \\sim \\text{Binomial}(X_{0\\text{·}}, \\mathbb{P}(D | E^c))\n",
    "\\quad \\text{and} \\quad\n",
    "X_{11} \\sim \\text{Binomial}(X_{1\\text{·}}, \\mathbb{P}(D | E))\n",
    "$$\n",
    "\n",
    "In this case we should write small letters $x_{0\\text{·}},  x_{1\\text{·}}$ instead of capital letters $ X_{0\\text{·}},  X_{1\\text{·}}$ since they are fixed and not random, but we'll keep using capital letters for notational simplicity.\n",
    "\n",
    "We can estimate $\\mathbb{P}(D | E))$ and $\\mathbb{P}(D | E^c)$ but we cannot estimate all probabilities in the table.  Still, we can estimate $\\psi$.  Now:\n",
    "\n",
    "$$\\hat{\\mathbb{P}}(D | E) = \\frac{X_{11}}{X_{1\\text{·}}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\mathbb{P}}(D | E^c) = \\frac{X_{01}}{X_{0\\text{·}}}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ \\hat{\\psi} = \\frac{X_{11} X_{00}}{X_{01} X_{10}}$$\n",
    "\n",
    "as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case-Control (Retrospective Sampling)**.  Here we get some diseased and non-diseased people and we observe how many are exposed.  This is much more efficient if the disease is rare.  Hence,\n",
    "\n",
    "$$\n",
    "X_{10} \\sim \\text{Binomial}(X_{\\text{·}0}, \\mathbb{P}(E | D^c))\n",
    "\\quad \\text{and} \\quad\n",
    "X_{11} \\sim \\text{Binomial}(X_{\\text{·}1}, \\mathbb{P}(E | D))\n",
    "$$\n",
    "\n",
    "From this data we can estimate $\\mathbb{P}(E | D)$ and $\\mathbb{P}(E | D^c)$.  Surprisingly, we can still estimate $\\psi$.  To understand why, note that\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(E | D) = \\frac{p_{11}}{p_{01} + p_{11}},\n",
    "\\quad 1 - \\mathbb{P}(E | D) = \\frac{p_{01}}{p_{01} + p_{11}},\n",
    "\\quad \\text{odds}(E | D) = \\frac{p_{11}}{p_{01}}\n",
    "$$\n",
    "\n",
    "By a similar argument,\n",
    "\n",
    "$$\\text{odds}(E | D^c) = \\frac{p_{10}}{p_{00}}$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\frac{\\text{odds}(E | D)}{\\text{odds}(E | D^c)} = \\frac{p_{11} p_{00}}{p_{01} p_{10}} = \\psi$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\hat{\\psi} = \\frac{X_{11} X_{00}}{X_{01} X_{10}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all three methods, the estimate of $\\psi$ turns out to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to try to estimate $\\mathbb{P}(D | E) - \\mathbb{P}(D | E^c)$.  In a case-control design, this quantity is not estimable.  To see this, we apply Bayes' theorem to get\n",
    "\n",
    "$$\\mathbb{P}(D | E) - \\mathbb{P}(D | E^c) = \\frac{\\mathbb{P}(E | D) \\mathbb{P}(D))}{\\mathbb{P}(E)} - \\frac{\\mathbb{P}(E^c | D) \\mathbb{P}(D)}{\\mathbb{P}(E^c)}$$\n",
    "\n",
    "Because of the way we obtained the data, $\\mathbb{P}(D)$ is not estimable from the data.\n",
    "\n",
    "However, we can estimate $\\xi = \\mathbb{P}(D | E) / \\mathbb{P}(D | E^c)$, which is called the **relative risk**, under the **rare disease assumption**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.9**.  Let $\\xi = \\mathbb{P}(D | E) / \\mathbb{P}(D | E^c)$.  Then\n",
    "\n",
    "$$ \\frac{\\psi}{\\xi} \\rightarrow 1$$\n",
    "\n",
    "as $\\mathbb{P}(D) \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, under the rare disease assumption, the relative risk is approximately the same as the odds ratio, which we can estimate.\n",
    "\n",
    "In a randomized experiment, we can interpret a strong association, that is $\\psi \\neq 1$, as a causal relationship.  In an observational (non-randomized) study, the association can be due to other unobserved **confounding** variables.  We'll discuss causation in more detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3 Two Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that $Y \\in \\{ 1, \\dots, I \\}$ and $Z \\in \\{ 1, \\dots, J \\}$ are two discrete variables.  The data can be represented by an $I \\times J$ table of contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c|cccccc|c} \n",
    "       & Y = 1  & Y = 2  & \\cdots & Y = j & \\cdots & Y = J   & \\\\\n",
    "\\hline\n",
    "Z = 1 & X_{11}  & X_{12} & \\cdots & X_{1j} & \\cdots & X_{1J} & X_{1\\text{·}}\\\\\n",
    "Z = 2 & X_{21}  & X_{22} & \\cdots & X_{2j} & \\cdots & X_{2J} & X_{2\\text{·}}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "Z = i & X_{i1}  & X_{i2} & \\cdots & X_{ij} & \\cdots & X_{iJ} & X_{i\\text{·}}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "Z = I & X_{I1}  & X_{I2} & \\cdots & X_{Ij} & \\cdots & X_{IJ} & X_{I\\text{·}}\\\\\n",
    " \\hline\n",
    "      & X_{\\text{·}1} & X_{\\text{·}1} & \\cdots & X_{\\text{·}j} & \\cdots & X_{\\text{·}J} & n\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider testing\n",
    "\n",
    "$$\n",
    "H_0: Y \\text{ ⫫ } Z\n",
    "\\quad \\text{versus} \\quad\n",
    "H_1: \\text{not } H_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.10**.  Let\n",
    "\n",
    "$$ T = 2 \\sum_{i=1}^I \\sum_{j=1}^J X_{ij} \\log \\left( \\frac{X_{ij} X_{\\text{··}}}{X_{i\\text{·}} X_{\\text{·}j}} \\right) $$\n",
    "\n",
    "The limiting distribution of $T$ under the null hypothesis of independence is $\\chi^2_\\nu$ where $\\nu = (I - 1)(J - 1)$.\n",
    "\n",
    "Pearson's $\\chi^2$ test statistic is\n",
    "\n",
    "$$ U = \\sum_{i=1}^I \\sum_{j=1}^J \\frac{(n_{ij} - E_{ij})^2}{E_{ij}}$$\n",
    "\n",
    "Asymptotically, under $H_0$, $U$ has a $\\chi^2_\\nu$ distribution where $\\nu = (I - 1)(J - 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of ways to quantify the strength of dependence between two discrete variables $Y$ and $Z$.  Most of them are not very intuitive.  The one we shall use is not standard but is more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define\n",
    "\n",
    "$$\\delta(Y, Z) = \\max_{A, B} \\Big|\\; \\mathbb{P}_{Y, Z}(Y \\in A, Z \\in B) - \\mathbb{P}_Y(Y \\in A) \\mathbb{P}_Z(Z \\in B) \\;\\Big|$$\n",
    "\n",
    "where the maximum is over all pairs of events $A$ and $B$.\n",
    "\n",
    "*(note: book PDF contains a typo on the formula above; this is the corrected version)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.12**.  Properties of $\\delta(Y, Z)$:\n",
    "\n",
    "1. $0 \\leq \\delta(Y, Z) \\leq 1$\n",
    "2. $\\delta(Y, Z) = 0$ if and only if $Y \\text{ ⫫ } Z$\n",
    "3. The following identity holds:\n",
    "\n",
    "$$ \\delta(X, Y) = \\frac{1}{2} \\sum_{i=1}^I \\sum_{j=1}^J \\Big|\\; p_{ij} - p_{i\\text{·}} p_{\\text{·}j} \\;\\Big|$$\n",
    "\n",
    "4.  The MLE of $\\delta$ is\n",
    "\n",
    "$$ \\hat{\\delta}(X, Y) = \\frac{1}{2} \\sum_{i=1}^I \\sum_{j=1}^J \\Big|\\; \\hat{p}_{ij} - \\hat{p}_{i\\text{·}} \\hat{p}_{\\text{·}j} \\;\\Big|$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{p}_{ij} = \\frac{X_{ij}}{n},\n",
    "\\quad \\hat{p}_{i\\text{·}} = \\frac{X_{i\\text{·}}}{n},\n",
    "\\quad \\hat{p}_{\\text{·}j} = \\frac{X_{\\text{·}j}}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of $\\delta$ is this: if one person makes probability statements assuming independence and another person makes probability statements without assuming independence, their probability statements may differ by as much as $\\delta$.  Here's a suggested scale for interpreting $\\delta$:\n",
    "\n",
    "| range        | interpretation             |\n",
    "|--------------|----------------------------|\n",
    "| 0 to 0.01    | negligible association     |\n",
    "| 0.01 to 0.05 | non-negligible association |\n",
    "| 0.05 to 0.1  | substantial association    |\n",
    "| over 0.1     | very strong association    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval for $\\delta$ can be obtained by bootstrapping.  The steps are:\n",
    "\n",
    "1. Draw $X^* \\sim \\text{Multinomial}(n, \\hat{p})$;\n",
    "2. Compute $\\hat{p}_ij, \\hat{p}_{i\\text{·}}, \\hat{p}_{\\text{·}j}$;\n",
    "3. Compute $\\delta^*$;\n",
    "4. Repeat.\n",
    "\n",
    "Now we use any of the methods we learned earlier for constructing bootstrap confidence intervals.  However, we should not use a Wald interval in this case.  The reason is that if $Y$ and $Z$ are independent then $\\delta = 0$ and we are on the boundary of the parameter space.  In this case, the Wald method is not valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.4 Two Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that $Y$ and $Z$ are both continuous.  If we assume that the joint distribution of $Y$ and $Z$ is bivariate Normal, then we measure the dependence between $Y$ and $Z$ by means of the correlation coefficient $\\rho$.  Tests, estimates, and confidence intervals for $\\rho$ in the Normal case are given in the previous chapter.  If we do not assume normality, then we need a nonparametric method for asserting dependence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the correlation is\n",
    "\n",
    "$$\\rho = \\frac{\\mathbb{E}((X_1 - \\mu_1)(X_2 - \\mu_2))}{\\sigma_1 \\sigma_2} $$\n",
    "\n",
    "A nonparametric estimator of $\\rho$ is the plug-in estimator which is:\n",
    "\n",
    "$$\\hat{\\rho} = \\frac{\\sum_{i=1}^n (X_{1i} - \\overline{X}_1)(X_{2i} - \\overline{X}_2)}{\\sqrt{\\sum_{i=1}^n (X_{1i} - \\overline{X}_1)^2 \\sum_{i=1}^n (X_{2i} - \\overline{X}_2)^2}} $$\n",
    "\n",
    "which is just the sample correlation.  A confidence interval can be constructed using the bootstrap.  A test for $\\rho = 0$ can be based on the Wald test using the bootstrap to estimate the standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plug-in approach is useful for large samples.  For small samples, we measure the correlation using the **Spearman rank correlation coefficient** $\\hat{\\rho}_S$.  We simply replace the data by their ranks, ranking each variable separately, then compute the correlation coefficient of the ranks.\n",
    "\n",
    "To test the null hypothesis that $\\rho_S = 0$, we need the distribution of $\\hat{\\rho}_S$ under the null hypothesis.  This can be obtained by simulation.  We fix the ranks of the first variable as $1, 2, \\dots, n$.  The ranks of the second variable are chosen at random from the set of $n!$ possible orderings, then we compute the correlation.  This is repeated many times, and the resulting distribution $\\mathbb{P}_0$ is the null distribution of $\\hat{\\rho}_S$. The p-value for the test is $\\mathbb{P}_0(|R| > |\\hat{\\rho}_S|)$ where $R$ is drawn from the null distribution $\\mathbb{P}_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.5 One Continuous Variable and One Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $Y \\in \\{ 1, \\dots, I \\}$ is discrete and $Z$ is continuous.  Let $F_i(z) = \\mathbb{P}(Z \\leq z | Y = i)$ denote the CDF of $Z$ conditional on $Y = i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.15**.  When $Y \\in \\{ 1, 2, \\dots, I \\}$ is discrete and $Z$ is continuous, then $Y \\text{ ⫫ } Z$ if and only if $F_1 = F_2 = \\dots = F_I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It follows that to test for independence, we need to test\n",
    "\n",
    "$$ \n",
    "H_0: F_1 = \\dots = F_I\n",
    "\\quad \\text{versus} \\quad\n",
    "H_1: \\text{not } H_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we consider the case where $I = 2$.  To test the null hypothesis that $F_1 = F_2$ we will use the **two sample Kolmogorov-Smirnov test**.  \n",
    "\n",
    "Let $n_k$ denote the number of observations for which $Y_i = k$.  Let\n",
    "\n",
    "$$\\hat{F}_k(z) = \\frac{1}{n_k} \\sum_{i=1}^n I(Z_i \\leq z) I(Y_i = k)$$\n",
    "\n",
    "denote the empirical distribution function of $Z$ given $Y = k$.  Define the test statistic\n",
    "\n",
    "$$ D = \\sup_x | \\hat{F}_1(x) - \\hat{F}_2(x) |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.16**.  Let\n",
    "\n",
    "$$ H(t) = 1 - 2 \\sum_{j=1}^\\infty (-1)^{j-1} e^{-2j^2t^2} $$\n",
    "\n",
    "Under the null hypothesis that $F_1 = F_2$,\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\mathbb{P} \\left( \\sqrt{\\frac{n_1 n_2}{n_1 + n_2}} D \\leq t \\right) = H(t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It follows from the theorem than an approximate level $\\alpha$ test is obtained by rejecting $H_0$ when\n",
    "\n",
    "$$ \\sqrt{\\frac{n_1 n_2}{n_1 + n_2}} D > H^{-1}(1 - \\alpha) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.1**.  Prove Theorem 16.2.\n",
    "\n",
    "The following statements are equivalent:\n",
    "\n",
    "1. $Y \\text{ ⫫ } Z$\n",
    "2. $\\psi = 1$\n",
    "3. $\\gamma = 0$\n",
    "4. For $i, j \\in \\{ 0, 1 \\}$, $p_{ij} = p_{i\\text{·}} p_{\\text{·}j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "$Y$ and $Z$ are independent if and only if $\\mathbb{P}(Y = j | Z = i) = \\mathbb{P}(Y = j) \\mathbb{P}(Z = i)$ for all $i$ and $j$, so (1) and (4) are equivalent.\n",
    "\n",
    "$\\gamma = \\log \\psi$, so (2) and (3) are equivalent.\n",
    "\n",
    "Finally,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\psi = 1\\\\\n",
    "&\\Leftrightarrow \\frac{\\text{odds}(Y | Z = 1)}{\\text{odds}(Y | Z = 0)} = 1 \\\\\n",
    "&\\Leftrightarrow \\text{odds}(Y | Z = 1) = \\text{odds}(Y | Z = 0) \\\\\n",
    "&\\Leftrightarrow \\mathbb{P}(Y | Z = 1) = \\mathbb{P}(Y | Z = 0) \\\\\n",
    "&\\Leftrightarrow Y \\text{ ⫫ } Z\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so (1) and (2) are equivalent.\n",
    "\n",
    "Therefore, all statements are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.2**.  Prove Theorem 16.3.\n",
    "\n",
    "Let\n",
    "\n",
    "$$ T = 2 \\sum_{i=0}^1 \\sum_{j=0}^1 X_{ij} \\log \\left( \\frac{X_{ij} X_{\\text{··}}}{X_{i\\text{·}} X_{\\text{·}j}} \\right)$$\n",
    "\n",
    "Under $H_0$, $T \\leadsto \\chi_1^2$.  Thus, an approximate level $\\alpha$ test is obtained by rejecting $H_0$ when $T > \\chi_{1, \\alpha}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**. Let $E_{ij} = \\frac{X_{i\\text{·}} X_{\\text{·}j}}{X_\\text{··}}$.  We can rewrite the test statistic as\n",
    "\n",
    "$$ T = 2 \\sum_{i, j} X_{ij} \\log \\frac{X_{ij}}{E_{ij}} $$\n",
    "\n",
    "The interpretation of $E_{ij}$ is that it is the expected count in cell $(i, j)$ under $H_0$:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathbb{E}_{H_0}(Y = i, Z = j) &= n \\mathbb{P}_{H_0}(Y = i, Z = j) \\\\\n",
    "&= n \\mathbb{P}(Y = i) \\mathbb{P}(Z = j) \\\\\n",
    "&= n \\hat{p}_{i\\text{·}} \\hat{p}_{\\text{·}j} \\\\\n",
    "&= X_{\\text{··}} \\frac{X_{i\\text{·}}}{X_{\\text{··}}} \\frac{X_{\\text{·}j}}{X_{\\text{··}}} \\\\\n",
    "&= E_{ij}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, the result will follow by applying the log-likelihood test between $H_0$ and $H_1$.  The test statistic is:\n",
    "\n",
    "$$ 2 \\log \\frac{\\mathcal{L}(\\tilde{p} | X)}{\\mathcal{L}(\\hat{p} | X)} \n",
    "= 2 \\log \\frac{\\prod_{i, j} \\tilde{p}_{ij}^{X_{ij}}}{\\prod_{i, j} \\hat{p}_{ij}^{X_{ij}}}$$\n",
    "\n",
    "where $\\tilde{p}$ is the MLE under $H_1$ and $\\hat{p}$ is the MLE under $H_0$.  But we have:\n",
    "\n",
    "$$\n",
    "\\tilde{p}_{ij} = \\frac{X_{ij}}{n}\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{p}_{ij} = \\frac{E_{ij}}{n}\n",
    "$$\n",
    "\n",
    "Substituting the MLEs on the log-likelihood ratio, we get\n",
    "\n",
    "$$ 2 \\log \\frac{\\mathcal{L}(\\tilde{p} | X)}{\\mathcal{L}(\\hat{p} | X)} \n",
    "= 2 \\log \\prod_{i, j} \\left( \\frac{X_{ij}}{E_{ij}} \\right)^{X_{ij}}\n",
    "= 2 \\sum_{i, j} X_{ij} \\log \\frac{X_{ij}}{E_{ij}}\n",
    "$$\n",
    "\n",
    "which is the desired result.\n",
    "\n",
    "*Reference: \"G-test.\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 22 July 2004. Web. 17 Mar. 2020, en.wikipedia.org/w/index.php?title=G-test&oldid=914538756*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.3**.  Prove Theorem 16.9.\n",
    "\n",
    "Let $\\xi = \\mathbb{P}(D | E) / \\mathbb{P}(D | E^c)$.  Then\n",
    "\n",
    "$$ \\frac{\\psi}{\\xi} \\rightarrow 1$$\n",
    "\n",
    "as $\\mathbb{P}(D) \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\mathbb{P}(D | E) = \\frac{\\mathbb{P}(D \\land E)}{\\mathbb{P}(E)} = \\frac{p_{11}}{p_{01} + p_{11}}\n",
    "\\quad\n",
    "\\mathbb{P}(D | E^c) = \\frac{\\mathbb{P}(D \\land E^c)}{\\mathbb{P}(E^c)} = \\frac{p_{10}}{p_{00} + p_{10}}\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\n",
    "\\xi = \\frac{\\mathbb{P}(D | E)}{\\mathbb{P}(D | E^c)}\n",
    "= \\frac{p_{11} (p_{00} + p_{10})}{p_{10} (p_{01} + p_{11})}\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$ \\psi = \\frac{p_{11}p_{00}}{p_{01}p_{10}}$$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$ \\frac{\\psi}{\\xi} = \\frac{p_{11}p_{00}}{p_{01}p_{10}} \\frac{p_{10} (p_{01} + p_{11})}{p_{11} (p_{00} + p_{10})} \n",
    "= \\frac{p_{00}}{p_{01}} \\frac{p_{\\text{·}1}}{p_{\\text{·}0}}\n",
    "$$ \n",
    "\n",
    "As $\\mathbb{P}(D) = p_{10} + p_{11} \\rightarrow 0$ and the probabilites are non-negative, $p_{10} \\rightarrow 0$, $p_{11} \\rightarrow 0$, so\n",
    "\n",
    "$$ \\frac{\\psi}{\\xi} \\rightarrow  \\frac{p_{00}}{p_{01}} \\frac{p_{01}}{p_{00}} = 1 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.4**.  Prove equation (16.14).\n",
    "\n",
    "$$ \\delta(X, Y) = \\frac{1}{2} \\sum_{i=1}^I \\sum_{j=1}^J \\Big|\\; p_{ij} - p_{i\\text{·}} p_{\\text{·}j} \\;\\Big|$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.5**.  The New York Times (January 8, 2003, page A12) reported the following data on death sentencing and rance, from a study in Maryland:\n",
    "\n",
    "|  | Death Sentence | No Death Sentence |\n",
    "|--|----------------|-------------------|\n",
    "| Black Victim | 14 | 641 |\n",
    "| White Victim | 62 | 594 |\n",
    "\n",
    "Analyse the data using the tools from this Chapter.  Interpret the results.  Explain why, based only on this information, you can't make causal conclusions.  (The authors of the study did use much more information in their full report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The statistic for the log-likelihood test for independence is is:\n",
    "\n",
    "$$ T = 2 \\sum_{i, j} X_{ij} \\log \\frac{X_{ij} X_{\\text{··}}}{X_{i\\text{·}} X_{\\text{·}j}} $$\n",
    "\n",
    "The statistic for Pearson's $\\chi^2$ test is:\n",
    "\n",
    "$$ U = \\sum_{i, j} \\frac{(X_{ij} - E_{ij})^2}{E_{ij}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_ij = np.array([[14, 641], [62, 594]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idot = X_ij.sum(axis = 1).reshape(2, 1)\n",
    "X_dotj = X_ij.sum(axis = 0).reshape(1, 2)\n",
    "n = X_ij.sum()\n",
    "E_ij = X_idot @ X_dotj / n\n",
    "\n",
    "# Log-likelihood test statistic\n",
    "T = 2 * (X_ij * np.log(X_ij / E_ij)).sum()\n",
    "\n",
    "# Pearson's \\chi^2 test statistic\n",
    "U = ((X_ij - E_ij)**2 / E_ij).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T test statistic: \t\t34.534\n",
      "Pearson chi^2 statistic: \t32.104\n",
      "p-value log likelihood:\t\t 0.000\n",
      "p-value Pearson: \t\t 0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "p_value_log_likelihood = 1 - chi2.cdf(T, 1)\n",
    "p_value_pearson = 1 - chi2.cdf(U, 1)\n",
    "\n",
    "print('T test statistic: \\t\\t%.3f'% T)\n",
    "print('Pearson chi^2 statistic: \\t%.3f'% U)\n",
    "\n",
    "print('p-value log likelihood:\\t\\t %.3f' % p_value_log_likelihood)\n",
    "print('p-value Pearson: \\t\\t %.3f' % p_value_pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tests indicate that these variables are correlated.  Note that this is not sufficient information, in itself, to imply causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta(X, Y): 0.037\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "p = X_ij / n\n",
    "p_idot = X_idot / n\n",
    "p_dotj = X_dotj / n\n",
    "\n",
    "delta = sum([abs(p[i, j] - p_idot[i, :] * p_dotj[:, j]) for (i, j) in product(range(2), range(2))]) / 2\n",
    "\n",
    "print('delta(X, Y): %.3f' % delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta(X, Y)$ is between 0.01 and 0.05, suggesting a non-negligible association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.6**.  Analyse the data on the variables Age and Financial Status from:\n",
    "\n",
    "http://lib.stat.cmu.edu/DASL/Datafiles/montanadat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/montana.csv')\n",
    "\n",
    "# Select wanted columns and remove missing data\n",
    "data = data[['AGE', 'FIN']].replace('*', np.nan).dropna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 16, 34],\n",
       "       [17, 23, 26],\n",
       "       [22, 37, 11]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count occurrences in each cell\n",
    "X_ij = np.zeros((3, 3)).astype(int)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "     X_ij[row['AGE'] - 1, row['FIN'] - 1] += 1\n",
    "           \n",
    "X_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idot = X_ij.sum(axis = 1).reshape(3, 1)\n",
    "X_dotj = X_ij.sum(axis = 0).reshape(1, 3)\n",
    "n = X_ij.sum()\n",
    "E_ij = X_idot @ X_dotj / n\n",
    "\n",
    "# Log-likelihood test statistic\n",
    "T = 2 * (X_ij * np.log(X_ij / E_ij)).sum()\n",
    "\n",
    "# Pearson's \\chi^2 test statistic\n",
    "U = ((X_ij - E_ij)**2 / E_ij).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T test statistic: \t\t22.064\n",
      "Pearson chi^2 statistic: \t20.679\n",
      "p-value log likelihood:\t\t 0.000\n",
      "p-value Pearson: \t\t 0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "p_value_log_likelihood = 1 - chi2.cdf(T, 4)\n",
    "p_value_pearson = 1 - chi2.cdf(U, 4)\n",
    "\n",
    "print('T test statistic: \\t\\t%.3f'% T)\n",
    "print('Pearson chi^2 statistic: \\t%.3f'% U)\n",
    "\n",
    "print('p-value log likelihood:\\t\\t %.3f' % p_value_log_likelihood)\n",
    "print('p-value Pearson: \\t\\t %.3f' % p_value_pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tests indicate correlation between the two sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta(X, Y): 0.128\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "p = X_ij / n\n",
    "p_idot = X_idot / n\n",
    "p_dotj = X_dotj / n\n",
    "\n",
    "delta = sum([abs(p[i, j] - p_idot[i, :] * p_dotj[:, j]) for (i, j) in product(range(3), range(3))]) / 2\n",
    "\n",
    "print('delta(X, Y): %.3f' % delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta(X, Y) > 0.1$ indicates a strong correlation between these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.7**.  Estimate the correlation between temperature and latitude using the data from\n",
    "\n",
    "http://lib.stat.cmu.edu/DASL/Datafiles/USTemperatures.html\n",
    "\n",
    "Use the correlation coefficient and Spearman rank correlation.  Provide estimates, tests, and confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/USTemperatures.txt', sep='\\t')\n",
    "filtered_data = data[['lat', 'JanTF']].dropna()\n",
    "X, Y = filtered_data['lat'], filtered_data['JanTF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def get_pearson_correlation(xx, yy):\n",
    "    mu_1 = xx.mean()\n",
    "    mu_2 = yy.mean()\n",
    "    sigma2_1 = ((xx - mu_1)**2).mean()\n",
    "    sigma2_2 = ((yy - mu_2)**2).mean()\n",
    "    \n",
    "    return ((xx - mu_1) * (yy - mu_2)).mean() / math.sqrt(sigma2_1 * sigma2_2)\n",
    "\n",
    "def get_spearman_correlation(xx, yy):\n",
    "    return get_pearson_correlation(rankdata(xx), rankdata(yy))\n",
    "\n",
    "def bootstrap_correlation(xx, yy, corr_fun, B=10000, alpha=0.05):\n",
    "    n = len(xx)\n",
    "    assert len(yy) == n, 'Sequences must have same length'\n",
    "    \n",
    "    t_boot = np.empty(B)\n",
    "    for i in tqdm_notebook(range(B)):\n",
    "        indexes = np.random.randint(0, n, size=n)\n",
    "        xx_selected, yy_selected = xx.iloc[indexes], yy.iloc[indexes]\n",
    "        t_boot[i] = corr_fun(xx_selected, yy_selected)\n",
    "        \n",
    "    confidence_interval = (np.quantile(t_boot, alpha / 2), np.quantile(t_boot, 1 - alpha / 2))\n",
    "    return confidence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464bbe3068984b69b17d5de87caee5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe50206afa6649d48db7f9f8befac16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pearson_rho = get_pearson_correlation(X, Y)\n",
    "pearson_rho_confidence = bootstrap_correlation(X, Y, corr_fun = get_pearson_correlation, B = 10000, alpha=0.05)\n",
    "\n",
    "spearman_rho = get_spearman_correlation(X, Y)\n",
    "spearman_rho_confidence = bootstrap_correlation(X, Y, corr_fun = get_spearman_correlation, B = 10000, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson confidence: \t -0.857\n",
      "95% confidence interval: -0.963, -0.678\n",
      "\n",
      "Spearman confidence: \t -0.834\n",
      "95% confidence interval: -0.953, -0.654\n"
     ]
    }
   ],
   "source": [
    "print('Pearson confidence: \\t %.3f' % pearson_rho)\n",
    "print('95%% confidence interval: %.3f, %.3f' % pearson_rho_confidence)\n",
    "print()\n",
    "print('Spearman confidence: \\t %.3f' % spearman_rho)\n",
    "print('95%% confidence interval: %.3f, %.3f' % spearman_rho_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16.7.8**.  Test whether calcium intake and drop in blood pressure are associated.  Use the data in\n",
    "\n",
    "http://lib.stat.cmu.edu/DASL/Datafiles/Calcium.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**. TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
