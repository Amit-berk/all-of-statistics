{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Inference about Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter addresses two questions:\n",
    "\n",
    "1. How do we test if two random variables are independent?\n",
    "2. How do we estimate the strength of dependence between two random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall we write $Y \\text{ ⫫ } Z$ to mean that $Y$ and $Z$ are independent.\n",
    "\n",
    "When $Y$ and $Z$ are not independent, we say they are **dependent** or **associated** or **related**.\n",
    "\n",
    "Note that dependence does not mean causation:\n",
    "- Smoking is related to heart disease, and quitting smoking will reduce the chance of heart disease.\n",
    "- Owning a TV is related to lower starvation, but giving a starving person a TV does not make them not hungry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.1 Two Binary Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that both $Y$ and $Z$ are binary.  Consider a data set $(Y_1, Z_1), \\dots, (Y_n, Z_n)$.  Represent the data as a two-by-two table:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & Y = 0  & Y = 1 & \\\\\n",
    "\\hline\n",
    "Z = 0 & X_{00} & X_{01} & X_{0\\text{·}}\\\\\n",
    "Z = 1 & X_{10} & X_{11} & X_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & X_{\\text{·}0} & X_{\\text{·}1} & n = X_{\\text{··}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $X_{ij}$ represents the number of observations where $(Z_k, Y_k) = (i, j)$.  The dotted subscripts denote sums, e.g. $X_{i\\text{·}} = \\sum_j X_{ij}$.  Denote the corresponding probabilities by:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & Y = 0  & Y = 1 & \\\\\n",
    "\\hline\n",
    "Z = 0 & p_{00} & p_{01} & p_{0\\text{·}}\\\\\n",
    "Z = 1 & p_{10} & p_{11} & p_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & p_{\\text{·}0} & p_{\\text{·}1} & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $p_{ij} = \\mathbb{P}(Z = i, Y = j)$.  Let $X = (X_{00}, X_{01}, X_{10}, X_{11})$ denote the vector of counts.  Then $X \\sim \\text{Multinomial}(n, p)$ where $p = (p_{00}, p_{01}, p_{10}, p_{11})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **odds ratio** is defined to be\n",
    "\n",
    "$$ \\psi = \\frac{p_{00} p_{11}}{p_{01} p_{10}}$$\n",
    "\n",
    "The **log odds ratio** is defined to be\n",
    "\n",
    "$$ \\gamma = \\log \\psi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.2**.  The following statements are equivalent:\n",
    "\n",
    "1. $Y \\text{ ⫫ } Z$\n",
    "2. $\\psi = 1$\n",
    "3. $\\gamma = 0$\n",
    "4. For $i, j \\in \\{ 0, 1 \\}$, $p_{ij} = p_{i\\text{·}} p_{\\text{·}j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider testing\n",
    "\n",
    "$$\n",
    "H_0: Y \\text{ ⫫ } Z\n",
    "\\quad \\text{versus} \\quad\n",
    "H_1: \\text{not} (Y \\text{ ⫫ } Z)\n",
    "$$\n",
    "\n",
    "First consider the likelihood ratio test.  Under $H_1$, $X \\sim \\text{Multinomial}(n, p)$ and the MLE is $\\hat{p} = X / n$.  Under $H_0$, again $X \\sim \\text{Multinomial}(n, p)$ but $p$ is subjected to the constraint $p_{ij} = p_{i.} p_{.j}$.  This leads to the following test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.3 (Likelihood Ratio Test for Independence in a 2-by-2 table)**. \n",
    "Let\n",
    "\n",
    "$$ T = 2 \\sum_{i=0}^1 \\sum_{j=0}^1 X_{ij} \\log \\left( \\frac{X_{ij} X_{\\text{··}}}{X_{i\\text{·}} X_{\\text{·}j}} \\right)$$\n",
    "\n",
    "Under $H_0$, $T \\leadsto \\chi_1^2$.  Thus, an approximate level $\\alpha$ test is obtained by rejecting $H_0$ when $T > \\chi_{1, \\alpha}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.4 (Pearson's $\\chi^2$ test for Independence in a 2-by-2 table)**. Let\n",
    "\n",
    "$$ U = \\sum_{i=0}^1 \\sum_{j=0}^1 \\frac{(X_{ij} - E_{ij})^2}{E_{ij}} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ E_{ij} = \\frac{X_{i\\text{·}} X_{\\text{·}j}}{n}$$\n",
    "\n",
    "Under $H_0$, $U \\leadsto \\chi_1^2$.  Thus, an approximate level $\\alpha$ test is obtained by rejecting $H_0$ when $U > \\chi_{1, \\alpha}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the intuition for the Pearson test: Under $H_0$, $p_{ij} = p_{i\\text{·}} p_{\\text{·}j}$, so the MLE of $p_{ij}$ is $\\hat{p}_{ij} = \\hat{p}_{i\\text{·}} \\hat{p}_{\\text{·}j} = \\frac{X_{i\\text{·}}}{n} \\frac{X_{\\text{·}j}}{n}$.  Thus, the expected number of observations in the $(i, j)$ cell is $E_{ij} = n \\hat{p}_{ij} = \\frac{X_{i\\text{·}} X_{\\text{·}j}}{n}$.  The statistic $U$ compares the observed and expected counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.6**. The MLE's of $\\psi$ and $\\gamma$ are\n",
    "\n",
    "$$\n",
    "\\hat{\\psi} = \\frac{X_{00} X_{11}}{X_{01} X_{10}}\n",
    ", \\quad\n",
    "\\hat{\\gamma} = \\log \\hat{\\psi}\n",
    "$$\n",
    "\n",
    "The asymptotic standard errors (computed from the delta method) are\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\text{se}}(\\hat{\\psi}) &= \\sqrt{\\frac{1}{X_{00}} + \\frac{1}{X_{01}} + \\frac{1}{X_{10}} + \\frac{1}{X_{11}}}\\\\\n",
    "\\hat{\\text{se}}(\\hat{\\gamma}) &= \\hat{\\psi} \\hat{\\text{se}}(\\hat{\\gamma})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another test of independence is the Wald test for $\\gamma = 0$ given by $W = (\\hat{\\gamma} - 0) / \\hat{\\text{se}}(\\hat{\\gamma})$. \n",
    "\n",
    "A $1 - \\alpha$ confidence interval for $\\gamma$ is $\\hat{\\gamma} \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\gamma})$.\n",
    "\n",
    "A $1 - \\alpha$ confidence interval for $\\psi$ can be obtained in two ways.  First, we could use $\\hat{\\psi} \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\psi})$.  Second, since $\\psi = e^{\\gamma}$ we could use  $\\exp \\{\\hat{\\gamma} \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\gamma})\\}$.  This second method is usually more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.2 Interpreting the Odds Ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose event $A$ has probability $\\mathbb{P}(A)$.  The odds of $A$ are defined as\n",
    "\n",
    "$$\\text{odds}(A) = \\frac{\\mathbb{P}(A)}{1 - \\mathbb{P}(A)}$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\\mathbb{P}(A) = \\frac{\\text{odds}(A)}{1 + \\text{odds}(A)}$$\n",
    "\n",
    "Let $E$ be the event that someone is exposed to something (smoking, radiation, etc) and let $D$ be the event that they get a disease.  The odds of getting the disease given exposure are:\n",
    "\n",
    "$$\\text{odds}(D | E) = \\frac{\\mathbb{P}(D | E)}{1 - \\mathbb{P}(D | E)}$$\n",
    "\n",
    "and the odds of getting the disease given non-exposure are:\n",
    "\n",
    "$$\\text{odds}(D | E^c) = \\frac{\\mathbb{P}(D | E^c)}{1 - \\mathbb{P}(D | E^c)}$$\n",
    "\n",
    "The **odds ratio** is defined to be\n",
    "\n",
    "$$\\psi = \\frac{\\text{odds}(D | E)}{\\text{odds}(D | E^c)}$$\n",
    "\n",
    "If $\\psi = 1$ then the disease probability is the same for exposed and unexposed; this implies these events are independent.  Recall that the log-odds ratio is defined as $\\gamma = \\log \\psi$.  Independence corresponds to $\\gamma = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this table of probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & D^c    & D      & \\\\\n",
    "\\hline\n",
    "E^c   & p_{00} & p_{01} & p_{0\\text{·}}\\\\\n",
    "E     & p_{10} & p_{11} & p_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & p_{\\text{·}0} & p_{\\text{·}1} & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Denote the data by\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c} \n",
    "      & D^c    & D      & \\\\\n",
    "\\hline\n",
    "E^c   & X_{00} & X_{01} & X_{0\\text{·}}\\\\\n",
    "E     & X_{10} & X_{11} & X_{1\\text{·}}\\\\\n",
    " \\hline\n",
    "      & X_{\\text{·}0} & X_{\\text{·}1} & X_{\\text{··}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(D | E) = \\frac{p_{11}}{p_{10} + p_{11}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{P}(D | E^c) = \\frac{p_{01}}{p_{00} + p_{01}}\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\n",
    "\\text{odds}(D | E) = \\frac{p_{11}}{p_{10}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\text{odds}(D | E^c) = \\frac{p_{01}}{p_{00}}\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$ \\psi = \\frac{p_{11}p_{00}}{p_{01}p_{10}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the parameters, we have to consider how the data were collected.  There are three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Sampling**.  We draw a sample from the population and, for each sample, record their exposure and disease status.  In this case, $X = (X_{00}, X_{01}, X_{10}, X_{11}) \\sim \\text{Multinomial}(n, p)$.  We then estimates the probabilities in the table by $\\hat{p}_{ij$ = X_{ij} / n$ and\n",
    "\n",
    "$$ \\hat{\\psi} = \\frac{\\hat{p}_{11} \\hat{p}_{00}}{\\hat{p}_{01} \\hat{p}_{10}} = \\frac{X_{11} X_{00}}{X_{01} X_{10}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prospective Sampling (Cohort Sampling)**.  We get some exposed and unexposed people and count the number with disease within each group.  Thus,\n",
    "\n",
    "$$\n",
    "X_{01} \\sim \\text{Binomial}(X_{0\\text{·}}, \\mathbb{P}(D | E^c))\n",
    "\\quad \\text{and} \\quad\n",
    "X_{11} \\sim \\text{Binomial}(X_{1\\text{·}}, \\mathbb{P}(D | E))\n",
    "$$\n",
    "\n",
    "In this case we should write small letters $x_{0\\text{·}},  x_{1\\text{·}}$ instead of capital letters $ X_{0\\text{·}},  X_{1\\text{·}}$ since they are fixed and not random, but we'll keep using capital letters for notational simplicity.\n",
    "\n",
    "We can estimate $\\mathbb{P}(D | E))$ and $\\mathbb{P}(D | E^c)$ but we cannot estimate all probabilities in the table.  Still, we can estimate $\\psi$.  Now:\n",
    "\n",
    "$$\\hat{\\mathbb{P}}(D | E) = \\frac{X_{11}}{X_{1\\text{·}}}\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\mathbb{P}}(D | E^c) = \\frac{X_{01}}{X_{0\\text{·}}}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ \\hat{\\psi} = \\frac{X_{11} X_{00}}{X_{01} X_{10}}$$\n",
    "\n",
    "as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case-Control (Retrospective Sampling)**.  Here we get some diseased and non-diseased people and we observe how many are exposed.  This is much more efficient if the disease is rare.  Hence,\n",
    "\n",
    "$$\n",
    "X_{10} \\sim \\text{Binomial}(X_{\\text{·}0}, \\mathbb{P}(E | D^c))\n",
    "\\quad \\text{and} \\quad\n",
    "X_{11} \\sim \\text{Binomial}(X_{\\text{·}1}, \\mathbb{P}(E | D))\n",
    "$$\n",
    "\n",
    "From this data we can estimate $\\mathbb{P}(E | D)$ and $\\mathbb{P}(E | D^c)$.  Surprisingly, we can still estimate $\\psi$.  To understand why, note that\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(E | D) = \\frac{p_{11}}{p_{01} + p_{11}},\n",
    "\\quad 1 - \\mathbb{P}(E | D) = \\frac{p_{01}}{p_{01} + p_{11}},\n",
    "\\quad \\text{odds}(E | D) = \\frac{p_{11}}{p_{01}}\n",
    "$$\n",
    "\n",
    "By a similar argument,\n",
    "\n",
    "$$\\text{odds}(E | D^c) = \\frac{p_{10}}{p_{00}}$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\frac{\\text{odds}(E | D)}{\\text{odds}(E | D^c)} = \\frac{p_{11} p_{00}}{p_{01} p_{10}} = \\psi$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\hat{\\psi} = \\frac{X_{11} X_{00}}{X_{01} X_{10}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all three methods, the estimate of $\\psi$ turns out to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to try to estimate $\\mathbb{P}(D | E) - \\mathbb{P}(D | E^c)$.  In a case-control design, this quantity is not estimable.  To see this, we apply Bayes' theorem to get\n",
    "\n",
    "$$\\mathbb{P}(D | E) - \\mathbb{P}(D | E^c) = \\frac{\\mathbb{P}(E | D) \\mathbb{P}(D))}{\\mathbb{P}(E)} - \\frac{\\mathbb{P}(E^c | D) \\mathbb{P}(D)}{\\mathbb{P}(E^c)}$$\n",
    "\n",
    "Because of the way we obtained the data, $\\mathbb{P}(D)$ is not estimable from the data.\n",
    "\n",
    "However, we can estimate $\\xi = \\mathbb{P}(D | E) / \\mathbb{P}(D | E^c)$, which is called the **relative risk**, under the **rare disease assumption**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.9**.  Let $\\xi = \\mathbb{P}(D | E) / \\mathbb{P}(D | E^c)$.  Then\n",
    "\n",
    "$$ \\frac{\\psi}{\\xi} \\rightarrow 1$$\n",
    "\n",
    "as $\\mathbb{P}(D) \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, under the rare disease assumption, the relative risk is approximately the same as the odds ratio, which we can estimate.\n",
    "\n",
    "In a randomized experiment, we can interpret a strong association, that is $\\psi \\neq 1$, as a causal relationship.  In an observational (non-randomized) study, the association can be due to other unobserved **confounding** variables.  We'll discuss causation in more detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3 Two Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that $Y \\in \\{ 1, \\dots, I \\}$ and $Z \\in \\{ 1, \\dots, J \\}$ are two discrete variables.  The data can be represented by an $I \\times J$ table of contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c|cccccc|c} \n",
    "       & Y = 1  & Y = 2  & \\cdots & Y = j & \\cdots & Y = J   & \\\\\n",
    "\\hline\n",
    "Z = 1 & X_{11}  & X_{12} & \\cdots & X_{1j} & \\cdots & X_{1J} & X_{1\\text{·}}\\\\\n",
    "Z = 2 & X_{21}  & X_{22} & \\cdots & X_{2j} & \\cdots & X_{2J} & X_{2\\text{·}}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "Z = i & X_{i1}  & X_{i2} & \\cdots & X_{ij} & \\cdots & X_{iJ} & X_{i\\text{·}}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "Z = I & X_{I1}  & X_{I2} & \\cdots & X_{Ij} & \\cdots & X_{IJ} & X_{I\\text{·}}\\\\\n",
    " \\hline\n",
    "      & X_{\\text{·}1} & X_{\\text{·}1} & \\cdots & X_{\\text{·}j} & \\cdots & X_{\\text{·}J} & n\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider testing\n",
    "\n",
    "$$\n",
    "H_0: Y \\text{ ⫫ } Z\n",
    "\\quad \\text{versus} \\quad\n",
    "H_1: \\text{not} (Y \\text{ ⫫ } Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.10**.  Let\n",
    "\n",
    "$$ T = 2 \\sum_{i=1}^I \\sum_{j=1}^J X_{ij} \\log \\left( \\frac{X_{ij} X_{\\text{··}}}{X_{i\\text{·}} X_{\\text{·}j}} \\right) $$\n",
    "\n",
    "The limiting distribution of $T$ under the null hypothesis of independence is $\\chi^2_\\nu$ where $\\nu = (I - 1)(J - 1)$.\n",
    "\n",
    "Pearson's $\\chi^2$ statistic is\n",
    "\n",
    "$$ U = \\sum_{i=1}^I \\sum_{j=1}^J \\frac{(n_{ij} - E_{ij})^2}{E_{ij}}$$\n",
    "\n",
    "Asymptotically, under $H_0$, $U$ has a $\\chi^2_\\nu$ distribution where $\\nu = (I - 1)(J - 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of ways to quantify the strength of dependence between two discrete variables $Y$ and $Z$.  Most of them are not very intuitive.  The one we shall use is not standard but is more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define\n",
    "\n",
    "$$\\delta(Y, Z) = \\max_{A, B} \\Big|\\; \\mathbb{P}_{Y, Z}(Y \\in A, Z \\in B) - \\mathbb{P}_Y(Y \\in A) - \\mathbb{P}_Z(Z \\in B) \\;\\Big|$$\n",
    "\n",
    "where the maximum is over all pairs of events $A$ and $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 16.12**.  Properties of $\\delta(Y, Z)$:\n",
    "\n",
    "1. $0 \\leq \\delta(Y, Z) \\leq 1$\n",
    "2. $\\delta(Y, Z) = 0$ if and only if $Y \\text{ ⫫ } Z$\n",
    "3. The following identity holds:\n",
    "\n",
    "$$ \\delta(X, Y) = \\frac{1}{2} \\sum_{i=1}^I \\sum_{j=1}^J \\Big|\\; p_{ij} - p_{i\\text{·}} p_{\\text{·}j} \\;\\Big|$$\n",
    "\n",
    "4.  The MLE of $\\delta$ is\n",
    "\n",
    "$$ \\hat{\\delta}(X, Y) = \\frac{1}{2} \\sum_{i=1}^I \\sum_{j=1}^J \\Big|\\; \\hat{p}_{ij} - \\hat{p}_{i\\text{·}} \\hat{p}_{\\text{·}j} \\;\\Big|$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{p}_{ij} = \\frac{X_{ij}}{n},\n",
    "\\quad \\hat{p}_{i\\text{·}} = \\frac{X_{i\\text{·}}}{n},\n",
    "\\quad \\hat{p}_{\\text{·}j} = \\frac{X_{\\text{·}j}}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of $\\delta$ is this: if one person makes probability statements assuming independence and another person makes probability statements without assuming independence, their probability statements may differ by as much as $\\delta$.  Here's a suggested scale for interpreting $\\delta$:\n",
    "\n",
    "| range        | interpretation             |\n",
    "|--------------|----------------------------|\n",
    "| 0 to 0.01    | negligible association     |\n",
    "| 0.01 to 0.05 | non-negligible association |\n",
    "| 0.05 to 0.1  | substantial association    |\n",
    "| over 0.1     | very strong association    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval for $\\delta$ can be obtained by bootstrapping.  The steps are:\n",
    "\n",
    "1. Draw $X^* \\sim \\text{Multinomial}(n, \\hat{p})$;\n",
    "2. Compute $\\hat{p}_ij, \\hat{p}_{i\\text{·}}, \\hat{p}_{\\text{·}j}$;\n",
    "3. Compute $\\delta^*$;\n",
    "4. Repeat.\n",
    "\n",
    "Now we use any of the methods we learned earlier for constructing bootstrap confidence intervals.  However, we should not use a Wald interval in this case.  The reason is that if $Y$ and $Z$ are independent then $\\delta = 0$ and we are on the boundary of the parameter space.  In this case, the Wald method is not valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.4 Two Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that $Y$ and $Z$ are both continuous.  If we assume that the joint distribution of $Y$ and $Z$ is bivariate Normal, then we measure the dependence between $Y$ and $Z$ by means of the correlation coefficient $\\rho$.  Tests, estimates, and confidence intervals for $\\rho$ in the Normal case are given in the previous chapter.  If we do not assume normality, then we need a nonparametric method for asserting dependence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the correlation is\n",
    "\n",
    "$$\\rho = \\frac{\\mathbb{E}((X_1 - \\mu_1)(X_2 - \\mu_2))}{\\sigma_1 \\sigma_2} $$\n",
    "\n",
    "A nonparametric estimator of $\\rho$ is the plug-in estimator which is:\n",
    "\n",
    "$$\\hat{\\rho} = \\frac{\\sum_{i=1}^n (X_{1i} - \\overline{X}_1)(X_{2i} - \\overline{X}_2)}{\\sqrt{\\sum_{i=1}^n (X_{1i} - \\overline{X}_1)^2 \\sum_{i=1}^n (X_{2i} - \\overline{X}_2)^2}} $$\n",
    "\n",
    "which is just the sample correlation.  A confidence interval can be constructed using the bootstrap.  A test for $\\rho = 0$ can be based on the Wald test using the bootstrap to estimate the standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plug-in approach is useful for large samples.  For small samples, we measure the correlation using the **Spearman rank correlation coefficient** $\\hat{\\rho}_S$.  We simply replace the data by their ranks, ranking each variable separately, then compute the correlation coefficient of the ranks.\n",
    "\n",
    "To test the null hypothesis that $\\rho_S = 0$, we need the distribution of $\\hat{\\rho}_S$ under the null hypothesis.  This can be obtained by simulation.  We fix the ranks of the first variable as $1, 2, \\dots, n$.  The ranks of the second variable are chosen at random from the set of $n!$ possible orderings, then we compute the correlation.  This is repeated many times, and the resulting distribution $\\mathbb{P}_0$ is the null distribution of $\\hat{\\rho}_S$. The p-value for the test is $\\mathbb{P}_0(|R| > |\\hat{\\rho}_S|)$ where $R$ is drawn from the null distribution $\\mathbb{P}_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
