{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Expectation of a Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **expected value**, **mean** or **first moment** of $X$ is defined to be\n",
    "\n",
    "$$ \\mathbb{E}(X) = \\int x \\; dF(x) = \\begin{cases}\n",
    "\\sum_x x f(x) &\\text{if } X \\text{ is discrete} \\\\\n",
    "\\int x f(x)\\; dx &\\text{if } X \\text{ is continuous}\n",
    "\\end{cases} $$\n",
    "\n",
    "assuming that the sum (or integral) is well-defined.  We use the following notation to denote the expected value of $X$:\n",
    "\n",
    "$$ \\mathbb{E}(X) = \\mathbb{E}X = \\int x\\; dF(x) = \\mu = \\mu_X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation is a one-number summary of the distribution.  Think of $\\mathbb{E}(X)$ as the average value you'd obtain if you computed the numeric average $n^{-1} \\sum_{i=1}^n X_i$ for a large number of IID draws $X_1, \\dots, X_n$.  The fact that $\\mathbb{E}(X) \\approx n^{-1} \\sum_{i=1}^n X_i$ is a theorem called the law of large numbers which we will discuss later.   We use $\\int x \\; dF(x)$ as a convenient unifying notation between the discrete case $\\sum_x x f(x)$ and the continuous case $\\int x f(x) \\; dx$ but you should be aware that $\\int x \\; dF(x)$ has a precise meaning discussed in real analysis courses.\n",
    "\n",
    "To ensure that $\\mathbb{E}(X)$ is well defined, we say that $\\mathbb{E}(X)$ exists if $\\int_x |x| \\; dF_X(x) < \\infty$.  Otherwise we say that the expectation does not exist.  From now on, wheneverwe discuss expectations, we implicitly assume they exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.6 (The rule of the lazy statician)**.  Let $Y = r(X)$.  Then\n",
    "\n",
    "$$ \\mathbb{E}(Y) = \\mathbb{E}(r(X)) = \\int r(x) \\; dF_X(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a special case, let $A$ be an event and let $r(x) = I_A(x)$, where $I_A(x) = 1$ if $x \\in A$ and $I_A(x) = 0$ otherwise.  Then\n",
    "\n",
    "$$ \\mathbb{E}(I_A(X)) = \\int I_A(x) f_X(x) dx = \\int_A f_X(x) dx = \\mathbb{P}(X \\in A) $$\n",
    "\n",
    "In other words, probability is a special case of expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions of several variables are handled in a similar way.  If $Z = r(X, Y)$ then\n",
    "\n",
    "$$ \\mathbb{E}(Z) = \\mathbb{E}(r(X, Y)) = \\int \\int r(x, y) \\; dF(x, y) $$\n",
    "\n",
    "The **$k$-th moment** of $X$ is defined to be $\\mathbb{E}(X^k)$, assuming that $\\mathbb{E}(|X|^k) < \\infty$.  We shall rarely make much use of moments beyond $k = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Properties of Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.10**.  If $X_1, \\dots, X_n$ are random variables and $a_1, \\dots, a_n$ are constants, then\n",
    "\n",
    "$$ \\mathbb{E}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i \\mathbb{E}(X_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.12**.  Let $X_1, \\dots, X_n$ be independent random variables.  Then,\n",
    "\n",
    "$$ \\mathbb{E}\\left(\\prod_i X_i \\right) = \\prod_i \\mathbb{E}(X_i) $$\n",
    "\n",
    "Notice that the summation rule does not require independence but the product does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Variance and Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be a random variable with mean $\\mu$.  The **variance** of $X$ -- denoted by $\\sigma^2$ or $\\sigma_X^2$ or $\\mathbb{V}(X)$ or $\\mathbb{V}X$ -- is defined by\n",
    "\n",
    "$$ \\sigma^2 = \\mathbb{E}(X - \\mu)^2 = \\int (x - \\mu)^2\\; dF(x) $$\n",
    "\n",
    "assuming this expectation exists.  The **standard deviation** is $\\text{sd}(X) = \\sqrt{\\mathbb{V}(X)}$ and is also denoted by $\\sigma$ and $\\sigma_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.14**.  Assuming the variance is well defined, it has the following properties:\n",
    "\n",
    "1.  $\\mathbb{V}(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2$\n",
    "2.  If $a$ and $b$ are constants then $\\mathbb{V}(aX + b) = a^2 \\mathbb{V}(X)$\n",
    "3.  If $X_1, \\dots, X_n$ are independent and $a_1, \\dots, a_n$ are constants then\n",
    "\n",
    "    $$ \\mathbb{V}\\left( \\sum_{i=1}^n a_iX_i \\right) = \\sum_{i=1}^n a_i^2 \\mathbb{V}(X_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X_1, \\dots, X_n$ are random variables then we define the **sample mean** to be\n",
    "\n",
    "$$ \\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i  $$\n",
    "\n",
    "and the **sample variance** to be\n",
    "\n",
    "$$ S_n^2 = \\frac{1}{n - 1} \\sum_{i=1}^n \\left(X_i - \\overline{X}_n\\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.16**.  Let $X_1, \\dots, X_n$ be IID and let $\\mu = \\mathbb{E}(X_i)$, $\\sigma^2 = \\mathbb{V}(X_i)$.  Then\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}\\left(\\overline{X}_n\\right) = \\mu,\n",
    "\\quad\n",
    "\\mathbb{V}\\left(\\overline{X}_n\\right) = \\frac{\\sigma^2}{n},\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{E}\\left(S_n^2\\right) = \\sigma^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X$ and $Y$ are random variables, then the covariance and correlation between $X$ and $Y$ measure how strong the linear relationship between $X$ and $Y$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ and $Y$ be random variables with means $\\mu_X$ and $\\mu_Y$ and standard deviation $\\sigma_X$ and $\\sigma_Y$.  Define the **covariance** between $X$ and $Y$ by\n",
    "\n",
    "$$ \\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] $$\n",
    "\n",
    "and the **correlation** by\n",
    "\n",
    "$$ \\rho = \\rho_{X, Y} = \\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.18**.  The covariance satisfies:\n",
    "\n",
    "$$ \\text{Cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y) $$\n",
    "\n",
    "The correlation satisfies:\n",
    "\n",
    "$$ -1 \\leq \\rho(X, Y) \\leq 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $Y = a + bX$ for some constants $a$ and $b$ then $\\rho(X, Y) = 1$ if $b > 0$ and $\\rho(X, Y) = -1$ if $b < 0$.  If $X$ and $Y$ are independent, then $\\text{Cov}(X, Y) = \\rho = 0$.  The converse is not true in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.19**.\n",
    "\n",
    "$$ \n",
    "\\mathbb{V}(X + Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) + 2 \\text{Cov}(X, Y)\n",
    "\\quad \\text{ and } \\quad\n",
    "\\mathbb{V}(X - Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) - 2 \\text{Cov}(X, Y)\n",
    "$$\n",
    "\n",
    "More generally, for random variables $X_1, \\dots, X_n$,\n",
    "\n",
    "$$ \\mathbb{V}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i^2 \\mathbb{V}(X_i) + 2 \\sum \\sum_{i < j} a_i a_j \\text{Cov}(X_i, X_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Expectation and Variance of Important Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\text{Distribution} & \\text{Mean} & \\text{Variance}           \\\\\n",
    "\\hline\n",
    "\\text{Point mass at } p      & a             & 0              \\\\\n",
    "\\text{Bernoulli}(p)          & p             & p(1-p)         \\\\\n",
    "\\text{Binomial}(n, p)        & np            & np(1-p)        \\\\\n",
    "\\text{Geometric}(p)          & 1/p           & (1 - p)/p^2    \\\\\n",
    "\\text{Poisson}(\\lambda)      & \\lambda       & \\lambda        \\\\\n",
    "\\text{Uniform}(a, b)         & (a + b) / 2   & (b - a)^2 / 12 \\\\\n",
    "\\text{Normal}(\\mu, \\sigma^2) & \\mu           & \\sigma^2       \\\\\n",
    "\\text{Exponential}(\\beta)    & \\beta         & \\beta^2        \\\\\n",
    "\\text{Gamma}(\\alpha, \\beta)  & \\alpha \\beta  & \\alpha \\beta^2 \\\\\n",
    "\\text{Beta}(\\alpha, \\beta)   & \\alpha / (\\alpha + \\beta) & \\alpha \\beta / ((\\alpha + \\beta)^2 (\\alpha + \\beta + 1)) \\\\\n",
    "t_\\nu                        & 0 \\text{ (if } \\nu > 1 \\text{)} & \\nu / (\\nu - 2) \\text{ (if } \\nu > 2 \\text{)} \\\\\n",
    "\\chi^2_p                     & p             & 2p             \\\\\n",
    "\\text{Multinomial}(n, p)     & np            & \\text{see below} \\\\\n",
    "\\text{Multivariate Nornal}(\\mu, \\Sigma) & \\mu & \\Sigma \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two entries in the table are multivariate models which involve a random vector $X$ of the form\n",
    "\n",
    "$$ X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_k \\end{pmatrix} $$\n",
    "\n",
    "The mean of a random vector $X$ is defined by\n",
    "\n",
    "$$ \\mu = \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_k \\end{pmatrix} = \\begin{pmatrix} \\mathbb{E}(X_1) \\\\ \\vdots \\\\ \\mathbb{E}(X_k) \\end{pmatrix} $$\n",
    "\n",
    "The **variance-covariance matrix** $\\Sigma$ is defined to be\n",
    "\n",
    "$$ \\Sigma = \\begin{pmatrix}\n",
    "\\mathbb{V}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_k) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\mathbb{V}(X_2) & \\cdots & \\text{Cov}(X_2, X_k) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_k, X_1) & \\text{Cov}(X_k, X_2) & \\cdots & \\mathbb{V}(X_k)\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X \\sim \\text{Multinomial}(n, p)$ then\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}(X) = np = n(p_1, \\dots, p_k)\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{V}(X) = \\begin{pmatrix}\n",
    "np_1(1 - p_1) & -np_1p_2 & \\cdots & np_1p_k \\\\\n",
    "-np_2p_1 & np_2(1 - p_2) & \\cdots & np_2p_k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-np_kp_1 & -np_kp_2 & \\cdots & np_k(1 - p_k)\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "To see this:\n",
    "\n",
    "- Note that the marginal distribution of any one component is $X_i \\sim \\text{Binomial}(n, p_i)$, so $\\mathbb{E}(X_i) = np_i$ and $\\mathbb{V}(X_i) = np_i(1 - p_i)$.  \n",
    "- Note that, for $i \\neq j$, $X_i + X_j \\sim \\text{Binomial}(n, p_i + p_j)$, so $\\mathbb{V}(X_i + X_j) = n(p_i + p_j)(1 - (p_i + p_j))$.\n",
    "- Using the formula for the covariance of a sum, for $i \\neq j$,\n",
    "\n",
    "$$ \\mathbb{V}(X_i + X_j) = \\mathbb{V}(X_i) + \\mathbb{V}(X_j) + 2 \\text{Cov}(X_i, X_j) =  np_i(1 - p_i) + np_j(1 - p_j) + 2 \\text{Cov}(X_i, X_j) $$\n",
    "\n",
    "Equating the last two formulas we get a formula for the covariance, $\\text{Cov}(X_i, X_j) = -np_ip_j$.\n",
    "\n",
    "Finally, here's a lemma that can be useful for finding means and variances of linear combinations of multivariate random vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 4.20**.  If $a$ is a vector and $X$ is a random vector with mean $\\mu$ and variance $\\Sigma$ then\n",
    "\n",
    "$$ \\mathbb{E}(a^T X) = a^T \\mu\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{V}(a^T X) = a^T \\Sigma a $$\n",
    "\n",
    "If $A$ is a matrix then\n",
    "\n",
    "$$ \\mathbb{E}(A X) = A \\mu\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{V}(AX) = A \\Sigma A^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Conditional Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional expectation of $X$ given $Y = y$ is\n",
    "\n",
    "$$ \\mathbb{E}(X | Y = y) = \\begin{cases}\n",
    "\\sum x f_{X | Y}(x | y) &\\text{ discrete case} \\\\\n",
    "\\int x f_{X | Y}(x | y) dy &\\text{ continuous case}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If $r$ is a function of $x$ and $y$ then\n",
    "\n",
    "$$ \\mathbb{E}(r(X, Y) | Y = y) = \\begin{cases}\n",
    "\\sum r(x, y) f_{X | Y}(x | y) &\\text{ discrete case} \\\\\n",
    "\\int r(x, y) f_{X | Y}(x | y) dy &\\text{ continuous case}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While $\\mathbb{E}(X)$ is a number, $\\mathbb{E}(X | Y = y)$ is a function of $y$.  Before we observe $Y$, we don't know the value of $\\mathbb{E}(X | Y = y)$ so it is a random variable which we denote $\\mathbb{E}(X | Y)$.  In other words, $\\mathbb{E}(X | Y)$ is the random variable whose value is $\\mathbb{E}(X | Y = y)$ when $Y$ is observed as $y$.  Similarly, $\\mathbb{E}(r(X, Y) | Y)$ is the random variable whose value is $\\mathbb{E}(r(X, Y) | Y = y)$ when $Y$ is observed as $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.23 (The rule of iterated expectations)**.  For random variables $X$ and $Y$, assuming the expectations exist, we have that\n",
    "\n",
    "$$ \\mathbb{E}[\\mathbb{E}(Y | X)] = \\mathbb{E}(Y)\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{E}[\\mathbb{E}(X | Y)] = \\mathbb{E}(X) $$\n",
    "\n",
    "More generally, for any function $r(x, y)$ we have\n",
    "\n",
    "$$ \\mathbb{E}[\\mathbb{E}(r(X, Y) | X)] = \\mathbb{E}(r(X, Y))\n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{E}[\\mathbb{E}(r(X, Y) | Y)] = \\mathbb{E}(r(X, Y)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  We will prove the first equation.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\mathbb{E}(Y | X)] &= \\int \\mathbb{E}(Y | X = x) f_X(x) dx = \\int \\int y f(y | x) dy f(x) dx \\\\\n",
    "&= \\int \\int y f(y|x) f(x) dx dy = \\int \\int y f(x, y) dx dy = \\mathbb{E}(Y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **conditional variance** is defined as\n",
    "\n",
    "$$ \\mathbb{V}(Y | X = x) = \\int (y - \\mu(x))^2 f(y | x) dx $$\n",
    "\n",
    "where $\\mu(x) = \\mathbb{E}(Y | X = x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.26**.  For random variables $X$ and $Y$,\n",
    "\n",
    "$$ \\mathbb{V}(Y) = \\mathbb{E}\\mathbb{V}(Y | X) + \\mathbb{V} \\mathbb{E} (Y | X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Technical Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Expectation as an Integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integral of a measurable function $r(x)$ is defined as follows.  First suppose that $r$ is simple, meaning that it takes finitely many values $a_1, \\dots, a_k$ over a partition $A_1, \\dots, A_k$.  Then  $\\int r(x) dF(x) = \\sum_{i=1}^k a_i \\mathbb{P}(r(X) \\in A_i)$.  The integral of a positive measurable function $r$ is defined by $\\int r(x) dF(x) = \\lim_i \\int r_i(x) dF(x)$, where $r_i$ is a sequence of simple functions such that $r_i(x) \\leq r(x)$ and $r_i(x) \\rightarrow r(x)$ as $i \\rightarrow \\infty$.  This does not depend on the particular sequence.  The integral of a measurable function $r$ is defined to be $\\int r(x) dF(x) = \\int r^+(x) dF(x) - \\int r^-(x) dF(x)$ assuming both integrals are finite, where $r^+(x) = \\max \\{ r(x), 0 \\}$ and $r^-(x) = \\min\\{ r(x), 0 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2  Moment Generating Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **moment generating function (mgf)** or **Laplace transform** of $X$ is defined by\n",
    "\n",
    "$$ \\psi_X(t) = \\mathbb{E}(e^{tX}) = \\int e^{tx} dF(x) $$\n",
    "\n",
    "where $t$ varies over the real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we assume the mgf is well defined for all $t$ in small neighborhood of 0.  A related function is the characteristic function, defined by $\\mathbb{E}(e^{itX})$ where $i = \\sqrt{-1}$.  This function is always defined for all $t$.  The mgf is useful for several reasons.  First, it helps us compute the moments of a distribution.  Second, it helps us find the distribution of sums of random variables.  Third, it is used to prove the central limit theorem.\n",
    "\n",
    "When the mgf is well defined, it can be shown that we can interchange the operations of differentiation and \"taking expectation\".  This leads to\n",
    "\n",
    "$$ \\psi'(0) = \\left[ \\frac{d}{dt} \\mathbb{E} e^{tX} \\right]_{t = 0} = \\mathbb{E} \\left[ \\frac{d}{dt} e^{tX} \\right]_{t = 0}\n",
    "= \\mathbb{E}[X e^{tX}]_{t = 0} = \\mathbb{E}(X)\n",
    "$$\n",
    "\n",
    "By taking further derivatives we conclude that $\\psi^{(k)}(0) = \\mathbb{E}(X^k)$.  This gives us a method for computing the moments of a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 4.30**.  Properties of the mgf.\n",
    "\n",
    "1.  If $Y = aX + b$ then $\\psi_Y(t) = e^{bt} \\psi_X(at) $\n",
    "2.  if $X_1, \\dots, X_n$ are independent and $Y = \\sum_i X_i$ then $\\psi_Y(t) = \\prod_i \\psi_{i}(t)$, where $\\psi_i$ is the mgf of $X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 4.32**.  Let $X$ and $Y$ be random variables.  If $\\psi_X(t) = \\psi_Y(t)$ for all $t$ in an open interval around 0, then $X \\overset{d}= Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moment Generating Function for Some Common Distributions**\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{Distribution} & \\text{mgf} \\\\\n",
    "\\hline\n",
    "\\text{Bernoulli}(p)   & pe^t + (1 - p)         \\\\\n",
    "\\text{Binomial}(n, p) & (pe^t + (1 - p))^n     \\\\\n",
    "\\text{Poisson}(\\lambda) & e^{\\lambda(e^t - 1)} \\\\\n",
    "\\text{Normal}(\\mu, \\sigma^2) & \\exp\\left\\{\\mu t + \\frac{\\sigma^2 t^2}{2} \\right\\} \\\\\n",
    "\\text{Gamma}(\\alpha, \\beta) & \\left( \\frac{\\beta}{\\beta - t} \\right)^\\alpha \\text{ for } t < \\beta\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.7.1**.  Suppose we play a game where we start with $c$ dollars.  On each play of the game you either double your money or half your money, with equal probability.  What is your expected fortune after $n$ trials?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Let the random variables $X_i$ be the fortune after the $i$-th trial, $X_0 = c$ always taking the value $c$.  Then:\n",
    "\n",
    "$$ \\mathbb{E}[X_{i + 1} | X_i = x] = 2x \\cdot \\frac{1}{2} + \\frac{x}{2} \\cdot \\frac{1}{2} = \\frac{5}{4}x $$\n",
    "\n",
    "Taking the expectation on $X_i$ on both sides (i.e. integrating over $F_{X_i}(x)$),\n",
    "\n",
    "$$ \\mathbb{E}(\\mathbb{E}[X_{i + 1} | X_i = x]) = \\frac{5}{4} \\mathbb{E}(X_i) \\Longrightarrow \\mathbb{E}(X_{i+1}) = \\frac{5}{4}  \\mathbb{E}(X_i)$$\n",
    "\n",
    "Therefore, by induction,\n",
    "\n",
    "$$ \\mathbb{E}(X_n) = \\left(\\frac{5}{4}\\right)^n c $$\n",
    "\n",
    "Note that this is **not** a martingale, as in the traditional double-or-nothing formulation -- the expected value goes up at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.7.2**.  Show that $\\mathbb{V}(X) = 0$ if and only if there is a constant $c$ such that $\\mathbb{P}(X = c) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have $\\mathbb{V}(X) = \\mathbb{E}[(X - \\mathbb{E}(X))^2]$:\n",
    "\n",
    "$$ \\mathbb{V}(X) = \\int (x - \\mu_X)^2 dF_X(x) $$\n",
    "\n",
    "Since $(x - \\mu_X)^2 \\geq 0$, in order for the variance to be 0 we must have the integrand be zero with probability 1, i.e. $\\mathbb{P}(X = \\mu_X) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.7.3**.  Let $X_1, \\dots, X_n \\sim \\text{Uniform}(0, 1)$ and let $Y_n = \\max \\{ X_1, \\dots, X_n \\}$.  Find $\\mathbb{E}(Y_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
