{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Loglinear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1 The Loglinear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X = (X_1, \\dots, X_m)$ be a random vector with probability\n",
    "\n",
    "$$ f(x) = \\mathbb{P}(X = x) = \\mathbb{P}(X_1 = x_1, \\dots, X_m = x_m) $$\n",
    "\n",
    "Let $r_j$ be the number of values that $X_j$ takes; without loss of generality, assume $X_j \\in \\{ 0, 1, \\dots, r_j - 1 \\}$.  Suppose we have $n$ such vectors.\n",
    "\n",
    "We can think of the data as a sample from a Multinomial with $N = r_1 \\times r_2 \\times \\dots \\times r_m$ categories.  The data can be represented as counts in a $r_1 \\times r_2 \\times \\dots \\times r_m$ table.  Let $p = \\{ p_1, \\dots, p_N \\}$ denote the multinomial parameter.\n",
    "\n",
    "Let $S = \\{ 1, \\dots, m \\}$.  Given a vector $x = (x_1, \\dots, x_m)$ and a subset $A \\subset S$, let $x_A = (x_j : j \\in A)$.  For example, if $A = \\{1, 3\\}$ then $x_A = (x_1, x_3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 18.1**.  The joint probability function $f(x)$ of a single random vector $X = (X_1, \\dots, X_m)$ can be written as \n",
    "\n",
    "$$ \\log f(x) = \\sum_{A \\subset S} \\psi_A(x) $$\n",
    "\n",
    "where the sum is over all subsets $A$ of $S = \\{1, \\dots, m \\}$ and the $\\psi$'s satisfy the following conditions:\n",
    "\n",
    "1. $\\psi_\\varnothing(x)$ is a constant;\n",
    "2. For every $A \\subset S$, $\\psi_A(x)$ is only a function of $x_A$ and not the rest of the $x_j$'s.\n",
    "3. If $i \\in A$ and $x_i = 0$, then $\\psi_A(x) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula in this theorem is known as the **log-linear expansion** of $f$.  Note that this is the probability function for a single draw.  Each $\\psi_A(x)$ will depend on some unknown parameters $\\beta_A$.  Let $\\beta = (\\beta_A : A \\subset S)$ be the set of all these parameters.  We will write $f(x) = f(x; \\beta)$ when we want to estimate the dependence on the unknown parameters $\\beta$.\n",
    "\n",
    "In terms of the multinomial, the parameter space is\n",
    "\n",
    "$$ \\mathcal{P} = \\left\\{ p = (p_1, \\dots, p_N) : p_j \\geq 0, \\sum_{j=1}^N p_j = 1 \\right\\} $$\n",
    "\n",
    "This is an $N - 1$ dimensional space.  In the log-linear representation, the parameter space is\n",
    "\n",
    "$$ \\Theta = \\Bigg\\{ \\beta = (\\beta_1, \\dots, \\beta_N) : \\beta = \\beta(p), p \\in \\mathcal{P} \\Bigg\\} $$\n",
    "\n",
    "where $\\beta(p)$ is the set of $\\beta$ values associated with $p$.  The set $\\Theta$ is a $N - 1$ dimensional surface in $\\mathbb{R}^N$.  We can always go back and forth between the two parametrizations by writing $\\beta = \\beta(p)$ and $p = p(\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 18.14**.  Let $(X_a, X_b, X_c)$ be a partition of vectors $(X_1, \\dots, X_m)$.  Then $X_b \\text{ тлл } X_c \\; | \\; X_a$ if and only if all the $\\psi$-terms in the log-linear expansion that have at least one coordinate in $b$ and one coordinate in $c$ are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove this Theorem, we will use the following Lemma whose proof follows from the definition of conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 18.5**.  A partition $(X_a, X_b, X_c)$ satisfies $X_b \\text{ тлл } X_c \\; | \\; X_a$ if and only if $(x_a, x_b, x_c) = g(x_a, x_b) h(x_a, x_c)$ for some functions $g$ and $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof of Theorem 18.14**.  Suppose that $\\psi_t$ is 0 whenever $t$ has coordinates in $b$ and $c$.  Hence, $\\psi_t$ is 0 if $t$ is not a subset of $a \\cup b$ or $t$ is not a subset of $a \\cup c$.  Therefore,\n",
    "\n",
    "$$ \\log f(x) = \\sum_{t \\subset a \\cup b} \\psi_t(x) + \\sum_{t \\subset a \\cup c} \\psi_t(x) - \\sum_{t \\subset a} \\psi_t(x) $$\n",
    "\n",
    "Exponentiating, we see that the joint density is of the form $g(x_a, x_b) h(x_a, x_c)$.  By Lemma 18.5, $X_b \\text{ тлл } X_c \\; | \\; X_a$.  The converse follows by reversing the argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2 Graphical Log-Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\log f(x) = \\sum_{A \\subset S} \\psi_A(x)$ be a log-linear model.  Then $f$ is **graphical** if all $\\psi$-terms are non-zero except for any pair of coordinates not in the edge set for some graph $\\mathcal{G}$.  In other words, $\\psi_A(x) = 0$ if and only if $\\{i, j\\} \\subset A$ and $(i, j)$ is not an edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to think about this definition: if you can add a term to the model and the graph does not change, then the model is not graphical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.3 Hierarchical Log-Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a set of log-linear models that is larger than the set of graphical models and that are used quite a bit.  These are hierarchical log-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A log-linear model is **hierarchical** if $\\psi_a = 0$ and $a \\subset t$ implies that $\\psi_t = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 18.9**.  A graphical model is hierarchical but the reverse need not be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.4 Model Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical models can be written succintly using **generators**.  This is most easily explained by example.  Suppose that $X = (X_1, X_2, X_3)$.  Then, $M = 1.2 + 1.3$ stands for:\n",
    "\n",
    "$$ \\log f = \\psi_\\varnothing + \\psi_1 + \\psi_2 + \\psi_3 + \\psi_{12} + \\psi_{13}$$\n",
    "\n",
    "The formula $M = 1.2 + 1.3$ says: \"include $\\psi_{12}$ and $\\psi_{13}$\".  We have to also include the lower terms or it won't be hierarchical.  The generator $M = 1.2.3$ is the **saturated** model\n",
    "\n",
    "$$ \\log f = \\psi_\\varnothing + \\psi_1 + \\psi_2 + \\psi_3 + \\psi_{12} + \\psi_{13} + \\psi_{23} + \\psi_{123}$$\n",
    "\n",
    "The saturated models corresponds to fitting and unconstrained multinomial.  Consider $M = 1 + 2 + 3$ which means\n",
    "\n",
    "$$ \\log f = \\psi_\\varnothing + \\psi_1 + \\psi_2 + \\psi_3 $$\n",
    "\n",
    "This is the mutual independence model.  Finally, consider $M = 1.2$ which has log-linear expansion\n",
    "\n",
    "$$ \\log f = \\psi_\\varnothing + \\psi_1 + \\psi_2 + \\psi_{12} $$\n",
    "\n",
    "This models makes $X_3 | X_2 = x_2, X_1 = x_1$ a uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.5 Lattices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical models can be organized into something called a **lattice**.  This is the set of all hierarchical models partially ordered by inclusion.  The set of all hierarchical models for two variables can be illustrated as in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"134pt\" height=\"260pt\"\r\n",
       " viewBox=\"0.00 0.00 134.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 130,-256 130,4 -4,4\"/>\r\n",
       "<!-- 1.2 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>1.2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.2</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1 + 2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-162\" rx=\"29.795\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1 + 2</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2&#45;&gt;1 + 2 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>1.2&#45;&gt;1 + 2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63,-215.697C63,-207.983 63,-198.712 63,-190.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.5001,-190.104 63,-180.104 59.5001,-190.104 66.5001,-190.104\"/>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2&#45;&gt;1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1 + 2&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.4685,-144.411C50.1645,-136.042 44.8508,-125.71 40.0473,-116.37\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"43.1563,-114.762 35.4703,-107.47 36.9313,-117.964 43.1563,-114.762\"/>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2&#45;&gt;2 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1 + 2&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.5315,-144.411C75.8355,-136.042 81.1492,-125.71 85.9527,-116.37\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.0687,-117.964 90.5297,-107.47 82.8437,-114.762 89.0687,-117.964\"/>\r\n",
       "</g>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;0 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>1&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.3496,-72.7646C39.7115,-64.2831 45.1469,-53.7144 50.0413,-44.1974\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.2346,-45.6409 54.6957,-35.1473 47.0096,-42.4395 53.2346,-45.6409\"/>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;0 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>2&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.6504,-72.7646C86.2885,-64.2831 80.8531,-53.7144 75.9587,-44.1974\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.9904,-42.4395 71.3043,-35.1473 72.7654,-45.6409 78.9904,-42.4395\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1e9d8bd8c88>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "d = Digraph()\n",
    "\n",
    "d.edge('1.2', '1 + 2')\n",
    "d.edge('1 + 2', '1')\n",
    "d.edge('1 + 2', '2')\n",
    "d.edge('1', '0')\n",
    "d.edge('2', '0')\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M = 1.2$ is the saturated model, $M = 1 + 2$ is the independence model, $M = 1$ is the independence model plus $X_2 | X_1$ is uniform, $M = 2$ is the independence model plus $X_1 | X_2$ is uniform, $M = 0$ is the uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lattice of trivariate models is shown in the figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"320pt\" height=\"548pt\"\r\n",
       " viewBox=\"0.00 0.00 319.55 548.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 544)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-544 315.546,-544 315.546,4 -4,4\"/>\r\n",
       "<!-- 1.2.3 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>1.2.3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"163\" cy=\"-522\" rx=\"28.6953\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-518.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.2.3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 1.3 + 2.3 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1.2 + 1.3 + 2.3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"163\" cy=\"-450\" rx=\"64.9885\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.2 + 1.3 + 2.3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2.3&#45;&gt;1.2 + 1.3 + 2.3 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>1.2.3&#45;&gt;1.2 + 1.3 + 2.3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163,-503.697C163,-495.983 163,-486.712 163,-478.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"166.5,-478.104 163,-468.104 159.5,-478.104 166.5,-478.104\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 1.3 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>1.2 + 1.3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"58\" cy=\"-378\" rx=\"43.5923\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"58\" y=\"-374.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.2 + 1.3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 1.3 + 2.3&#45;&gt;1.2 + 1.3 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1.2 + 1.3 + 2.3&#45;&gt;1.2 + 1.3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.174,-433.116C124.164,-423.11 104.645,-410.096 88.508,-399.339\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.2035,-396.263 79.9415,-393.628 86.3206,-402.087 90.2035,-396.263\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 2.3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>1.2 + 2.3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"163\" cy=\"-378\" rx=\"43.5923\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-374.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.2 + 2.3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 1.3 + 2.3&#45;&gt;1.2 + 2.3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1.2 + 1.3 + 2.3&#45;&gt;1.2 + 2.3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163,-431.697C163,-423.983 163,-414.712 163,-406.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"166.5,-406.104 163,-396.104 159.5,-406.104 166.5,-406.104\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.3 + 2.3 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>1.3 + 2.3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"268\" cy=\"-378\" rx=\"43.5923\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-374.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.3 + 2.3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 1.3 + 2.3&#45;&gt;1.3 + 2.3 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>1.2 + 1.3 + 2.3&#45;&gt;1.3 + 2.3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.826,-433.116C201.836,-423.11 221.355,-410.096 237.492,-399.339\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.679,-402.087 246.058,-393.628 235.797,-396.263 239.679,-402.087\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 3 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>1.2 + 3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"61\" cy=\"-306\" rx=\"36.2938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"61\" y=\"-302.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.2 + 3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 1.3&#45;&gt;1.2 + 3 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>1.2 + 1.3&#45;&gt;1.2 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58.7416,-359.697C59.0722,-351.983 59.4695,-342.712 59.838,-334.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.3355,-334.245 60.267,-324.104 56.3419,-333.945 63.3355,-334.245\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.3 + 2 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>1.3 + 2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"166\" cy=\"-306\" rx=\"36.2938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"166\" y=\"-302.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.3 + 2</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 1.3&#45;&gt;1.3 + 2 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>1.2 + 1.3&#45;&gt;1.3 + 2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.3777,-362.496C96.4775,-352.061 118.368,-337.873 135.984,-326.455\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.265,-329.147 144.753,-320.771 134.458,-323.273 138.265,-329.147\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 2.3&#45;&gt;1.2 + 3 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>1.2 + 2.3&#45;&gt;1.2 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.368,-362.155C126.467,-351.929 106.492,-338.22 90.184,-327.028\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.7825,-323.88 81.5568,-321.108 87.8215,-329.652 91.7825,-323.88\"/>\r\n",
       "</g>\r\n",
       "<!-- 2.3 + 1 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>2.3 + 1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"264\" cy=\"-306\" rx=\"36.2938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"264\" y=\"-302.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2.3 + 1</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 2.3&#45;&gt;2.3 + 1 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>1.2 + 2.3&#45;&gt;2.3 + 1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M184.42,-362.155C199.175,-351.929 218.954,-338.22 235.102,-327.028\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"237.419,-329.681 243.645,-321.108 233.432,-323.927 237.419,-329.681\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.3 + 2.3&#45;&gt;1.3 + 2 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>1.3 + 2.3&#45;&gt;1.3 + 2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.368,-362.155C231.467,-351.929 211.492,-338.22 195.184,-327.028\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.782,-323.88 186.557,-321.108 192.822,-329.652 196.782,-323.88\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.3 + 2.3&#45;&gt;2.3 + 1 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>1.3 + 2.3&#45;&gt;2.3 + 1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M267.011,-359.697C266.57,-351.983 266.041,-342.712 265.549,-334.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"269.042,-333.888 264.977,-324.104 262.054,-334.288 269.042,-333.888\"/>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2 + 3 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>1 + 2 + 3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"116\" cy=\"-234\" rx=\"44.393\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"116\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1 + 2 + 3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 3&#45;&gt;1 + 2 + 3 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>1.2 + 3&#45;&gt;1 + 2 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M73.7563,-288.765C80.5305,-280.143 88.9992,-269.365 96.5719,-259.727\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"99.5048,-261.659 102.931,-251.633 94.0005,-257.334 99.5048,-261.659\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.2 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>1.2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.2</text>\r\n",
       "</g>\r\n",
       "<!-- 1.2 + 3&#45;&gt;1.2 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>1.2 + 3&#45;&gt;1.2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.9425,-288.411C48.9199,-280.129 43.9635,-269.925 39.4644,-260.662\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"42.5171,-258.936 34.9997,-251.47 36.2205,-261.994 42.5171,-258.936\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.3 + 2&#45;&gt;1 + 2 + 3 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>1.3 + 2&#45;&gt;1 + 2 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.403,-288.765C148.307,-280.23 140.702,-269.582 133.871,-260.019\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.541,-257.736 127.881,-251.633 130.845,-261.805 136.541,-257.736\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.3 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>1.3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"205\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"205\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.3</text>\r\n",
       "</g>\r\n",
       "<!-- 1.3 + 2&#45;&gt;1.3 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>1.3 + 2&#45;&gt;1.3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.242,-288.411C179.905,-280.042 185.662,-269.71 190.865,-260.37\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"194.014,-261.909 195.824,-251.47 187.899,-258.502 194.014,-261.909\"/>\r\n",
       "</g>\r\n",
       "<!-- 2.3 + 1&#45;&gt;1 + 2 + 3 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>2.3 + 1&#45;&gt;1 + 2 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M238.558,-292.967C214.9,-281.777 179.387,-264.981 152.67,-252.344\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"154.157,-249.176 143.621,-248.064 151.164,-255.503 154.157,-249.176\"/>\r\n",
       "</g>\r\n",
       "<!-- 2.3 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>2.3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"277\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"277\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2.3</text>\r\n",
       "</g>\r\n",
       "<!-- 2.3 + 1&#45;&gt;2.3 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>2.3 + 1&#45;&gt;2.3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M267.147,-288.055C268.579,-280.346 270.309,-271.027 271.918,-262.364\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275.407,-262.746 273.792,-252.275 268.525,-261.468 275.407,-262.746\"/>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>1 + 2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"77\" cy=\"-162\" rx=\"29.795\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1 + 2</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2 + 3&#45;&gt;1 + 2 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>1 + 2 + 3&#45;&gt;1 + 2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.559,-216.055C101.893,-207.679 96.1679,-197.404 91.0033,-188.134\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.0099,-186.339 86.0853,-179.307 87.8949,-189.746 94.0099,-186.339\"/>\r\n",
       "</g>\r\n",
       "<!-- 1 + 3 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>1 + 3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"155\" cy=\"-162\" rx=\"29.795\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"155\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1 + 3</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2 + 3&#45;&gt;1 + 3 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>1 + 2 + 3&#45;&gt;1 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M125.441,-216.055C130.107,-207.679 135.832,-197.404 140.997,-188.134\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.105,-189.746 145.915,-179.307 137.99,-186.339 144.105,-189.746\"/>\r\n",
       "</g>\r\n",
       "<!-- 2 + 3 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>2 + 3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"233\" cy=\"-162\" rx=\"29.795\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"233\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2 + 3</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2 + 3&#45;&gt;2 + 3 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>1 + 2 + 3&#45;&gt;2 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.959,-218.666C158.511,-207.566 184.285,-192.146 204.001,-180.35\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.802,-183.351 212.587,-175.213 202.208,-177.344 205.802,-183.351\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.2&#45;&gt;1 + 2 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>1.2&#45;&gt;1 + 2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M38.0966,-217.465C44.4162,-208.617 52.4737,-197.337 59.6148,-187.339\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6234,-189.149 65.5877,-178.977 56.9273,-185.08 62.6234,-189.149\"/>\r\n",
       "</g>\r\n",
       "<!-- 1.3&#45;&gt;1 + 3 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>1.3&#45;&gt;1 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.903,-217.465C187.584,-208.617 179.526,-197.337 172.385,-187.339\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.073,-185.08 166.412,-178.977 169.377,-189.149 175.073,-185.08\"/>\r\n",
       "</g>\r\n",
       "<!-- 2.3&#45;&gt;2 + 3 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>2.3&#45;&gt;2 + 3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M267.016,-217.116C261.58,-208.469 254.733,-197.575 248.609,-187.833\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"251.535,-185.911 243.25,-179.307 245.609,-189.636 251.535,-185.911\"/>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"83\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2&#45;&gt;1 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>1 + 2&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M78.4831,-143.697C79.1443,-135.983 79.9389,-126.712 80.6761,-118.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.167,-118.367 81.5339,-108.104 77.1926,-117.769 84.167,-118.367\"/>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"155\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"155\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 2&#45;&gt;2 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>1 + 2&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M93.1617,-146.496C104.289,-136.51 119.247,-123.086 131.664,-111.942\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.287,-114.292 139.391,-105.008 129.611,-109.082 134.287,-114.292\"/>\r\n",
       "</g>\r\n",
       "<!-- 1 + 3&#45;&gt;1 -->\r\n",
       "<g id=\"edge25\" class=\"edge\"><title>1 + 3&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.731,-146.155C129.669,-136.372 116.33,-123.404 105.114,-112.5\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"107.365,-109.806 97.7549,-105.345 102.485,-114.825 107.365,-109.806\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"227\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">3</text>\r\n",
       "</g>\r\n",
       "<!-- 1 + 3&#45;&gt;3 -->\r\n",
       "<g id=\"edge26\" class=\"edge\"><title>1 + 3&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.269,-146.155C180.331,-136.372 193.67,-123.404 204.886,-112.5\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.515,-114.825 212.245,-105.345 202.635,-109.806 207.515,-114.825\"/>\r\n",
       "</g>\r\n",
       "<!-- 2 + 3&#45;&gt;2 -->\r\n",
       "<g id=\"edge27\" class=\"edge\"><title>2 + 3&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M216.838,-146.496C205.711,-136.51 190.753,-123.086 178.336,-111.942\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"180.389,-109.082 170.609,-105.008 175.713,-114.292 180.389,-109.082\"/>\r\n",
       "</g>\r\n",
       "<!-- 2 + 3&#45;&gt;3 -->\r\n",
       "<g id=\"edge28\" class=\"edge\"><title>2 + 3&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M231.517,-143.697C230.856,-135.983 230.061,-126.712 229.324,-118.112\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"232.807,-117.769 228.466,-108.104 225.833,-118.367 232.807,-117.769\"/>\r\n",
       "</g>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"155\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"155\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;0 -->\r\n",
       "<g id=\"edge29\" class=\"edge\"><title>1&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M97.5703,-74.8345C107.75,-64.9376 121.524,-51.5462 133.031,-40.3591\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.474,-42.865 140.204,-33.3847 130.595,-37.8461 135.474,-42.865\"/>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;0 -->\r\n",
       "<g id=\"edge30\" class=\"edge\"><title>2&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155,-71.6966C155,-63.9827 155,-54.7125 155,-46.1124\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158.5,-46.1043 155,-36.1043 151.5,-46.1044 158.5,-46.1043\"/>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;0 -->\r\n",
       "<g id=\"edge31\" class=\"edge\"><title>3&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212.43,-74.8345C202.25,-64.9376 188.476,-51.5462 176.969,-40.3591\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179.405,-37.8461 169.796,-33.3847 174.526,-42.865 179.405,-37.8461\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1e9d8c13be0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "d = Digraph()\n",
    "\n",
    "d.edge('1.2.3', '1.2 + 1.3 + 2.3')\n",
    "\n",
    "d.edge('1.2 + 1.3 + 2.3', '1.2 + 1.3')\n",
    "d.edge('1.2 + 1.3 + 2.3', '1.2 + 2.3')\n",
    "d.edge('1.2 + 1.3 + 2.3', '1.3 + 2.3')\n",
    "\n",
    "d.edge('1.2 + 1.3', '1.2 + 3')\n",
    "d.edge('1.2 + 2.3', '1.2 + 3')\n",
    "\n",
    "d.edge('1.2 + 1.3', '1.3 + 2')\n",
    "d.edge('1.3 + 2.3', '1.3 + 2')\n",
    "\n",
    "d.edge('1.2 + 2.3', '2.3 + 1')\n",
    "d.edge('1.3 + 2.3', '2.3 + 1')\n",
    "\n",
    "d.edge('1.2 + 3', '1 + 2 + 3')\n",
    "d.edge('1.3 + 2', '1 + 2 + 3')\n",
    "d.edge('2.3 + 1', '1 + 2 + 3')\n",
    "\n",
    "d.edge('1.2 + 3', '1.2')\n",
    "d.edge('1.3 + 2', '1.3')\n",
    "d.edge('2.3 + 1', '2.3')\n",
    "\n",
    "d.edge('1.2', '1 + 2')\n",
    "d.edge('1.3', '1 + 3')\n",
    "d.edge('2.3', '2 + 3')\n",
    "\n",
    "d.edge('1 + 2 + 3', '1 + 2')\n",
    "d.edge('1 + 2 + 3', '1 + 3')\n",
    "d.edge('1 + 2 + 3', '2 + 3')\n",
    "\n",
    "d.edge('1 + 2', '1')\n",
    "d.edge('1 + 2', '2')\n",
    "d.edge('1 + 3', '1')\n",
    "d.edge('1 + 3', '3')\n",
    "d.edge('2 + 3', '2')\n",
    "d.edge('2 + 3', '3')\n",
    "\n",
    "d.edge('1', '0')\n",
    "d.edge('2', '0')\n",
    "d.edge('3', '0')\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.6 Fitting Log-Linear Models to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\beta$ denote all the parameters in a log-linear model $M$.  The log-likelihood for $\\beta$ is:\n",
    "\n",
    "$$ \\ell(\\beta) = \\sum_j x_j \\log p_j(\\beta) $$\n",
    "\n",
    "where the sum is over the cells and $p(\\beta)$ denotes the cell probabilities corresponding to $\\beta$.  The MLE $\\hat{\\beta}$ generally has to be found numerically.  The model with all possible $\\psi$-terms is called the  **saturated models**.  We can also fit any **sub-model** which corresponds to setting some subset of $\\psi$ terms to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any submodel $M$, define the **deviance** $\\text{dev}(M)$ by\n",
    "\n",
    "$$ \\text{dev}(M) = 2 (\\hat{\\ell}_\\text{sat} - \\hat{\\ell}_M) $$\n",
    "\n",
    "where $\\ell_\\text{sat}$ is the log-likelihood of the saturated model evaluated at the MLE and $\\hat{\\ell}_M$ is the log-likelihood of the model $M$ evaluated at its MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 18.14**.  The deviance is the likelihood test statistic for\n",
    "\n",
    "$$\n",
    "H_0 : \\text{the model is } M\n",
    "\\quad \\text{versus} \\quad\n",
    "H_1 : \\text{the model is not } M\n",
    "$$\n",
    "\n",
    "Under $H_0$, $\\text{dev}(M) \\leadsto \\chi^2_\\nu$ with $\\nu$ degrees of freedom equal to the difference in the number of parameters between the saturated model and $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to find a good model is to use the deviance to test every sub-model.  Every model that is not rejected by this test is then considered a plausible model.  However, this is not a good strategy for two reasons.  First, we will end up doing many tests, which means there is ample opportunity for making Type I and Type II errors.  Second, we will end up using models where we failed to reject $H_0$.  But we might fail to reject $H_0$ due to low power.  The result is that we end up with a bad model just due to low power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many model searching strategies.  A common approach is to use some form of *penalized likelihood*.  One version of penalized is the AIC that we used in regression.  For any model $M$ define\n",
    "\n",
    "$$ \\text{AIC}(M) = -2 \\left( \\hat{\\ell}(M) - |M|\\right) $$\n",
    "\n",
    "where $|M|$ is the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of models $\\{ M_1, M_2, \\dots \\}$.  Let $\\hat{f}_j(x)$ denote the estimated probability function obtained by using the maximum likelihood estimator of model $M_j$.  Thus, $\\hat{f}_j(x) = \\hat{f}(x; \\hat{\\beta}_j)$ where $\\hat{\\beta}_j$ is the MLE of the set of parameters $\\beta_j$ for model $M_j$.  We will use the loss function $D(f, \\hat{f})$ where\n",
    "\n",
    "$$ D(f, g) = \\sum_x f(x) \\log \\frac{f(x)}{g(x)} $$\n",
    "\n",
    "is the Kullback-Leibler divergence between two probability density functions.  The corresponding risk function is $R(f, \\hat{f}) = \\mathbb{E}(D(f, \\hat{f}))$.\n",
    "\n",
    "Notice that $D(f, \\hat{f})  = c - A(f, \\hat{f})$ where $c = \\sum_x f(x) \\log f(x)$ does not depend on $\\hat{f}$ and \n",
    "\n",
    "$$ A(f, \\hat{f}) = \\sum_x f(x) \\log \\hat{f}(x) $$\n",
    "\n",
    "Thus minimizing the risk is equivalent to minimizing $a(f, \\hat{f}) = \\mathbb{E}(A(f, \\hat{f}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to estimate $a(f, \\hat{f})$ by $\\sum_x \\log \\hat{f}(x)$ but, just as the training error in regression is highly biased estimate of prediction risk, it is also the case that $\\sum_x \\log \\hat{f}(x)$ is a highly biased estimate of $a(f, \\hat{f})$.  In fact, the bias is approximately equal to $|M_j|$.  Thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 18.15**.  $\\text{AIC}(M_j)$ is an approximately unbiased estimate of $a(f, \\hat{f})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding a \"best model\" this way we can draw the corresponding graph.  We can also check the overall fit of the selected model using the deviance as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.8 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 18.8.1**.  Solve for the $p_{ij}$'s in terms of the $\\beta$'s in Example 18.3:\n",
    "\n",
    "*Example*: Let $X = (X_1, X_2)$ where $X_1 \\in \\{0, 1\\}$ and $X_2 \\in \\{ 0, 1, 2 \\}$.  The joint distribution of $n$ such random vectors is a multinomial with 6 categories.  The multinomial parameters can be written as a 2-by-3 table as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccccc}\n",
    "\\hline\n",
    "\\text{multinomial} & x_2 & 0 & 1 & 2 \\\\\n",
    "\\hline\n",
    "x_1 & 0 & p_{00} & p_{01} & p_{02} \\\\\n",
    "    & 1 & p_{10} & p_{11} & p_{12} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The $n$ data vectors can be summarized as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccccc}\n",
    "\\hline\n",
    "\\text{multinomial} & x_2 & 0 & 1 & 2 \\\\\n",
    "\\hline\n",
    "x_1 & 0 & C_{00} & C_{01} & C_{02} \\\\\n",
    "    & 1 & C_{10} & C_{11} & C_{12} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For $x = (x_1, x_2)$, the log-linear expansion takes the form\n",
    "\n",
    "$$ \\log f(x) = \\psi_\\varnothing(x) + \\psi_1(x) + \\psi_2(x) + \\psi_{12}(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\psi_\\varnothing(x) = \\log p_{00} \\\\\n",
    "& \\psi_1(x) = x_1 \\log \\left( \\frac{p_{10}}{p_{00}} \\right) \\\\\n",
    "& \\psi_2(x) = I(x_2 = 1) \\log \\left( \\frac{p_{01}}{p_{00}} \\right) \n",
    "            + I(x_2 = 2) \\log \\left( \\frac{p_{02}}{p_{00}} \\right) \\\\\n",
    "& \\psi_{12}(x) = I(x_1 = 1, x_2 = 1) \\log \\left( \\frac{p_{11}p_{00}}{p_{01}p_{10}} \\right)\n",
    "               + I(x_1 = 1, x_2 = 2) \\log \\left( \\frac{p_{12}p_{00}}{p_{02}p_{10}} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The six parameters of this model are:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\beta_1 = \\log p_{00} &\n",
    "\\beta_2 = \\log \\left( \\frac{p_{10}}{p_{00}} \\right) &\n",
    "\\beta_3 = \\log \\left( \\frac{p_{01}}{p_{00}} \\right) \\\\\n",
    "\\beta_4 = \\log \\left( \\frac{p_{02}}{p_{00}} \\right) &\n",
    "\\beta_5 = \\log \\left( \\frac{p_{11}p_{00}}{p_{01}p_{10}} \\right) &\n",
    "\\beta_6 = \\log \\left( \\frac{p_{12}p_{00}}{p_{02}p_{10}} \\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiate the six definitions for $\\beta_k$ above and get:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p_{00} = \\exp \\beta_1 &\n",
    "\\frac{p_{10}}{p_{00}} = \\exp \\beta_2 &\n",
    "\\frac{p_{01}}{p_{00}} = \\exp \\beta_3 \\\\\n",
    "\\frac{p_{02}}{p_{00}} = \\exp \\beta_4 &\n",
    "\\frac{p_{11}p_{00}}{p_{01}p_{10}} = \\exp \\beta_5 &\n",
    "\\frac{p_{12}p_{00}}{p_{02}p_{10}} = \\exp \\beta_6\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "and then, isolating the $p_{ij}$s,\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p_{00} = \\exp \\beta_1 &\n",
    "p_{10} = \\exp \\left( \\beta_1 + \\beta_2 \\right) &\n",
    "p_{01} = \\exp \\left( \\beta_1 + \\beta_3 \\right) \\\\\n",
    "p_{02} = \\exp \\left( \\beta_1 + \\beta_4 \\right) &\n",
    "p_{11} = \\exp \\left( \\beta_1 + \\beta_2 + \\beta_3 + \\beta_5 \\right) &\n",
    "p_{12} = \\exp \\left( \\beta_1 + \\beta_2 + \\beta_4 + \\beta_6 \\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 18.8.2**. Repeat example 18.17 using 7 covariates and $n = 1000$. To avoid numerical problems, replace any zero count with a one.\n",
    "\n",
    "*Example*: Here is a synthetic example.  We generate $n = 100$ random vectors $X = (X_1, \\dots, X_5)$ of length 5.  We generated the data as follows:\n",
    "\n",
    "$$ X_1 \\sim \\text{Bernoulli}(1/2)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ X_j | X_1, \\dots, X_{j-1} = \\begin{cases}\n",
    "1/4 & \\text{if } X_{j-1} = 0 \\\\\n",
    "3/4 & \\text{if } X_{j-1} = 1\n",
    "\\end{cases}$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_1, \\dots, x_5) &= \n",
    "\\left( \\frac{1}{2} \\right) \\left( \\frac{1}{4} \\right)^{x_1} \\left( \\frac{3}{4} \\right)^{1 - x_1} \\left( \\frac{1}{4} \\right)^{x_2} \\left( \\frac{3}{4} \\right)^{1 - x_2} \\left( \\frac{1}{4} \\right)^{x_3} \\left( \\frac{3}{4} \\right)^{1 - x_3} \\left( \\frac{1}{4} \\right)^{x_4} \\left( \\frac{3}{4} \\right)^{1 - x_4} \\\\\n",
    "&= \\left( \\frac{1}{2} \\right) \\left( \\frac{1}{4} \\right)^{x_1 + x_2 + x_3 + x_4} \\left( \\frac{3}{4} \\right)^{4 - (x_1 + x_2 + x_3 + x_4)} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We estimated $f$ using three methods: (i)  maximum likelihood treating this as a multinomial with 32 categories (ii) maximum likelihood from the best loglinear model using AIC and forward selection and (iii) maximum likelihood from the best loglinear model using BIC and forward selection. We estimated the risk by simulating the sample 100 times.  The average risks were:\n",
    "\n",
    "| Method | Risk |\n",
    "|--------|------|\n",
    "| MLE    | 0.63 |\n",
    "| AIC    | 0.54 |\n",
    "| BIC    | 0.53 |\n",
    "\n",
    "In this example, there is little difference between AIC and BIC.  Both are better than maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate $n = 1000$ random vectors $X = (X_1, \\dots, X_k)$ of length $k = 7$ as follows:\n",
    "\n",
    "$$ X_1 \\sim \\text{Bernoulli}(1/2) \n",
    "\\quad \\text{and} \\quad\n",
    "X_j | X_1, \\dots, X_{j-1} = \\begin{cases}\n",
    "1/4 & \\text{if } X_{j-1} = 0 \\\\\n",
    "3/4 & \\text{if } X_{j-1} = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$ f(x) = \\frac{1}{2} \\left( \\frac{1}{4} \\right)^{\\sum_{i=1}^{k - 1} x_i} \\left( \\frac{3}{4} \\right)^{(k - 1) - \\left(\\sum_{i=1}^{k - 1} x_i \\right)} $$\n",
    "\n",
    "We will use the KL divergence as our loss function:\n",
    "\n",
    "$$ D(f, g) = \\sum_{x \\in \\Omega} f(x) \\log \\frac{f(x)}{g(x)} $$\n",
    "\n",
    "and estimate the risk function $R(f, \\hat{f}) = \\mathbb{E}[D(f, \\hat{f})]$ by bootstraping the estimation process and calculating the average of the loss functions in each bootstrap step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE estimate\n",
    "\n",
    "The MLE estimate for the full multinomial model is relatively simple: consider the adjusted counts $\\tilde{C}_{\\xi}$ to be the number of times an observation $\\xi$ appears, or 1 if that number of observations is 0.  There are $2^k = 128$ possible observations, so we get $2^k$ adjusted counts $\\tilde{C}_1, \\dots, \\tilde{C}_{2^k}$.  The MLE estimate is then computed as $\\hat{p} = (\\hat{p}_1, \\dots, \\hat{p}_{2^k})$, with $\\hat{p}_i = \\frac{\\tilde{C}_i}{\\sum_\\xi \\tilde{C}_\\xi}$.\n",
    "\n",
    "The function density estimate corresponding to the MLE estimate is a simple lookup table $\\hat{f}(\\xi) = \\hat{p}_\\xi$, as we already have a probability estimate associated with every single possible event.  The function density estimate can then be used to compute the loss function $D(f, \\hat{f})$, which is the KL divergence computed over the universe.   Repeating this process gives us the risk estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain, combinations\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "# Recipe from itertools documentation, https://docs.python.org/2.7/library/itertools.html#recipes\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "\n",
    "def generate_samples(n, k):\n",
    "    \"\"\"\n",
    "    Generates n samples of size k according to the synthetic distribution\n",
    "    \n",
    "    Args:\n",
    "       n:  number of samples\n",
    "       k:  sample size\n",
    "       \n",
    "    Returns:\n",
    "       X:  2D array of shape (n, k), representing n samples of size k\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a random unifom value between 0 and 1 for each variable in each sample\n",
    "    random_seeds = np.random.uniform(low=0, high=1, size=(k, n))\n",
    "    \n",
    "    # Create a variable to store the generated samples\n",
    "    output = np.empty_like(random_seeds).astype(int)\n",
    "    \n",
    "    # Generate x_1's as Bernoulli(1/2)\n",
    "    output[0] = random_seeds[0] > 0.5\n",
    "    for j in range(1, k):\n",
    "        # Generate x_j's recursively\n",
    "        output[j] = random_seeds[j] > (output[j - 1] == 0) * (1/4) + (output[j - 1] == 1) * (3/4)\n",
    "        \n",
    "    return output.T\n",
    "\n",
    "\n",
    "def generate_universe(k):\n",
    "    \"\"\"\n",
    "    Generates 2**k samples of size k iterating through the valid universe\n",
    "    \n",
    "    Args:\n",
    "       k: sample size\n",
    "       \n",
    "    Returns:\n",
    "       X_universe:  2D array of shape (2**k, k), representing the universe of all samples of size k\n",
    "    \"\"\"\n",
    "    X_universe = np.zeros(shape=(2**k, k), dtype=int)\n",
    "    for i, line in enumerate(powerset(range(k))):\n",
    "        X_universe[i, line] = 1\n",
    "        \n",
    "    return X_universe\n",
    "\n",
    "\n",
    "def row_to_binary(x):\n",
    "    \"\"\"\n",
    "    Translates a single row into a binary low-endian representation\n",
    "    \n",
    "    Args:\n",
    "       x:  1D array of 0s and 1s\n",
    "       \n",
    "    Returns:\n",
    "       sum(x[i] * (2**i) for index i)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Translate a row of 0s and 1s into a binary representation (low-endian)\n",
    "    return sum([x[i] * (2**i) for i in range(len(x))])\n",
    "\n",
    "def samples_to_count(X):\n",
    "    \"\"\"\n",
    "    Counts the number of occurrences of each sample in the dataset\n",
    "    \n",
    "    Args:\n",
    "       X:   2D array  n samples of size k\n",
    "       \n",
    "    Returns:\n",
    "       count:  1D array of size 2**k, where count[i] is the number of\n",
    "               occurrences of i as a row in binary (low-endian) in X,\n",
    "               or 1 if the count would be 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    binary_samples = np.apply_along_axis(row_to_binary, 1, X)\n",
    "    k = X.shape[1]\n",
    "    count = np.zeros(2**k)\n",
    "    for sample in binary_samples:\n",
    "        count[sample] += 1\n",
    "    \n",
    "    # Replace all zeroes with ones\n",
    "    return np.where(count == 0, 1, count)\n",
    "\n",
    "def true_density(x):\n",
    "    \"\"\"\n",
    "    True density function for the synthetic distribution.\n",
    "    \n",
    "    Args:\n",
    "       x:  1D array  sample\n",
    "       \n",
    "    Returns:\n",
    "       value of PDF = (1/2) * (3/4)**(sum(x)) * (1/4)**(len(x) - sum(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    s = sum(x)\n",
    "    k = len(x)\n",
    "    return (1/2) * ((3/4)**s) * ((1/4)**(k - s))\n",
    "\n",
    "def KL_divergence(f, g, X):\n",
    "    \"\"\"\n",
    "    Returns the KL divergence between f and g calculated on universe X\n",
    "    \n",
    "    Args:\n",
    "       f:  1D array => double  probability density function\n",
    "       g:  1D array => double  probability density function\n",
    "       X:  2D array (n, k) of n samples of size k\n",
    "       \n",
    "    Returns:  D(f, g) = sum over samples of f(x) * log(f(x) / g(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    def term(x):\n",
    "        \"\"\" \n",
    "        Return f(x) * log(f(x) / g(x)) \n",
    "        \n",
    "        Args:\n",
    "           x:  1D array sample of values\n",
    "        \"\"\"\n",
    "        fx = f(x)\n",
    "        gx = g(x)\n",
    "        return fx * np.log(fx / gx)\n",
    "    \n",
    "    return sum(np.apply_along_axis(term, 1, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b850553e85f94ab3b96975e1aef4fd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE mean: 0.747\n"
     ]
    }
   ],
   "source": [
    "# We can now calculate the expected value of the loss function by simulation\n",
    "\n",
    "def p_hat_mle(X):\n",
    "    \"\"\" MLE estimate for the multinomial (replacing zeros with ones) \"\"\"\n",
    "    count = samples_to_count(X)\n",
    "    return count / sum(count)\n",
    "\n",
    "def f_hat_mle(p_hat_mle):\n",
    "    \"\"\" Density function for the estimated multinomial \"\"\"\n",
    "    return lambda x : p_hat[row_to_binary(x)]\n",
    "\n",
    "B = 10000\n",
    "X_universe = generate_universe(k=7)\n",
    "risk_mle = np.empty(B)\n",
    "for i in tqdm_notebook(range(B)):\n",
    "    XX = generate_samples(n=1000, k=7)\n",
    "    p_hat = p_hat_mle(XX)\n",
    "    f_hat = f_hat_mle(p_hat)\n",
    "    risk_mle[i] = KL_divergence(true_density, f_hat, X_universe)\n",
    "\n",
    "print('MLE mean: %.3f' % risk_mle.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWXElEQVR4nO3dfbRldX3f8fdHHgoMAygzKDIOF4zPStHeFBMbHUUaoDykqyaBChXUNSVdsbZ1ND601S6TShJWfFg0cd0omYBmWC5FYiBRpuqIrWIdLAIKphRn4AI6wxjDQxkF+faPcwYPZ+bee+55uPfuc9+vte6695y999nfs+++n7vPb//2b6eqkCQ1z1MWuwBJUn8McElqKANckhrKAJekhjLAJamhDHBJaigDXIsmyUeT/Kce5tuW5LULUVPHOt+d5GMLuU5pvgxwjUw7eB9J8lCSHyTZmOTQPdOr6qKqev9i1giQZF2S6c7nquq/VtWbR7S+Bf+HpPFkgGvUzqyqQ4ETgZcC71rkeqSxYYBrQVTVD4Av0ApyANpH5L/b/nlVkmuS/DjJj5J8Ncle+2eS5yf5fpJz9rWe9vTN7df4XpLf6Jh2epLvJnkwyT1JNiRZAfwN8Mz2J4WHkjwzyfuSfKK93ESSSnJhkruT/F2Si5L8YpKb2zVf2rGeZyf5UpJdSe5P8skkR7SnXQGsBf6qva53tJ9/eZKvtV/r20nWdbzeBUnubNf9/SSvH+BXoTFigGtBJFkDnAbcMcMsbwOmgdXA04F3A08a5yHJy4DrgLdU1ZX7WMcKYDPwF8BRwLnAHyd5UXuWjwP/uqpWAi8GvlRVD7frureqDm1/3TtDjScBzwF+E/gQ8B7gtcCLgN9I8qo9pQAfAJ4JvAB4FvA+gKo6H7iL9ieTqvqDJMcA1wK/CzwN2AB8Jsnq9nv6CHBau+5fBm6aoT4tMwa4Ru3qJA8CdwM7gPfOMN+jwNHAsVX1aFV9tZ48UM+vAJ8D3lBV18zwGmcA26rqz6rqsar6FvAZ4HUd63hhksOq6u/a0+fj/VW1u6quAx4GNlXVjqq6B/gqrSYiquqOqtpcVT+pqp3AHwGvmvllOQ/466r666p6vKo2A1uB09vTHwdenOTgqrqvqr4zz7o1pgxwjdqvtY8c1wHPB1bNMN8f0jo6v67dXPDOrukXAV+rqi/Psq5jgZPazRA/TvJj4PXAM9rT/wWtUNye5CtJfmme7+WHHT8/so/HhwIkOSrJle1mmgeATzDz+95T96931f1PgKPbnxB+k9b7vy/JtUmeP8+6NaYMcC2IqvoKsBG4ZIbpD1bV26rqeOBM4D8kObljlouAtUk+OMtq7ga+UlVHdHwdWlW/1V7HN6vqbFrNK1cDn9qz+oHe3N4+0H7NE6rqMFpH2OmY3r2+u4EruupeUVUXt+v+QlWdQusTyu3Anw65XjWUAa6F9CHglCQndk9IckaSX0gS4AHgZ+2vPR4ETgVemeTiGV7/GuC5Sc5PckD76xeTvCDJgUlen+Twqnq0Yx3QOpI+MsnhQ3qfK4GHgB+327ff3jX9h8DxHY8/AZyZ5FeT7JfkoHbXxjVJnp7krHZb+E/ar/szJAxwLaB2e/DlwL4u3nkO8N9pBdTXgT+uqi1dy/8YOAU4Lcle/cer6kHgnwLnAPcCPwB+H/gH7VnOB7a1mzUuonVkTFXdDmwC7mw3YTxzsHfKfwFeBvw9rZOTV3VN/wDwH9vr2lBVdwNn0zpxu5PWEfnbaf19PoXWCd57gR/Rakv/NwPWpzERb+ggSc3kEbgkNZQBLkkNNWeAJ7ksyY4kt3Y9/5b2lW7fSfIHoytRkrQvvRyBb6R19v8JSV5N66TLCVX1ImboGiZJGp3955qhqq5PMtH19G8BF1fVT9rz7OhlZatWraqJie6XkiTN5sYbb7y/qlZ3Pz9ngM/gucCvJPk9YDewoaq+ua8Zk6wH1gOsXbuWrVu39rlKSVqekmzf1/P9nsTcH3gq8HJa/VU/1b4AYy9VNVVVk1U1uXr1Xv9AJEl96jfAp4GrquV/0RpsZ7axHiRJQ9ZvgF8NvAYgyXOBA4H7h1WUJGluc7aBJ9lEayS5VWndduq9wGXAZe2uhT+lNcRnX5d0Pvroo0xPT7N79+5+Fl/SDjroINasWcMBBxyw2KVIGkO99EI5d4ZJ5w2jgOnpaVauXMnExAQzNKM3UlWxa9cupqenOe644xa7HEljaNGvxNy9ezdHHnnkWIU3QBKOPPLIsfxkIWlpWPQAB8YuvPcY1/claWlYEgEuSZq/fi/kGZmJiSm2b39gaK937LGHsW3b+lnnScJ5553HFVdcAcBjjz3G0UcfzUknncQ111zDxo0b2bp1K5deeumTlpuYmGDlypXst99+ALzyla/kIx/5yNBql6TZLLkA3779Aao2DO31krmHaVmxYgW33norjzzyCAcffDCbN2/mmGOO6en1v/zlL7NqlV3gNXydBzO9HIho+bEJpe20007j2muvBWDTpk2ce+5MnW+khbHnYKZqw1A/lWp8GOBt55xzDldeeSW7d+/m5ptv5qSTTuppuVe/+tWceOKJnHjiiXzwg7Pdb1f6uYmJKZJLSC5hYmJqwZbVeFlyTSiL5YQTTmDbtm1s2rSJ008/veflbEJRPzqbCntp5hvWshovBniHs846iw0bNrBlyxZ27dq12OVI0qwM8A5vfOMbOfzww3nJS17Cli1bFrscaU7HHnvYE0fhnuhcfpZcgHfukMN6vV6tWbOGt771rfuctnHjRq6++uonHt9www1Aqw18TzfCE044gcsvv3yAaqX56Qxsm1OWnyUX4ItxBPHQQw/t9dy6detYt24dABdccAEXXHDBXvNs27ZttIVJ0iyWXIBLy1l3329pNga4tIQM+0K2fngBUXMsiQCvqrEc+KnPIdKlkeklnO2m2ByLHuAHHXQQu3btGrshZfeMB37QQQctdilahmYKasN5vCx6gK9Zs4bp6Wl27ty52KUM3Z478kgLzaBeHhY9wA844ADvWCNJfZhzLJQklyXZ0b7/Zfe0DUkqideSSyO05/qI5BJ7p+gJvRyBbwQuBZ50hUqSZwGnAHcNvyxJnewJon3p5abG1yeZ2MekDwLvAP5yyDVJ6kP3ZfVzPa/m66sNPMlZwD1V9e25eo4kWQ+sB1i7dm0/q5PUg5mO0j16H1/zHg88ySHAe4D/3Mv8VTVVVZNVNbl69er5rk6SNIN+bujwbOA44NtJtgFrgG8lecYwC5MkzW7eTShVdQtw1J7H7RCfrKr7h1iXJGkOvXQj3AR8HXhekukkbxp9WZJGrbNrordma6ZeeqHMenffqpoYWjWSFoxjiTefNzWWpIYywCWpoQxwSWooA1ySGmrRRyOUljsvdVe/DHBpkXmpu/plE4okNZQBLkkNZROKJNvhG8oAl2Q7fEPZhCJJDWWAS1JDGeCS1FAGuCQ1lAEuSQ1lgEtSQxngktRQBrgkNVQv98S8LMmOJLd2PPeHSW5PcnOSzyY5YrRlSpK69XIEvhE4teu5zcCLq+oE4G+Bdw25LqmxJiamvFmwFkQvNzW+PslE13PXdTy8AXjdcMuSmmv79geo2gB4s2CN1jDawN8I/M1ME5OsT7I1ydadO3cOYXWSJBhwMKsk7wEeAz450zxVNQVMAUxOTtYg65OabGJiiu3bHwAc8U/D0XeAJ3kDcAZwclUZzNIcOptWpGHoK8CTnAr8DvCqqvp/wy1JktSLXroRbgK+DjwvyXSSNwGXAiuBzUluSvLREdcpSerSSy+Uc/fx9MdHUIskaR68I4+kGXXfas079ywtBrikGXUGtn3alx4DXBqhcbpZsEfjS48BLg3BTH28xynkPBpfegxwaQjs463F4HCyktRQBrgkNZQBLkkNZYBLUkMZ4JLUUAa4JDWUAS5JDWWAS1JDGeCS1FBeiSn1qPNyeXA8EC0+A1zqUffl8st5PBAHtloaDHBJ8+bAVkuDbeCS1FC93BPzsiQ7ktza8dzTkmxO8n/a35862jIlSd16OQLfCJza9dw7gS9W1XOAL7YfS5IW0JwBXlXXAz/qevps4M/bP/858GtDrkuSNId+28CfXlX3AbS/HzW8kiRJvRj5Scwk65NsTbJ1586do16dJC0b/XYj/GGSo6vqviRHAztmmrGqpoApgMnJyepzfdKSM043LFYz9RvgnwPeAFzc/v6XQ6tIaggvXmnp/Ee257HbZmHMGeBJNgHrgFVJpoH30gruTyV5E3AX8OujLFLS0tUd1l7Ys3DmDPCqOneGSScPuRZJ0jx4JaYkNZQBLkkNZYBLUkMZ4JLUUAa4JDWUAS5JDWWAS1JDGeCS1FAGuCQ1lAEuSQ1lgEtSQxngktRQ/Q4nKy0LExNTbN/+AOCY31p6DHBpFtu3P0DVhsUuQ9onA1zSyHR/gvFGD8NlgEsamc5PMN7oYfg8iSlJDeURuNTFE5dqioECPMm/B94MFHALcGFV7R5GYdJC6g5tT1yqCfoO8CTHAP8WeGFVPZLkU8A5wMYh1SYtGHubqIkGbQPfHzg4yf7AIcC9g5ckSepF3wFeVfcAlwB3AfcBf19V1w2rMEnS7PoO8CRPBc4GjgOeCaxIct4+5lufZGuSrTt37uy/UknSkwzShPJa4PtVtbOqHgWuAn65e6aqmqqqyaqaXL169QCrkyR1GiTA7wJenuSQJAFOBm4bTlmSpLkM0gb+DeDTwLdodSF8CjA1pLokSXMYqB94Vb0XeO+QapEkzYOX0ktSQ3kpvaShOvbYw54YuMqhCEbLAJc0VA4Zu3BsQpGkhjLAJamhbELRsuWwsWo6A1zLliMQqulsQpGkhjLAJamhDHBJaijbwCUtiO4LfOwvPjgDXNKC6AzsPUGuwRjgWlbsOqhxYoBrWbHroMaJJzElqaEMcElqKANckhrKAJekhhoowJMckeTTSW5PcluSXxpWYZKk2Q3aC+XDwOer6nVJDgQOGUJNkqQe9B3gSQ4DXglcAFBVPwV+OpyyJElzGaQJ5XhgJ/BnSf53ko8lWTGkuiRJcxgkwPcHXgb8SVW9FHgYeGf3TEnWJ9maZOvOnTsHWJ0kqdMgAT4NTFfVN9qPP00r0J+kqqaqarKqJlevXj3A6iRJnfoO8Kr6AXB3kue1nzoZ+O5QqpIkzWnQXihvAT7Z7oFyJ3Dh4CVJknoxUIBX1U3A5JBqkQbSPdKg401r3DkaocZG50iDjjet5cBL6SWpoTwC11jy9l1aDgxwjSVv36XlwCYUSWooA1ySGsoAl6SGMsAlqaE8iamx190jRYvPXkLDYYBr7BkOS0/n72RiYsow75MBLmlR2eWzfwa4Gq17/BNpOTHA1Wid459Iy429UCSpoQxwSWooA1ySGsoAl6SGMsAlqaEMcElqqIG7ESbZD9gK3FNVZwxekjQ7+35LLcPoB/5W4DbAvyQtCPt+Sy0DNaEkWQP8M+BjwylHktSrQY/APwS8A1g50wxJ1gPrAdauXTvg6iSNM0cpnJ++AzzJGcCOqroxybqZ5quqKWAKYHJysvpdn6Tx58BW8zNIE8orgLOSbAOuBF6T5BNDqUqSNKe+A7yq3lVVa6pqAjgH+FJVnTe0yiRJs3I0QjWCXQelvQ0lwKtqC7BlGK+l5a07qPe0idp1UNqbR+BaUjqD2pNY0uy8lF6SGsoAl6SGsglF0pLkRT1zM8AlLUle1DM3m1AkqaEMcElqKJtQJC15tofvmwEuacmzPXzfbEKRpIbyCFxSo9ic8nMGuKRGsTnl52xCkaSGMsAlqaEMcElqKNvAtWR1n6yS9GQGuJas5dy7QOqFTSiS1FB9H4EneRZwOfAM4HFgqqo+PKzCtHx4v0upP4M0oTwGvK2qvpVkJXBjks1V9d0h1aZlwvtdSv3puwmlqu6rqm+1f34QuA04ZliFSZJmN5STmEkmgJcC39jHtPXAeoC1a9cOY3UaAzabSIMbOMCTHAp8Bvh3VfVA9/SqmgKmACYnJ2vQ9Wk82GwiDW6gXihJDqAV3p+sqquGU5IkqRd9B3iSAB8HbquqPxpeSZKkXgxyBP4K4HzgNUluan+dPqS6JElz6LsNvKr+B5Ah1iJJmgevxJSkhnIsFI1UZ3fBTnYdlAZngGuk7C6oUeocsbL7+eUwGJoBLqmxZgrp5XKrNQNcQ+dVltLCMMA1dDabSAvDXiiS1FAegatv3U0ly+GkkZqh+3Z847pvGuDqW2dTyXI5aaRm6Azscd43DXANhTcglhaeAa459dJUMq4fUTVexq3ZzwDXnGwq0bgYt33ZXiiS1FAegWtebOtW08y0z45DTxUDXMDMg07Bk3f6Ju7kWt5m2mfHoaeKAS7Aqye1vDX1aNwAXwZmG9K1KTuqNEpNPRo3wJeBmY6um7SjSothqR/8DBTgSU4FPgzsB3ysqi4eSlUaWC8jAnpCUtpb99/FUj746TvAk+wH/DfgFGAa+GaSz1XVd4dV3HIxrIsLul9nrjbtpXAEIS01Tfq7GOQI/B8Dd1TVnQBJrgTOBkYS4Ev9CqpePmrNNs+esJ2YmOr7v3svoS1pcL3cCaj7730UuZWq6m/B5HXAqVX15vbj84GTquq3u+ZbD+yp+nnA97peahVwf19FjC+3yd7cJntzm+xtXLfJsVW1uvvJQY7As4/n9vpvUFVTwNSML5JsrarJAeoYO26TvblN9uY22dty2yaDXEo/DTyr4/Ea4N7BypEk9WqQAP8m8JwkxyU5EDgH+NxwypIkzaXvJpSqeizJbwNfoNWN8LKq+k4fLzVj88oy5jbZm9tkb26TvS2rbdL3SUxJ0uJyOFlJaigDXJIaamQBnuTUJN9LckeSd84wz7okNyX5TpKvzGfZJhpwm2xLckt72taFq3q05tomSd7efs83Jbk1yc+SPK2XZZtqwG0ylvsJ9LRdDk/yV0m+3f77ubDXZRurqob+Reuk5v8FjgcOBL4NvLBrniNoXbW5tv34qF6XbeLXINuk/fM2YNViv4+F3iZd858JfGm57yczbZNx3U963S7Au4Hfb/+8GvhRe96x3FeqamRH4E9cZl9VPwX2XGbf6V8CV1XVXQBVtWMeyzbRINtkXM33d30usKnPZZtikG0yznrZLgWsTBLgUFoB/liPyzbSqAL8GODujsfT7ec6PRd4apItSW5M8q/msWwTDbJNoLVzXtd+fmkNBNO/nn/XSQ4BTgU+M99lG2aQbQLjuZ9Ab9vlUuAFtC4ovAV4a1U93uOyjTSq8cB7ucx+f+AfAScDBwNfT3JDj8s2Ud/bpKr+FnhFVd2b5Chgc5Lbq+r60ZY8cvP5XZ8J/M+q+lEfyzbJINsExnM/gd62y68CNwGvAZ5N6/1/tcdlG2lUR+C9XGY/DXy+qh6uqvuB64F/2OOyTTTINqGq7m1/3wF8ltbHwqabz+/6HJ7cVLCc95M9urfJuO4n0Nt2uZBWE2RV1R3A94Hn97hsM43ohMP+wJ3Acfz8pMGLuuZ5AfDF9ryHALcCL+5l2SZ+DbhNVgAr2/OsAL5GayTIRX9fo94m7fkOp9WeuWK+yzbta8BtMpb7Sa/bBfgT4H3tn58O3ENrdMKx3FeqajRNKDXDZfZJLmpP/2hV3Zbk88DNwOO07uhzK8CQLtFfUgbZJkmOBz7bOjfD/sBfVNXnF+edDE8v26Q96z8Hrquqh+dadmHfwfANsk1ohdbY7SfQ83Z5P7AxyS20mk1+p1qfZMcyU8BL6SWpsbwSU5IaygCXpIYywCWpoQxwSWooA1ySGsoAl6SGMsAlqaH+P30+JCbWznL5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(risk_mle, bins=100, label='MLE', density=True, histtype='step', color='darkblue')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Risk estimates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrizing and fitting log-linear models\n",
    "\n",
    "A log-linear model $M_S$ has density of the form:\n",
    "\n",
    "$$ \\log f(x) = \\sum_{A \\subset S} \\psi_A(x) $$\n",
    "\n",
    "where $\\psi_A$ depends on $x_i$ only if $i \\in A$.  Since x = $(x_1, \\dots, x_k)$ has its components with values $x_i \\in \\{ 0, 1 \\}$, we can express that as\n",
    "\n",
    "$$ \\psi_A(x) = \\sum_{B \\subset A} c_{B, A} g_A(x) = \\sum_{B \\subset A} c_{B, A} \\left( \\prod_{j \\in B} x_j \\right) $$\n",
    "\n",
    "for some constants $c_{B, A}$, and where $g_A(x) = \\prod_{j \\in A} x_j$. We can then write the log-likelihood for a parameter $\\beta = \\left\\{ \\beta_A | A \\subset S \\right\\}$ as the sum of the log density evaluated at each observation:\n",
    "\n",
    "$$ \\ell(\\beta) = \\sum_{x \\in \\text{obs}} \\log f(x; \\beta) =  \\sum_{x \\in \\text{obs}} \\sum_{A \\subset S} \\sum_{B \\subset A} c_{B, A} g_A(x) = \\sum_{x \\in \\text{obs}} \\sum_{A \\subset S} \\beta_A g_A(x) = \\sum_{A \\subset S} \\beta_A \\sum_{x \\in \\text{obs}} g_A(x) $$\n",
    "\n",
    "We will want to find the MLE $\\hat{\\beta}$ subject to the constraint that $f(x)$ is a probability density function which adds up to one, that is, $\\sum_{x \\in \\Omega} f(x) = 1$ over the random variable universe $\\Omega = \\{ 0, 1 \\}^k$.\n",
    "\n",
    "We can now explicitly frame this as an optimization problem, with target being the maximization of the log likelihood, and a nonlinear equality constraint that the density function adds up to 1:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\text{argmax}_\\beta \\left\\{ \\sum_{A \\subset S} \\beta_A \\sum_{x \\in \\text{obs}} g_A(x) \n",
    "\\;\\Bigg|\\;  \\sum_{x \\in \\Omega} \\exp \\left( \\sum_{A \\subset S} \\beta_A g_A(x) \\right) = 1\n",
    "\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix representation\n",
    "\n",
    "We can use matrix multiplications to speed up the function and constraint evaluations during optimization.  If $X$ is a matrix of $n$ samples by $k$ dimensions, containing $n$ observations, we define the matrix $h_S(X)$, where\n",
    "\n",
    "$$ (h_S(X))_{i, j} = \\begin{cases}\n",
    "1 & \\text{if } X_{r}^j = 1 \\text{ for all } r \\in A \\text{, where the } j \\text{-th element of } X \\text{ is } X^j = (X^j_1, \\dots, X^j_k) \\text{ and } A \\text{ is the } i \\text{-th subset of } S \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "By definition, the log-likelihood then becomes the sum of the elements of the matrix product $\\beta \\cdot h_S(X_\\text{obs})$:\n",
    "\n",
    "$$ \\ell(\\beta) = \\sum_{A \\in S} \\sum_{x \\in \\text{obs}} \\beta_A g_A(x) = \\sum_i \\left( \\beta \\cdot h_S(X_\\text{obs}) \\right)_i $$\n",
    "\n",
    "The constraint can also be computed using a matrix multiplication, first defining a matrix $X_\\Omega$ of shape $(2^k, k)$ that contains one row for each possible observation:\n",
    "\n",
    "$$ (X_\\Omega)_{i, j} = \\begin{cases}\n",
    "1 & \\text{if the } j \\text{-th digit in the binary representation of } i \\text{ is } 1 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Then the sum of the density functions is the sum of the element-wise exponentials resulting from a matrix product $\\beta \\cdot h_S(X_\\Omega)$:\n",
    "\n",
    "$$ \\sum_{x \\in \\Omega} f(x) = \\sum_{x \\in \\Omega} \\exp \\left(\\sum_{A \\subset S} \\beta_A g_A(x) \\right) \n",
    "= \\sum_{i} \\exp \\left( \\beta \\cdot h_S(X_\\Omega) \\right)_i$$\n",
    "\n",
    "Now, after pre-computing $h_S(X_\\text{obs})$ and $h_S(X_\\Omega)$, an optimizer will only need to perform sums, matrix multiplications and exponentiations to compute the target function and constraint penalty at each step.\n",
    "\n",
    "Finally, note that $\\sum_{x \\in \\Omega} = 1$ if and only if $\\log \\sum_{x \\in \\Omega} f(x) = 0$, so we can use scipy's logsumexp function to represent the total probability density function constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection and bootstraping\n",
    "\n",
    "**Model selection** is to be performed using forward selection with AIC or BIC scores.  This means that, for each observation set $X$, given a scoring process to pick between models, we will:\n",
    "\n",
    "1. Fit the uniform model $M_\\varnothing$;\n",
    "2. Given the current best model $M_{S_0}$, fit all models $M_{S}$ where $S = S_0 \\cup \\{ i \\}$ for each $i$ not in $S_0$;\n",
    "3. If a new model was found as an improvement, select it as the new best model and return to step 2.  Otherwise, keep the current best model and stop.\n",
    "\n",
    "**AIC** will be computed as the log likelihood of the candidate model at its MLE minus its number of parameters:\n",
    "\n",
    "$$ \\text{AIC}(M_S) = \\hat{\\ell}_S - |S| $$\n",
    "\n",
    "**BIC** will be computed as the log likelihood of the candidate model at its MLE minus the BIC penalty on this formulation (half times number of parameters times log sample size):\n",
    "\n",
    "$$ \\text{BIC}(M_S) = \\hat{\\ell}_S - \\frac{1}{2} |S| \\log n $$\n",
    "\n",
    "Once a model is selected, we can compute its loss $D(f, \\hat{f})$, the KL divergence over the universe of observations for the fitted density function.\n",
    "\n",
    "**Bootstraping** means repeating this process multiple times, observing the loss at each point and reporting its average and distribution.  Note that the model selection itself is a part of the bootstraping process -- different model formulations could potentially be chosen for different observations, even though they come from the same synthetic data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def get_loglinear_mle(X, S):\n",
    "    \"\"\"\n",
    "    Estimates a loglinear model for observations X given variables S via maximizing the likelihood estimator.\n",
    "    \n",
    "    Args:\n",
    "       X:  2D array,  shape (n, k), of samples and observations (0 or 1)\n",
    "       S:  iterable of variables between 0 and k-1 inclusive\n",
    "       \n",
    "    Returns:\n",
    "       beta_hat:  1D array of size 2**|S| with estimated parameters via MLE\n",
    "       log_likelihood:  Value of the log-likelihood of the model estimated at the MLE\n",
    "    \"\"\"\n",
    "    k = X.shape[1]\n",
    "    \n",
    "    # Assert values in S are valid\n",
    "    for j in S:\n",
    "        assert 0 <= j, 'Invalid value in S: must be non-negative, was ' + (j)\n",
    "        assert j < k, 'Invalid value in S: must be less than than ' + str(k) + ', was ' + str(j)\n",
    "        \n",
    "    # Assert values in S are unique\n",
    "    assert len(S) == len(set(S)), 'Values in S must be unique'\n",
    "        \n",
    "    # List all potential values in A \\subset S\n",
    "    subsets = [list(s) for s in powerset(S)]\n",
    "    n_subsets = len(subsets)\n",
    "    \n",
    "    # Speed up calculation of gA operations with vector operations\n",
    "    def get_h(XX, subsets):\n",
    "        \"\"\"\n",
    "        Calculate the matrix gA(XX) of shape (2**|S|, XX.shape[0]), where\n",
    "        gA(XX){i, j} = 1 if all elements of the i-th subset A of S are in the j-th sample of XX\n",
    "                       0 otherwise\n",
    "        \"\"\"\n",
    "        h = np.zeros(shape=(len(subsets), XX.shape[0]), dtype=int)\n",
    "        for i, A in enumerate(subsets):\n",
    "            h[i] = XX[:, A].all(axis = 1)\n",
    "            \n",
    "        return h\n",
    "    \n",
    "    h_obs = get_h(X, subsets)\n",
    "    h_universe = get_h(generate_universe(k), subsets)\n",
    "    \n",
    "    def neg_log_likelihood(beta):\n",
    "        return -np.sum(beta @ h_obs)\n",
    "    \n",
    "    def log_density_sum(beta):\n",
    "        # Use scipy's logsumexp to avoid overflows\n",
    "        # exp(sum(beta @ h_universe)) - 1 == 0 iff logsumexp(beta @ h_universe) == 0\n",
    "        return logsumexp(beta @ h_universe)\n",
    "    \n",
    "    # Constraint: PDF adds up to 1\n",
    "    pdf_constraint = {'type': 'eq', 'fun': log_density_sum}\n",
    "    \n",
    "    # Get initial guess: all values zero other than first\n",
    "    beta0 = np.zeros(len(subsets))\n",
    "    beta0[0] = -k * np.log(2)\n",
    "    \n",
    "    res = minimize(neg_log_likelihood, beta0,\n",
    "                   constraints=[pdf_constraint],\n",
    "                   options={'ftol': 1e-18, 'disp': False, 'maxiter': 1000})\n",
    "    beta_hat = res.x\n",
    "    log_likelihood = -res.fun\n",
    "    \n",
    "    return beta_hat, log_likelihood\n",
    "\n",
    "\n",
    "def f_loglinear(S, beta):\n",
    "    \"\"\"\n",
    "    Computes the density function for a given set of variables S and corresponding parameters beta.\n",
    "    \n",
    "    f(x) = exp ( \\sum_{A in S} \\beta(A) * g_A(x) )\n",
    "    \"\"\"\n",
    "    # Compute the powerset of S only once and save it in the environment\n",
    "    subsets = [list(s) for s in powerset(S)]\n",
    "    \n",
    "    def f(x):\n",
    "        return np.exp(np.sum([x[A].all() * beta[i] for i, A in enumerate(subsets)]))\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def get_AIC(X, S):\n",
    "    \"\"\" \n",
    "    Calculates AIC using the loglinear model log likelihood function.\n",
    "    \n",
    "    Args:\n",
    "       X:    2D array (n, k), observed data for log-ikelihood function\n",
    "       S:    iterable, list of variables for log-likelihood function\n",
    "       \n",
    "    Returns:\n",
    "       AIC score for the given submodel:  ll - |S|\n",
    "    \"\"\"\n",
    "    _, log_likelihood = get_loglinear_mle(X, S)\n",
    "    penalty = len(S)\n",
    "    \n",
    "    return log_likelihood - penalty\n",
    "\n",
    "\n",
    "def get_BIC(X, S):\n",
    "    \"\"\" \n",
    "    Calculates BIC using the loglinear model log likelihood function.\n",
    "    \n",
    "    Args:\n",
    "       X:     2D array (n, k), observed data for log-ikelihood function\n",
    "       S:     iterable, list of variables for log-likelihood function\n",
    "       \n",
    "    Returns:\n",
    "       BIC score for the given submodel:  ll - (|S| log n) / 2\n",
    "    \"\"\"\n",
    "    _, log_likelihood = get_loglinear_mle(X, S)\n",
    "    n = X.shape[0]\n",
    "    penalty = len(S) * np.log(n) / 2\n",
    "    \n",
    "    return log_likelihood - penalty\n",
    "\n",
    "\n",
    "def forward_selection(score_func, S):\n",
    "    \"\"\"\n",
    "    Uses forward selection to select a subset A of S, in a search to maximize score_func(A).\n",
    "\n",
    "    Args:\n",
    "       score_func:  (A) => score, a function to score subsets\n",
    "       S:           iterable to select a subset from\n",
    "       \n",
    "    Returns:\n",
    "       A:           a subset of S resulting from forward selection\n",
    "       \n",
    "    \"\"\"\n",
    "    current_subset = []\n",
    "    current_score = score_func([])\n",
    "    \n",
    "    while True:\n",
    "        best_subset, best_score = current_subset, current_score\n",
    "        improved = False\n",
    "        for s in S:\n",
    "            if s not in current_subset:\n",
    "                candidate_subset = current_subset.copy()\n",
    "                candidate_subset.append(s)\n",
    "                candidate_subset.sort()\n",
    "                candidate_score = score_func(candidate_subset)\n",
    "                \n",
    "                if candidate_score > best_score:\n",
    "                    improved = True\n",
    "                    best_subset, best_score = candidate_subset, candidate_score\n",
    "        \n",
    "        if not improved:\n",
    "            break\n",
    "        current_subset, current_score = best_subset, best_score\n",
    "    \n",
    "    return current_subset               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71767354b7ef4e0dacc7dc1700803658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AIC bootstrapping\n",
    "\n",
    "B = 100\n",
    "\n",
    "n = 1000\n",
    "k = 7\n",
    "risk_aic = np.empty(B)\n",
    "S_full = [i for i in range(k)]\n",
    "X_universe = generate_universe(k=7)\n",
    "for i in tqdm_notebook(range(B)):\n",
    "    XX = generate_samples(n=n, k=k)    \n",
    "    score_func = lambda S: get_AIC(XX, S)\n",
    "    best_subset = forward_selection(score_func, S_full)\n",
    "    beta_hat, _ = get_loglinear_mle(XX, best_subset)\n",
    "    f_hat = f_loglinear(best_subset, beta_hat)\n",
    "    risk_aic[i] = KL_divergence(true_density, f_hat, X_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ba58349670464e9017d7cd56076361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BIC bootstrapping\n",
    "\n",
    "B = 100\n",
    "\n",
    "n = 1000\n",
    "k = 7\n",
    "risk_bic = np.empty(B)\n",
    "S_full = [i for i in range(k)]\n",
    "X_universe = generate_universe(k=7)\n",
    "for i in tqdm_notebook(range(B)):\n",
    "    XX = generate_samples(n=n, k=k)    \n",
    "    score_func = lambda S: get_BIC(XX, S)\n",
    "    best_subset = forward_selection(score_func, S_full)\n",
    "    beta_hat, _ = get_loglinear_mle(XX, best_subset)\n",
    "    f_hat = f_loglinear(best_subset, beta_hat)\n",
    "    risk_bic[i] = KL_divergence(true_density, f_hat, X_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE mean: 0.747\n",
      "AIC mean: 4.143\n",
      "BIC mean: 0.655\n"
     ]
    }
   ],
   "source": [
    "print('MLE mean: %.3f' % risk_mle.mean())\n",
    "print('AIC mean: %.3f' % risk_aic.mean())\n",
    "print('BIC mean: %.3f' % risk_bic.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the better risk is obtained by the BIC, followed by the MLE and the AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
