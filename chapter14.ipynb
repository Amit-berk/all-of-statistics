{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression** is a method for studying the relationship between a **response variable** $Y$ and a **covariates** $X$.  The covariate is also called a **predictor variable** or **feature**.  Later we will generalize and allow for more than one covariate.  The data are of the form\n",
    "\n",
    "$$ (Y_1, X_1), \\dots, (Y_n, X_n) $$\n",
    "\n",
    "One way to summarize the relationship between $X$ and $Y$ is through the **regression function**\n",
    "\n",
    "$$ r(x) = \\mathbb{E}(Y | X = x) = \\int y f(y | x) dy $$\n",
    "\n",
    "Most of this chapter is concerned with estimating the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest version of regression is when $X_i$ is simple (a scalar, not a vector) and $r(x)$ is assumed to be linear:\n",
    "\n",
    "$$r(x) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "This model is called the **simple linear regression model**.  Let $\\epsilon_i = Y_i - (\\beta_0 + \\beta_1 X_i)$.  Then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(\\epsilon_i | Y_i) &= \\mathbb{E}(Y_i - (\\beta_0 + \\beta_1 X_i) | X_i)\\\\\n",
    "&= \\mathbb{E}(Y_i | X_i) - (\\beta_0 + \\beta_1 X_i)\\\\\n",
    "&= r(X_i) - (\\beta_0 + \\beta_1 X_i)\\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let $\\sigma^2(x) = \\mathbb{V}(\\epsilon_i | X_i = x)$.  We will make the further simplifying assumption that $\\sigma^2(x) = \\sigma^2$ does not depend on $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Linear Regression Model**\n",
    "\n",
    "$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\n",
    "\n",
    "where $\\mathbb{E}(\\epsilon_i | X_i) = 0$ and $\\mathbb{V}(\\epsilon_i | X_i) = \\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unknown models in the parameter are the intercept $\\beta_0$, the slope $\\beta_1$ and the variance $\\sigma^2$.  Let $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ denote the estimates of $\\beta_0$ and $\\beta_1$.  The **fitted line** is defined to be\n",
    "\n",
    "$$\\hat{r}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **predicted values** or **fitted values** are $\\hat{Y}_i = \\hat{r}(X_i)$ and the **residuals** are defined to be\n",
    "\n",
    "$$\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **residual sum of squares** or RSS is defined by\n",
    "\n",
    "$$ \\text{RSS} = \\sum_{i=1}^n \\hat{\\epsilon}_i^2$$\n",
    "\n",
    "The quantity RSS measures how well the fitted line fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **least squares estimates** are the values $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize $\\text{RSS} = \\sum_{i=1}^n \\hat{\\epsilon}_i^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.4**.  The least square estimates are given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (X_i - \\overline{X}_n) (Y_i - \\overline{Y}_n)}{\\sum_{i=1}^n (X_i - \\overline{X}_n)^2}\\\\\n",
    "\\hat{\\beta}_0 &= \\overline{Y}_n - \\hat{\\beta}_1 \\overline{X}_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "An unbiased estimate of $\\sigma^2$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\left( \\frac{1}{n - 2} \\right) \\sum_{i=1}^n \\hat{\\epsilon}_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Least Squares and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we add the assumption that $\\epsilon_i | X_i \\sim N(0, \\sigma^2)$, that is,\n",
    "\n",
    "$Y_i | X_i \\sim N(\\mu_i, \\sigma_i^2)$\n",
    "\n",
    "where $\\mu_i = \\beta_0 + \\beta_i X_i$.  The likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_{i=1}n f(X_i, Y_i) &= \\prod_{i=1}^n f_X(X_i) f_{Y|X}(Y_i | X_i)\\\\\n",
    "&= \\prod_{i=1}^n f_X(X_i) \\times \\prod_{i=1}^n f_{Y|X}(Y_i | X_i) \\\\\n",
    "&= \\mathcal{L}_1 \\times \\mathcal{L}_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}_1 = \\prod_{i=1}^n f_X(X_i)$ and $\\mathcal{L}_2 = \\prod_{i=1}^n f_{Y|X}(Y_i | X_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $\\mathcal{L}_1$ does not involve the parameters $\\beta_0$ and $\\beta_1$.  We shall focus on the second term $\\mathcal{L}_2$ which is called the **conditional likelihood**, given by\n",
    "\n",
    "$$\\mathcal{L}_2 \\equiv \\mathcal{L}(\\beta_0, \\beta_1, \\sigma)\n",
    "= \\prod_{i=1}^n f_{Y|X}(Y_i | X_i)\n",
    "\\propto \\sigma^{-n} \\exp \\left\\{ - \\frac{1}{2 \\sigma^2} \\sum_i (Y_i - \\mu_i)^2 \\right\\}\n",
    "$$\n",
    "\n",
    "The conditional log-likelihood is\n",
    "\n",
    "$$\\ell(\\beta_0, \\beta_1, \\sigma) = -n \\log \\sigma - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n \\left(Y_i - (\\beta_0 + \\beta_1 X_i) \\right)^2$$\n",
    "\n",
    "To find the MLE of $(\\beta_0, \\beta_1)$ we maximize the conditional log likelihood. We can see from the equation above that this is the same as minimizing the RSS.  Therefore, we have shown the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.7**.  Under the assumption of Normality, the least squares estimator is also the maximum likelihood estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also maximize $\\ell(\\beta_0, \\beta_1, \\sigma)$ over $\\sigma$ yielding the MLE\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\epsilon}_i^2 $$\n",
    "\n",
    "This estimator is similar to, but not identical to, the unbiased estimator.  Common practice is to use the unbiased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 Properties of the Least Squares Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.8**.  Let $\\hat{\\beta}^T = (\\hat{\\beta}_0, \\hat{\\beta}_1)^T$ denote the least squares estimators.  Then,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\beta} | X^n) = \\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(\\hat{\\beta} | X^n) = \\frac{\\sigma^2}{n s_X^2} \\begin{pmatrix} \n",
    "\\frac{1}{n} \\sum_{i=1}^n X_i^2 & -\\overline{X}_n \\\\\n",
    "-\\overline{X}_n & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $s_X^2 = n^{-1} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated standard errors of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are obtained by taking the square roots of the corresponding diagonal terms of $\\mathbb{V}(\\hat{\\beta} | X^n)$ and inserting the estimate $\\hat{\\sigma}$ for $\\sigma$.  Thus,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_0) &= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\sqrt{\\frac{\\sum_{i=1}^n X_i^2}{n}}\\\\\n",
    "\\hat{\\text{se}}(\\hat{\\beta}_1) &= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We should write $\\hat{\\text{se}}(\\hat{\\beta}_0 | X^n)$ and $\\hat{\\text{se}}(\\hat{\\beta}_1 | X^n)$ but we will use the shorter notation $\\hat{\\text{se}}(\\hat{\\beta}_0)$ and $\\hat{\\text{se}}(\\hat{\\beta}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.9**. Under appropriate conditions we have:\n",
    "\n",
    "1. (Consistency) $\\hat{\\beta}_0 \\xrightarrow{\\text{P}} \\beta_0$ and $\\hat{\\beta}_1 \\xrightarrow{\\text{P}} \\beta_1$\n",
    "\n",
    "2. (Asymptotic Normality):\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_0 - \\beta_0}{\\hat{se}(\\hat{\\beta}_0)} \\leadsto N(0, 1)\n",
    "\\quad \\text{and} \\quad\n",
    "\\frac{\\hat{\\beta}_1 - \\beta_1}{\\hat{se}(\\hat{\\beta}_1)} \\leadsto N(0, 1)\n",
    "$$\n",
    "\n",
    "3. Approximate $1 - \\alpha$ confidence intervals for $\\beta_0$ and $\\beta_1$ are\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\beta}_0)\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\beta}_1 \\pm z_{\\alpha/2} \\hat{\\text{se}}(\\hat{\\beta}_1)\n",
    "$$\n",
    "\n",
    "The Wald statistic for testing $H_0 : \\beta_1 = 0$ versus $H_1: \\beta_1 \\neq 0$ is: reject $H_0$ if $W > z_{\\alpha / 2}$ where $W = \\hat{\\beta}_1 / \\hat{\\text{se}}(\\hat{\\beta}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have estimated a regression model $\\hat{r}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ from data $(X_1, Y_1), \\dots, (X_n, Y_n)$.  We observe the value $X_* = x$ of the covariate for a new subject and we want to predict the outcome $Y_*$.  An estimate of $Y_*$ is \n",
    "\n",
    "$$ \\hat{Y}_* = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_*$$\n",
    "\n",
    "Using the formula for the variance of the sum of two random variables,\n",
    "\n",
    "$$ \\mathbb{V}(\\hat{Y}_*) = \\mathbb{V}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_*) = \\mathbb{V}(\\hat{\\beta}_0) + x_* \\mathbb{V}(\\hat{\\beta}_1) + 2 x_* \\text{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) $$ \n",
    "\n",
    "Theorem 14.8 gives the formulas for all terms in this equation.  The estimated standard error $\\hat{\\text{se}}(\\hat{Y}_*)$ is the square root of this variance, with $\\hat{\\sigma}^2$ in place of $\\sigma^2$.  However, **the confidence interval for $\\hat{Y}_*$ is not of the usual form** $\\hat{Y}_* \\pm z_{\\alpha} \\hat{\\text{se}}(\\hat{Y}_*)$.  The appendix explains why.  The correct form is given in the following theorem.  We can the interval a **prediction interval**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 14.11 (Prediction Interval)**.  Let\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\xi}_n^2 &= \\hat{\\text{se}}^2(\\hat{Y}_*) + \\hat{\\sigma}^2 \\\\\n",
    "&= \\hat{\\sigma}^2 \\left(\\frac{\\sum_{i=1}^n (X_i - X_*)^2}{n \\sum_{i=1}^n (X_i - \\overline{X})^2} + 1 \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "An approximate $1 - \\alpha$ prediction interval for $Y_*$ is\n",
    "\n",
    "$$ \\hat{Y}_* \\pm z_{\\alpha/2} \\xi_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
