{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting a discrete variable $Y$ from another random variable $X$ is called **classfication**, **supervised learning**, **discrimination** or **pattern recognition**.\n",
    "\n",
    "In more detail, consider IID data $(X_1, Y_1), \\dots, (X_n, Y_n)$ where\n",
    "\n",
    "$$ X_i = (X_{i1}, \\dots, X_{id}) \\in \\mathcal{X} \\subset \\mathbb{R}^d $$\n",
    "\n",
    "is a $d$-dimensional vector and $Y_i$ takes values in some finite set $\\mathcal{Y}$.  A **classification rule** is a function $h : \\mathcal{X} \\rightarrow \\mathcal{Y} $.  When we observe a new $X$, we predict $Y$ to be $h(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth revisiting the vocabulary:\n",
    "\n",
    "| Statistics     | Computer Science    | Meaning                                      |\n",
    "|----------------|---------------------|----------------------------------------------|\n",
    "| classification | supervised learning | predicting a discrete $Y$ from $X$           |\n",
    "| data           | training sample     | $(X_1, Y_1), \\dots, (X_n, Y_n)$              |\n",
    "| covariates     | features            | the $X_i$'s                                  |\n",
    "| classifier     | hypothesis          | map $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ |\n",
    "| estimation     | learning            | finding a good classifier                    |\n",
    "\n",
    "In most cases with this chapter, we deal with the case $\\mathcal{Y} = \\{ 0, 1 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.2 Error Rates and The Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **true error rate** of a classifier is \n",
    "\n",
    "$$ L(h) = \\mathbb{P}( \\{ h(X) \\neq Y\\} ) $$\n",
    "\n",
    "and the **empirical error rate** or **training error rate** is\n",
    "\n",
    "$$ \\hat{L}_n(h) = \\frac{1}{n} \\sum_{i=1}^n I(h(X_i) \\neq Y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the special case where $\\mathcal{Y} = \\{0, 1\\}$.  Let\n",
    "\n",
    "$$ r(x) = \\frac{\\pi f_1(x)}{\\pi f_1(x) + (1 - \\pi) f_0(x)} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ f_0(x) = f(x | Y = 0)\n",
    "\\quad \\text{and} \\quad\n",
    "f_1(x) = f(x | Y = 1)$$\n",
    "\n",
    "and $\\pi = \\mathbb{P}(Y = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bayes classification rule** $h^*$ is defined to be\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } r(x) > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The set $\\mathcal{D}(h) = \\{ x : \\mathbb{P}(Y = 1 | X = x) = \\mathbb{P}(Y = 0 | X = x) \\}$ is called the **decision boundary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the Bayes rule has nothing to do with Bayesian inference.  We could estimate the Bayes rule using either frequentist or Bayesian methods.\n",
    "\n",
    "The Bayes rule may be written in several different forms:\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } \\mathbb{P}(Y = 1 | X = x) > \\mathbb{P}(Y = 0 | X  = x)\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } \\pi f_1(x) > (1 - \\pi) f_0(x) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.5**.  The Bayes rule is optimal, that is, if $h$ is any classification rule then $L(h^*) \\leq L(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes rule depends on unknown quantities so we need to use the data to find some approximation to the Bayes rule.  At the risk of oversimplifying, there are three main approaches:\n",
    "\n",
    "1. **Empirical Risk Maximization**.  Choose a set of classifiers $\\mathcal{H}$ and find $\\hat{h} \\in \\mathcal{H}$ that minimizes some estimate of $L(h)$.\n",
    "\n",
    "2. **Regression**.  Find an estimate $\\hat{r}$ of the regression function $r$ and define\n",
    "\n",
    "$$ \n",
    "\\hat{h}(x) = \\begin{cases}\n",
    "1 & \\text{if } \\hat{r} > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. **Density Estimation**.  Estimate $f_0$ from the $X_i$'s for which $Y_i = 0$, estimate $f_1$ from the $X_i$'s for which $Y_i = 1$, and let $\\hat{\\pi} = n^{-1} \\sum_{i=1}^n Y_i$.  Define\n",
    "\n",
    "$$ \\hat{r}(x) = \\hat{\\mathbb{P}}(Y = 1 | X = x) = \\frac{\\hat{\\pi} \\hat{f}_1(x)}{\\hat{\\pi} \\hat{f}_1(x) + (1 - \\hat{\\pi}) \\hat{f}_0(x)} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \n",
    "\\hat{h}(x) = \\begin{cases}\n",
    "1 & \\text{if } \\hat{r} > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to generalize to the case where $Y$ takes more than two values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.6**.  Suppose that $Y \\in \\mathcal{Y} = \\{ 1, \\dots, K \\}$.  The optimal rule is\n",
    "\n",
    "$$ h(x) = \\text{argmax}_h \\mathbb{P}(Y = k | X = x) = \\text{argmax}_h \\pi_k f_k(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\mathbb{P}(Y = k | X = x) = \\frac{f_k(x) \\pi_k}{\\sum_r f_r(x) \\pi_r} $$\n",
    "\n",
    "and $ \\pi_r = \\mathbb{P}(Y = r)$, $f_r(x) = f(x | Y = r)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.3 Gaussian and Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest approach to classification is to use the density estimation strategy and assume a parametric model for the densities.  Suppose that $\\mathcal{Y} = \\{ 0, 1 \\}$ and that $f_0(x) = f(x | Y = 0)$ and $f_1(x) = f(x | Y = 1)$ are both multivariate Gaussians:\n",
    "\n",
    "$$ f_k(x) = \\frac{1}{(2\\pi)^{d/2} | \\Sigma_k |^{1/2}} \\exp \\left\\{ -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) \\right\\}, \\quad k = 0, 1$$\n",
    "\n",
    "Thus, $X | Y = 0 \\sim N(\\mu_0, \\Sigma_0)$ and $X | Y = 1 \\sim N(\\mu_1, \\Sigma_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.7**.  If $X | Y = 0 \\sim N(\\mu_0, \\Sigma_0)$ and $X | Y = 1 \\sim N(\\mu_1, \\Sigma_1)$, then the Bayes rule is\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } r_1^2 < r_0^2 + 2 \\log \\left( \\frac{\\pi_1}{\\pi_0} \\right) + \\log \\left( \\frac{| \\Sigma_0 | }{ | \\Sigma_1| }\n",
    "\\right) \\\\\n",
    "0 & \\text{otherwise} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ r_i^2 = (x - \\mu_i)^T \\Sigma_i^{-1}(x - \\mu_i), \\quad i = 1, 2 $$\n",
    "\n",
    "is the **Manalahobis distance**.  An equivalent way of expressing Bayes' rule is \n",
    "\n",
    "$$ h(x) = \\text{argmax}_k \\delta_k(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\delta_k(x) = -\\frac{1}{2} \\log | \\Sigma_k | - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k $$\n",
    "\n",
    "and $|A|$ denotes the determinant of matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary of the above classifier is quadratic so this procedure is often called **quadratic discriminant analysis (QDA)**.  In practice, we use sample estimates of $\\pi, \\mu_0, \\mu_1, \\Sigma_0, \\Sigma_1$ in place of the true value, namely:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\hat{\\pi}_0 = \\frac{1}{n} \\sum_{i=1}^n (1 - Y_i) & \\hat{\\pi}_1 = \\frac{1}{n} \\sum_{i=1}^n Y_i \\\\\n",
    "\\hat{\\mu}_0 = \\frac{1}{n_0} \\sum_{i: Y_i = 0} X_i & \\hat{\\mu}_1 = \\frac{1}{n_0} \\sum_{i: Y_i = 1} X_i \\\\\n",
    "S_0 = \\frac{1}{n_0} \\sum_{i: Y_i = 0} (X_i - \\hat{\\mu}_0) (X_i - \\hat{\\mu}_0)^T & \n",
    "S_1 = \\frac{1}{n_1} \\sum_{i: Y_i = 1} (X_i - \\hat{\\mu}_1) (X_i - \\hat{\\mu}_1)^T\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $n_0 = \\sum_i (1 - Y_i)$ and $n_1 = \\sum_i Y_i$ are the number of $Y_i$ variables equal to 0 or 1, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplification occurs if we assume $\\Sigma_0 = \\Sigma_1 = \\Sigma$.  In that case, the Bayes rule is\n",
    "\n",
    "$$ h(x) = \\text{argmax}_k \\delta_k(x) $$\n",
    "\n",
    "where now\n",
    "\n",
    "$$ \\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$\n",
    "\n",
    "The parameters are estimated as before, except the MLE of $\\Sigma$ now is\n",
    "\n",
    "$$ S = \\frac{n_0 S_0 + n_1 S_1}{n_0 + n_1} $$\n",
    "\n",
    "The classification rule is\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 &\\text{if } \\delta_1(x) > \\delta_0(x) \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\delta_j(x) = x^T S \\hat{\\mu}_j - \\frac{1}{2} \\hat{\\mu}_j^T S^{-1} \\hat{\\mu}_j + \\log \\hat{\\pi}_j $$\n",
    "\n",
    "is called the **discriminant function**.  The decision boundary $ \\{ x : \\delta_0(x) = \\delta_1(x) \\}$ is linear so this method is called **linear discrimination analysis (LDA)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generalize to the case where $Y$ takes on more than two values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.9**.  Suppose that $Y \\in \\{ 1, \\dots, K \\}$.  If $f_k(x) = f(x | Y = k)$ is Gaussian, the Bayes rule is\n",
    "\n",
    "$$ h(x) = \\text{argmax}_k \\delta_k(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\delta_k(x) = -\\frac{1}{2} \\log | \\Sigma_k | - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k $$\n",
    "\n",
    "If the variances of the Gaussians are equal then\n",
    "\n",
    "$$ \\delta_k(x) = x^T \\Sigma_{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate $\\delta_k(x)$ by inserting estimates of $\\mu_k$, $\\Sigma_k$, and $\\pi_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
