{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of predicting a discrete variable $Y$ from another random variable $X$ is called **classfication**, **supervised learning**, **discrimination** or **pattern recognition**.\n",
    "\n",
    "In more detail, consider IID data $(X_1, Y_1), \\dots, (X_n, Y_n)$ where\n",
    "\n",
    "$$ X_i = (X_{i1}, \\dots, X_{id}) \\in \\mathcal{X} \\subset \\mathbb{R}^d $$\n",
    "\n",
    "is a $d$-dimensional vector and $Y_i$ takes values in some finite set $\\mathcal{Y}$.  A **classification rule** is a function $h : \\mathcal{X} \\rightarrow \\mathcal{Y} $.  When we observe a new $X$, we predict $Y$ to be $h(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth revisiting the vocabulary:\n",
    "\n",
    "| Statistics     | Computer Science    | Meaning                                      |\n",
    "|----------------|---------------------|----------------------------------------------|\n",
    "| classification | supervised learning | predicting a discrete $Y$ from $X$           |\n",
    "| data           | training sample     | $(X_1, Y_1), \\dots, (X_n, Y_n)$              |\n",
    "| covariates     | features            | the $X_i$'s                                  |\n",
    "| classifier     | hypothesis          | map $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ |\n",
    "| estimation     | learning            | finding a good classifier                    |\n",
    "\n",
    "In most cases with this chapter, we deal with the case $\\mathcal{Y} = \\{ 0, 1 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.2 Error Rates and The Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **true error rate** of a classifier is \n",
    "\n",
    "$$ L(h) = \\mathbb{P}( \\{ h(X) \\neq Y\\} ) $$\n",
    "\n",
    "and the **empirical error rate** or **training error rate** is\n",
    "\n",
    "$$ \\hat{L}_n(h) = \\frac{1}{n} \\sum_{i=1}^n I(h(X_i) \\neq Y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the special case where $\\mathcal{Y} = \\{0, 1\\}$.  Let\n",
    "\n",
    "$$ r(x) = \\frac{\\pi f_1(x)}{\\pi f_1(x) + (1 - \\pi) f_0(x)} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ f_0(x) = f(x | Y = 0)\n",
    "\\quad \\text{and} \\quad\n",
    "f_1(x) = f(x | Y = 1)$$\n",
    "\n",
    "and $\\pi = \\mathbb{P}(Y = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bayes classification rule** $h^*$ is defined to be\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } r(x) > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The set $\\mathcal{D}(h) = \\{ x : \\mathbb{P}(Y = 1 | X = x) = \\mathbb{P}(Y = 0 | X = x) \\}$ is called the **decision boundary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the Bayes rule has nothing to do with Bayesian inference.  We could estimate the Bayes rule using either frequentist or Bayesian methods.\n",
    "\n",
    "The Bayes rule may be written in several different forms:\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } \\mathbb{P}(Y = 1 | X = x) > \\mathbb{P}(Y = 0 | X  = x)\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } \\pi f_1(x) > (1 - \\pi) f_0(x) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.5**.  The Bayes rule is optimal, that is, if $h$ is any classification rule then $L(h^*) \\leq L(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes rule depends on unknown quantities so we need to use the data to find some approximation to the Bayes rule.  At the risk of oversimplifying, there are three main approaches:\n",
    "\n",
    "1. **Empirical Risk Maximization**.  Choose a set of classifiers $\\mathcal{H}$ and find $\\hat{h} \\in \\mathcal{H}$ that minimizes some estimate of $L(h)$.\n",
    "\n",
    "2. **Regression**.  Find an estimate $\\hat{r}$ of the regression function $r$ and define\n",
    "\n",
    "$$ \n",
    "\\hat{h}(x) = \\begin{cases}\n",
    "1 & \\text{if } \\hat{r} > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. **Density Estimation**.  Estimate $f_0$ from the $X_i$'s for which $Y_i = 0$, estimate $f_1$ from the $X_i$'s for which $Y_i = 1$, and let $\\hat{\\pi} = n^{-1} \\sum_{i=1}^n Y_i$.  Define\n",
    "\n",
    "$$ \\hat{r}(x) = \\hat{\\mathbb{P}}(Y = 1 | X = x) = \\frac{\\hat{\\pi} \\hat{f}_1(x)}{\\hat{\\pi} \\hat{f}_1(x) + (1 - \\hat{\\pi}) \\hat{f}_0(x)} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \n",
    "\\hat{h}(x) = \\begin{cases}\n",
    "1 & \\text{if } \\hat{r} > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to generalize to the case where $Y$ takes more than two values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.6**.  Suppose that $Y \\in \\mathcal{Y} = \\{ 1, \\dots, K \\}$.  The optimal rule is\n",
    "\n",
    "$$ h(x) = \\text{argmax}_h \\mathbb{P}(Y = k | X = x) = \\text{argmax}_h \\pi_k f_k(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\mathbb{P}(Y = k | X = x) = \\frac{f_k(x) \\pi_k}{\\sum_r f_r(x) \\pi_r} $$\n",
    "\n",
    "and $ \\pi_r = \\mathbb{P}(Y = r)$, $f_r(x) = f(x | Y = r)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.3 Gaussian and Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest approach to classification is to use the density estimation strategy and assume a parametric model for the densities.  Suppose that $\\mathcal{Y} = \\{ 0, 1 \\}$ and that $f_0(x) = f(x | Y = 0)$ and $f_1(x) = f(x | Y = 1)$ are both multivariate Gaussians:\n",
    "\n",
    "$$ f_k(x) = \\frac{1}{(2\\pi)^{d/2} | \\Sigma_k |^{1/2}} \\exp \\left\\{ -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) \\right\\}, \\quad k = 0, 1$$\n",
    "\n",
    "Thus, $X | Y = 0 \\sim N(\\mu_0, \\Sigma_0)$ and $X | Y = 1 \\sim N(\\mu_1, \\Sigma_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.7**.  If $X | Y = 0 \\sim N(\\mu_0, \\Sigma_0)$ and $X | Y = 1 \\sim N(\\mu_1, \\Sigma_1)$, then the Bayes rule is\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{if } r_1^2 < r_0^2 + 2 \\log \\left( \\frac{\\pi_1}{\\pi_0} \\right) + \\log \\left( \\frac{| \\Sigma_0 | }{ | \\Sigma_1| }\n",
    "\\right) \\\\\n",
    "0 & \\text{otherwise} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ r_i^2 = (x - \\mu_i)^T \\Sigma_i^{-1}(x - \\mu_i), \\quad i = 1, 2 $$\n",
    "\n",
    "is the **Manalahobis distance**.  An equivalent way of expressing Bayes' rule is \n",
    "\n",
    "$$ h(x) = \\text{argmax}_k \\delta_k(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\delta_k(x) = -\\frac{1}{2} \\log | \\Sigma_k | - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k $$\n",
    "\n",
    "and $|A|$ denotes the determinant of matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary of the above classifier is quadratic so this procedure is often called **quadratic discriminant analysis (QDA)**.  In practice, we use sample estimates of $\\pi, \\mu_0, \\mu_1, \\Sigma_0, \\Sigma_1$ in place of the true value, namely:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\hat{\\pi}_0 = \\frac{1}{n} \\sum_{i=1}^n (1 - Y_i) & \\hat{\\pi}_1 = \\frac{1}{n} \\sum_{i=1}^n Y_i \\\\\n",
    "\\hat{\\mu}_0 = \\frac{1}{n_0} \\sum_{i: Y_i = 0} X_i & \\hat{\\mu}_1 = \\frac{1}{n_0} \\sum_{i: Y_i = 1} X_i \\\\\n",
    "S_0 = \\frac{1}{n_0} \\sum_{i: Y_i = 0} (X_i - \\hat{\\mu}_0) (X_i - \\hat{\\mu}_0)^T & \n",
    "S_1 = \\frac{1}{n_1} \\sum_{i: Y_i = 1} (X_i - \\hat{\\mu}_1) (X_i - \\hat{\\mu}_1)^T\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $n_0 = \\sum_i (1 - Y_i)$ and $n_1 = \\sum_i Y_i$ are the number of $Y_i$ variables equal to 0 or 1, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplification occurs if we assume $\\Sigma_0 = \\Sigma_1 = \\Sigma$.  In that case, the Bayes rule is\n",
    "\n",
    "$$ h(x) = \\text{argmax}_k \\delta_k(x) $$\n",
    "\n",
    "where now\n",
    "\n",
    "$$ \\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$\n",
    "\n",
    "The parameters are estimated as before, except the MLE of $\\Sigma$ now is\n",
    "\n",
    "$$ S = \\frac{n_0 S_0 + n_1 S_1}{n_0 + n_1} $$\n",
    "\n",
    "The classification rule is\n",
    "\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 &\\text{if } \\delta_1(x) > \\delta_0(x) \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\delta_j(x) = x^T S \\hat{\\mu}_j - \\frac{1}{2} \\hat{\\mu}_j^T S^{-1} \\hat{\\mu}_j + \\log \\hat{\\pi}_j $$\n",
    "\n",
    "is called the **discriminant function**.  The decision boundary $ \\{ x : \\delta_0(x) = \\delta_1(x) \\}$ is linear so this method is called **linear discrimination analysis (LDA)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generalize to the case where $Y$ takes on more than two values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.9**.  Suppose that $Y \\in \\{ 1, \\dots, K \\}$.  If $f_k(x) = f(x | Y = k)$ is Gaussian, the Bayes rule is\n",
    "\n",
    "$$ h(x) = \\text{argmax}_k \\delta_k(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\delta_k(x) = -\\frac{1}{2} \\log | \\Sigma_k | - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k $$\n",
    "\n",
    "If the variances of the Gaussians are equal then\n",
    "\n",
    "$$ \\delta_k(x) = x^T \\Sigma_{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate $\\delta_k(x)$ by inserting estimates of $\\mu_k$, $\\Sigma_k$, and $\\pi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another version of LDA due to Fisher.  The idea is to first reduce the dimension of the covariates to one dimension by projecting the data onto a line.  Algebraically, this means replacing the covariate $X = (X_1, \\dots, X_d)$ with a linear combination $U = w^T X = \\sum_{j=1}^d w_j X_j$.  The goal is to choose the vector $w = (w_1, \\dots, w_d)$ that \"best separates the data\".  Then we perform classification with the new covariate $U$ instead of $X$.\n",
    "\n",
    "We need to define what we mean by separation of the groups.  We would like the two groups to have means that are far apart relative to their spread.  Let $\\mu_j$ denote the mean of $X$ for $Y = j$ and let $\\Sigma$ be the variance matrix of $X$.  Then $\\mathbb{E}(U | Y = j) = \\mathbb{E}(w^T X | Y = j) = w^T \\mu_j$ and $\\mathbb{V}(U) = w^T \\Sigma w$.  Define the separation by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(w) &= \\frac{(\\mathbb{E}(U | Y = 0) - \\mathbb{E}(U | Y = 1))^2}{w^T \\Sigma w} \\\\\n",
    "&= \\frac{(w^T \\mu_0 - w^T \\mu_1)^2}{w^T \\Sigma w} \\\\\n",
    "&= \\frac{w^T (\\mu_0 - \\mu_1)(\\mu_0 - \\mu_1)^T w}{w^T \\Sigma w}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*The quantity $J$ arises in physics, where it is called the Rayleight coefficient.*\n",
    "\n",
    "We estimate $J$ as follows.  Let $n_j = \\sum_{i=1}^n I(Y_i = j)$ be the number of observations in group $j$, let $\\overline{X}_j = n_j^{-1} \\sum_{i: Y_i = j} X_j$ be the sample mean vetor of $X$'s for group $j$, and let $S_j = (n_j - 1)^{-1} \\sum_{i: Y_i = j} (X_i - \\overline{X}_j)(X_i - \\overline{X}_j)^T $ be the sample covariance matrix in group $j$.  Define\n",
    "\n",
    "$$ \\hat{J}(w) = \\frac{w^T S_B w}{w^T S_W w} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \n",
    "S_B = (\\overline{X}_0 - \\overline{X}_1) (\\overline{X}_0 - \\overline{X}_1)^T\n",
    "\\quad \\text{and} \\quad\n",
    "S_W = \\frac{(n_0 - 1) S_0 + (n_1 - 1) S_1}{(n_0 - 1) + (n_1 -1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 23.10**.  The vector\n",
    "\n",
    "$$ w = S_W^{-1}(\\overline{X}_0 - \\overline{X}_1) $$\n",
    "\n",
    "is a minimizer of $\\hat{J}(w)$.  We call\n",
    "\n",
    "$$ U = w^T X = (\\overline{X}_0 - \\overline{X}_1)^T S_W^{-1} X $$\n",
    "\n",
    "the **Fisher linear discriminant function**.  The midpoint $m$ between $\\overline{X}_0$ and $\\overline{X}_1$ is\n",
    "\n",
    "$$ m = \\frac{1}{2} (\\overline{X}_0 + \\overline{X}_1) = \\frac{1}{2}  (\\overline{X}_0 - \\overline{X}_1)^T S_B^{-1}  (\\overline{X}_0 + \\overline{X}_1)$$\n",
    "\n",
    "Fisher's classification rule is\n",
    "\n",
    "$$\n",
    "h(x) = \\begin{cases}\n",
    "0 & \\text{if } w^T X \\geq m \\\\\n",
    "1 & \\text{if } w^T X < m\n",
    "\\end{cases}\n",
    "= \\begin{cases}\n",
    "0 & \\text{if } (\\overline{X}_0 - \\overline{X}_1)^T S_W^{-1}x \\geq m \\\\\n",
    "1 & \\text{if } (\\overline{X}_0 - \\overline{X}_1)^T S_W^{-1}x < m\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Fisher's rule is the same as the Bayes linear classifier when $\\hat{\\pi} = 1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.4 Linear Regression and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more direct approach to classification is to estimate the regression function $r(x) = \\mathbb{E}(Y | X = x)$ without bothering to estimate the densities $f_k$.  For the rest of this section we will only consider the case where $\\mathcal{Y} = \\{ 0, 1 \\}$.  Thus, $r(x) = \\mathbb{P}(Y = 1 | X = x)$ and once we have an estimate $\\hat{r}$, we will use the classification rule\n",
    "\n",
    "$$ \n",
    "h(x) = \\begin{cases}\n",
    "1 &\\text{if } \\hat{r}(x) > \\frac{1}{2} \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "The simplest regression is the linear regression model\n",
    "\n",
    "$$ Y = r(x) + \\epsilon = \\beta_0 + \\sum_{j=1}^d \\beta_j X_j + \\epsilon $$\n",
    "\n",
    "where $\\mathbb{E}(\\epsilon) = 0$.  This model can't be correct since it doesn't force $Y \\in \\mathcal{Y}$.  Nonetheless, it can sometimes lead to a decent classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the least square estimate of $\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_d)^T$ minimizes the residual sum of squares\n",
    "\n",
    "$$ \\text{RSS}(\\beta) = \\sum_{i=1}^n \\left( Y_i - \\left( \\beta_0  + \\sum_{j=1}^d X_{ij} \\beta_j \\right) \\right)^2 $$\n",
    "\n",
    "Briefly reviewing this estimator:  let\n",
    "\n",
    "$$ \n",
    "X = \\begin{bmatrix}\n",
    "1 & X_{11} & \\cdots & X_{1d} \\\\\n",
    "1 & X_{21} & \\cdots & X_{2d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & X_{n1} & \\cdots & X_{nd}\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = (Y_1, \\dots, Y_n)^T\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ \\text{RSS}(\\beta) = (Y - X \\beta)^T (Y - X \\beta) $$\n",
    "\n",
    "and the model can be written as\n",
    "\n",
    "$$ Y = X \\beta + \\epsilon $$\n",
    "\n",
    "where $ \\epsilon = (\\epsilon_1, \\dots, \\epsilon_n)^T$.  The least squares solution $\\hat{\\beta}$ that minimizes RSS is given by\n",
    "\n",
    "$$ \\hat{\\beta} = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "and the predicted values are\n",
    "\n",
    "$$ \\hat{Y} = X \\hat{\\beta} $$\n",
    "\n",
    "Now we can use $h(x)$ to classify, by taking $\\hat{r}(x) = \\hat{\\beta}_0 + \\sum_k \\hat{\\beta}_k x_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems sensible to use a regression model that takes into account that $Y \\in \\{ 0, 1 \\}$.  The most common method for doing so is **logistic regression**.  Recall that the model is\n",
    "\n",
    "$$ r(x) = \\mathbb{P}(Y = 1 | X = x) = \\frac{\\exp \\left\\{ \\beta_0 + \\sum_j \\beta_j x_j \\right\\} }{1 + \\exp \\left\\{ \\beta_0 + \\sum_j \\beta_j x_j \\right\\} } $$\n",
    "\n",
    "We may write this as\n",
    "\n",
    "$$ \\text{logit} \\; \\mathbb{P}(Y = 1 | X = x) = \\beta_0 + \\sum_j \\beta_j x_j $$\n",
    "\n",
    "where $\\text{logit}(a) = \\log (a / (1 - a))$.  Under this model, each $Y_i$ is a Bernoulli with success probability\n",
    "\n",
    "$$ p_i(\\beta) = \\frac{\\exp \\left\\{ \\beta_0 + \\sum_j \\beta_j X_{ij} \\right\\} }{1 + \\exp \\left\\{ \\beta_0 + \\sum_j \\beta_j X_{ij} \\right\\} } $$\n",
    "\n",
    "The likelihood function for the data set is\n",
    "\n",
    "$$ \\mathcal{L}(\\beta) = \\prod_{i=1}^n p_i(\\beta)^{Y_i} (1 - p_i(\\beta))^{1 - Y_i}$$\n",
    "\n",
    "We obtain the MLE numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a better classifier by fitting a richer model.  For example we could fit\n",
    "\n",
    "$$ \\text{logit} \\; \\mathbb{P}(Y = 1 | X = x) = \\beta_0 + \\sum_j \\beta_j x_j + \\sum_{j, k} \\beta_{jk} x_j x_k $$\n",
    "\n",
    "More generally, we could add terms of up to order $r$ for some integer $r$.  Large values of $r$ give a more complicated model which should fit the data better.  But there is a bias-variance tradeoff which we'll discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression can be easily extend to $k$ groups but we shall not give the details here.\n",
    "\n",
    "*(Student note: multiple treatments are easily found searching for \"multinomial logistic regression\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.5  Relationship Between Logistic Regression and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA and logistic regression are almost the same thing.  \n",
    "\n",
    "If we assume each group is Gaussian with the same covariance matrix then\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\log \\left( \\frac{\\mathbb{P}(Y = 1 | X = x)}{\\mathbb{P}(Y = 0 | X = x)} \\right) \n",
    "&= \\log \\left( \\frac{\\pi_0}{\\pi_1} \\right) - \\frac{1}{2} (\\mu_0 + \\mu_1)^T \\Sigma^{-1} (\\mu_1 - \\mu_0) + x^T \\Sigma^{-1}( \\mu_1 - \\mu_0) \\\\\n",
    "&\\equiv \\alpha_0 + \\alpha^T x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "On the other hand, the logistic model is, by assumption,\n",
    "\n",
    "$$ \\log \\left( \\frac{\\mathbb{P}(Y = 1 | X = x)}{\\mathbb{P}(Y = 0 | X = x)} \\right) = \\beta_0 + \\beta^T x $$\n",
    "\n",
    "These are the same model since they both lead to classification rules that are linear in $x$.  The difference is in how we estimate the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint density of a single observation is $f(x, y) = f(x | y) f(y) = f(y | x) f(x)$.  In LDA we estimated the whole distribution by estimating $f(x | y)$ and $f(y)$; specifically, we estimated $f_k(x) = f(x | Y = k)$, $\\pi_k = f_Y(k)$.  We maximized the likelihood\n",
    "\n",
    "$$ \\prod_i f(x_i, y_i) = \\underbrace{\\prod_i f(x_i | y_i)}_\\text{Gaussian} \\underbrace{ \\prod_i f(y_i) }_\\text{Bernoulli}$$\n",
    "\n",
    "In logistic regression, we maximized the conditional likelihood $\\prod_i f(y_i | x_i)$ but we ignored the second term $f(x_i)$:\n",
    "\n",
    "$$ \\prod_i f(x_i, y_i) = \\underbrace{\\prod_i f(y_i | x_i)}_\\text{logistic} \\underbrace{ \\prod_i f(x_i) }_\\text{ignored}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since classification requires only knowing $f(y | x)$, we don't really need to estimate the whole joint distribution.  Logistic regression leaves the marginal distribution $f(x)$ unspecified so it is more nonparametric than LDA.\n",
    "\n",
    "To summarize:  LDA and logistic regression both lead to a linear classification rule.  In LDA we estimate the whole joint distribution $f(x, y) = f(x | y) f(y)$.  In logistic regression we only estimate $f(y | x)$ and we don't bother estimating $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.6 Density Estimation and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Bayes rule is $h(x) = \\text{argmax}_k \\pi_k f_k(x)$.  If we can estimate $\\pi_k$ and $f_k$ then we can estimate the Bayes classification rule.  Estimating $\\pi_k$ is easy, but what about $f_k$? We did it previously by assuming $f_k$ was Gaussian.  Another strategy is to estimate $f_k$ with some nonparametric density estimator $\\hat{f}_k$ such as a kernel estimator.  But if $x = (x_1, \\dots, x_d)$ is high dimensional, nonparametric density estimation is not very reliable.  The problem is ameliorated if we assume that $X_1, \\dots, X_d$ are independent, for then $f_k(x_1, \\dots, x_d) = \\prod_{j=1}^d f_{kj}(x_j)$.  This reduces the problem to $d$ one-dimensional density estimation problems, within each of the $k$ groups.  The resulting classifier is called the **naive Bayes classifier**.  The assumption that the components of $X$ are independent is usually wrong yet the resulting classifier might still be accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Classifier**\n",
    "\n",
    "1.  For each group $k$, compute an estimate $\\hat{f}_{kj}$ of the density $f_{kj}$ for $X_j$, using the data for which $Y_i = k$.\n",
    "\n",
    "2.  Let\n",
    "\n",
    "$$ \\hat{f}_k(x) = \\hat{f}_k(x_1, \\dots, x_d) = \\prod_{j=1}^d \\hat{f}_{kj}(x_j) $$\n",
    "\n",
    "3. Let\n",
    "\n",
    "$$ \\hat{\\pi}_k = \\frac{1}{n} \\sum_{i=1}^n I(Y_i = k) $$\n",
    "\n",
    "where $I(t) = 1$ if $t$ and $I(t) = 0$ otherwise.\n",
    "\n",
    "4. Let\n",
    "\n",
    "$$ h(x) = \\text{argmax}_k \\hat{\\pi}_k \\hat{f}_k(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive Bayes classifier is especially popular when $x$ is high dimensional and discrete.  In that case, $\\hat{f}_{kj}(x_k)$ is especially simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.7 Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
