{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Multivariate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review of notation from linear algebra:\n",
    "\n",
    "- If $x$ and $y$ are vectors, then $x^T y = \\sum_j x_j y_j$.\n",
    "- If $A$ is a matrix then $\\text{det}(A)$ denotes the determinant of $A$, $A^T$ denotes the transpose of A, and $A^{-1}$ denotes the inverse of $A$ (if the inverse exists).\n",
    "- The trace of a square matrix $A$, denoted by $\\text{tr}(A)$, is the sum of its diagonal elements.\n",
    "- The trace satisfies $\\text{tr}(AB) = \\text{tr}(BA)$ and $\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)$.\n",
    "- The trace satisfies $\\text{tr}(a) = a$ if $a$ is a scalar.\n",
    "- A matrix $\\Sigma$ is positive definite if $x^T \\Sigma x > 0$ for all non-zero vectors $x$.\n",
    "- If a matrix $\\Sigma$ is symmetric and positive definite, there exists a matrix $\\Sigma^{1/2}$, called the square root of $\\Sigma$, with the following properties:\n",
    "    - $\\Sigma^{1/2}$ is symmetric\n",
    "    - $\\Sigma = \\Sigma^{1/2} \\Sigma^{1/2}$\n",
    "    - $\\Sigma^{1/2} \\Sigma^{-1/2} = \\Sigma^{-1/2} \\Sigma^{1/2} = I$ where $\\Sigma^{-1/2} = (\\Sigma^{1/2})^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1 Random Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate models involve a random vector $X$ of the form\n",
    "\n",
    "$$X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_k \\end{pmatrix}$$\n",
    "\n",
    "The mean of a random vector $X$ is defined by\n",
    "\n",
    "$$\\mu \n",
    "= \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ mu_k \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\mathbb{E}(X_1) \\\\ \\vdots \\\\ \\mathbb{E}(X_k) \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The covariance matrix $\\Sigma$ is defined to be\n",
    "\n",
    "$$\\Sigma = \\mathbb{V}(X) = \\begin{pmatrix}\n",
    "\\mathbb{V}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_k) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\mathbb{V}(X_2) & \\cdots & \\text{Cov}(X_2, X_k) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_k, X_1) & \\text{Cov}(X_k, X_2) & \\cdots & \\mathbb{V}(X_k)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "This is also called the variance matrix or the variance-covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 15.1**.  Let $a$ be a vector of length $k$ and let $X$ be a random vector of the same length with mean $\\mu$ and variance $\\Sigma$.  Then $\\mathbb{E}(a^T X) = a^T\\mu$ and $\\mathbb{V}(a^T X) = a^T \\Sigma a$.  If $A$ is a matrix with $k$ columns then $\\mathbb{E}(AX) = A\\mu$ and $\\mathbb{V}(AX) = A \\Sigma A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have a random sample of $n$ vectors:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}X_{11} \\\\ X_{21} \\\\ \\vdots \\\\ X_{k1} \\end{pmatrix}, \\;\n",
    "\\begin{pmatrix}X_{21} \\\\ X_{22} \\\\ \\vdots \\\\ X_{k2} \\end{pmatrix}, \\;\n",
    "\\cdots , \\;\n",
    "\\begin{pmatrix}X_{1n} \\\\ X_{2n} \\\\ \\vdots \\\\ X_{kn} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The sample mean $\\overline{X}$ is a vector defined by\n",
    "\n",
    "$$\\overline{X} = \\begin{pmatrix} \\overline{X}_1 \\\\ \\vdots \\\\ \\overline{X}_k \\end{pmatrix}$$\n",
    "\n",
    "where $\\overline{X}_i = n^{-1} \\sum_{j = 1}^n X_{ij}$.  The sample variance matrix is\n",
    "\n",
    "$$ S = \\begin{pmatrix} \n",
    "s_{11} & s_{12} & \\cdots & s_{1k} \\\\\n",
    "s_{12} & s_{22} & \\cdots & s_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "s_{1k} & s_{2k} & \\cdots & s_{kk}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$s_{ab} = \\frac{1}{n - 1} \\sum_{j = 1}^n (X_{aj} - \\overline{X}_a) (X_{bj} - \\overline{X}_b)$$\n",
    "\n",
    "It follows that $\\mathbb{E}(\\overline{X}) = \\mu$ and $\\mathbb{E}(S) = \\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 Estimating the Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $n$ data points from a bivariate distribution\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} X_{11} \\\\ X_{21}\\end{pmatrix}, \\;\n",
    "\\begin{pmatrix} X_{12} \\\\ X_{22}\\end{pmatrix}, \\;\n",
    "\\cdots \\;\n",
    "\\begin{pmatrix} X_{1n} \\\\ X_{1n}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Recall that the correlation between $X_1$ and $X_2$ is\n",
    "\n",
    "$$\\rho = \\frac{\\mathbb{E}((X_1 - \\mu) (X_2 - \\mu_2))}{\\sigma_1 \\sigma_2}$$\n",
    "\n",
    "The sample correlation (the plug-in estimator) is\n",
    "\n",
    "$$\\hat{\\rho} = \\frac{\\sum_{i=1}^n (X_{1i} - \\overline{X}_1)(X_{2i} - \\overline{X}_2)}{s_1 s_2}$$\n",
    "\n",
    "We can construct a confidence interval for $\\rho$ by applying the delta method as usual.  However, it turns out that we get a more accurate confidence interval by first constructing a confidence interval for a function $\\theta = f(\\rho)$ and then applying the inverse function $f^{-1}$.  The method, due to Fisher, is as follows.  Define\n",
    "\n",
    "$$f(r) = \\frac{1}{2} \\left( \\log(1 + r) - \\log(1 - r)\\right) $$\n",
    "\n",
    "and let $\\theta = f(\\rho)$.  The inverse of $f$ is\n",
    "\n",
    "$$g(z) \\equiv f^{-1}(z) = \\frac{e^{2z} - 1}{e^{2z} + 1}$$\n",
    "\n",
    "Now do the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximate Confidence Interval for the Correlation**\n",
    "\n",
    "1. Compute\n",
    "\n",
    "$$\\hat{\\theta} = f(\\hat{\\rho}) = \\frac{1}{2} \\left( \\log(1 + \\hat{\\rho}) - \\log(1 - \\hat{\\rho})\\right) $$\n",
    "\n",
    "2. Compute the approximate standard error of $\\hat{\\theta}$ which can be shown to be\n",
    "\n",
    "$$\\hat{\\text{se}}(\\hat{\\theta}) = \\frac{1}{\\sqrt{n - 3}} $$\n",
    "\n",
    "3. An approximate $1 - \\alpha$ confidence interval for $\\theta = f(\\rho)$ is\n",
    "\n",
    "$$(a, b) \\equiv \\left(\\hat{\\theta} - \\frac{z_{\\alpha/2}}{\\sqrt{n - 3}}, \\; \\hat{\\theta} + \\frac{z_{\\alpha/2}}{\\sqrt{n - 3}} \\right)$$\n",
    "\n",
    "4. Apply the inverse transformation $f^{-1}(z)$ to get a confidence interval for $\\rho$:\n",
    "\n",
    "$$ \\left( \\frac{e^{2a} - 1}{e^{2a} + 1}, \\frac{e^{2b} - 1}{e^{2b} + 1} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3 Multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review of Multinomial distribution: consider drawing a ball from an urn that has $n$ balls of $k$ colors.  Let $p = (p_1, \\dots, p_k)$ where $p_j \\geq 0$ are the probabilities of drawing (with replacement) a ball of each color; $\\sum_j p_j = 1$.  Draw $n$ times and let $X = (X_1, \\dots, X_n)$ where $X_j$ is the number of times that color $j$ appeared; so $\\sum_k X_k = n$.  We say $X$ has a Multinomial($n$, $p$) distribution.  The probability function is\n",
    "\n",
    "$$ f(x; p) = \\binom{n}{x_1 \\dots x_k} p_1^{x_1}\\dots p_k^{x_k}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\binom{n}{x_1 \\dots x_k} = \\frac{n!}{x_1! \\dots x_k!} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 15.2**. Let $X \\sim \\text{Multinomial}(n, p)$. Then the marginal distribution of $X_j$ is $X_j \\sim \\text{Binomial}(n, p_j)$.  The mean and variance of $X$ are\n",
    "\n",
    "$$\\mathbb{E}(X) = \\begin{pmatrix} np_1 \\\\ \\cdots \\\\ np_k \\end{pmatrix}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mathbb{V}(X) = \\begin{pmatrix}\n",
    "np_1(1 - p_1) & -np_1p_2 & \\cdots & -np_1p_k \\\\\n",
    "-np_1p_2 & np_2(1 - p_2) & \\cdots & -np_2p_k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-np_1p_k & -np_2p_k & \\cdots & -np_k(1 - p_k)\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  That $X_j \\sim \\text{Bimomial}(n, p_j)$ follows easily.  Hence $\\mathbb{E}(X_j) = np_j$ and $\\mathbb{V}(X_j) = np_j(1 - p_j)$.  \n",
    "\n",
    "To compute $\\text{Cov}(X_i, X_j)$, notice that $X_i + X_j \\sim \\text{Binomial}(n, p_i + p_j)$, so $\\mathbb{V}(X_i + X_j) = n (p_i + p_j) (1 - p_i - p_j)$.  On the other hand, decomposing the sum of the random variables on the variance,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{V}(X_i + X_j) &= \\mathbb{V}(X_i) + \\mathbb{V}(X_j) + 2 \\text{Cov}(X_i, X_j) \\\\\n",
    "&= np_i(1 - p_i) + np_j(1 - p_j) + 2 \\text{Cov}(X_i, X_j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Equating both expressions and isolating the covariance we get $\\text{Cov}(X_i, X_j) = -np_ip_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 15.3**.  The maximum likelihood estimator of $p$ is\n",
    "\n",
    "$$\\hat{p} \n",
    "= \\begin{pmatrix} \\hat{p}_1 \\\\ \\cdots \\\\ \\hat{p}_k \\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{X_1}{n} \\\\ \\cdots \\\\ \\frac{X_k}{n} \\end{pmatrix}\n",
    "= \\frac{X}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  The log-likelihood (ignoring a constant) is $\\ell(p) = \\sum_j X_j \\log p_j$.  When we maximize it we need to be careful to enforce the constraint that $\\sum_j p_j = 1$.  Using Lagrange multipliers, instead we maximize\n",
    "\n",
    "$$A(p) = \\sum_{j=1}^k X_j \\log p_j + \\lambda \\left( \\sum_{j=1}^k p_j - 1 \\right)$$\n",
    "\n",
    "But\n",
    "\n",
    "$$\\frac{\\partial A(p)}{\\partial p_j} = \\frac{X_j}{p_j} + \\lambda$$\n",
    "\n",
    "Setting it to zero we get $\\hat{p}_j = - X_j / \\lambda$.  Since $\\sum_j \\hat{p}_j = 1$ we get $\\lambda = -n$ and so $\\hat{p}_j = X_j / n$, which is our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want the variance of the MLE.  The direct approach is to compute the variance matrix of $\\hat{p}$ directly:  $\\mathbb{V}(\\hat{p}) = \\mathbb{V}(X / n) = n^{-2} \\mathbb{V}(X)$, so\n",
    "\n",
    "$$\\mathbb{V}(\\hat{p}) = \\frac{1}{n} \\begin{pmatrix}\n",
    "p_1(1 - p_1) & -p_1p_2 & \\cdots & -p_1p_k \\\\\n",
    "-p_1p_2 & p_2(1 - p_2) & \\cdots & -p_2p_k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-p_1p_k & -p_2p_k & \\vdots & p_k(1 - p_k)\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4 Multivariate Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall the definition of the multivariate normal.  Let\n",
    "\n",
    "$$ Z = \\begin{pmatrix} Z_1 \\\\ \\cdots \\\\ Z_k \\end{pmatrix} $$\n",
    "\n",
    "where $Z_1, \\dots, Z_k \\sim N(0, 1)$ are independent.  The density of $Z$ is\n",
    "\n",
    "$$ f(z) = \\frac{1}{(2\\pi)^{k / 2}} \\exp \\left\\{ -\\frac{1}{2} \\sum_{j=1}^k z_j^2 \\right\\} = \\frac{1}{(2\\pi)^{k / 2}} \\exp \\left\\{ -\\frac{1}{2} z^T z \\right\\}$$\n",
    "\n",
    "The variance matrix of $Z$ is the identity matrix $I$.  We write $Z \\sim N(0, I)$ where it is understood that $0$ is a vector of $k$ zeroes. We say $Z$ has a standard multivariate Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, a vector $X$ has a multivariate Normal distribution, denoted by $X \\sim N(\\mu, \\Sigma)$, if its density is\n",
    "\n",
    "$$ f(x; \\mu, \\Sigma) = \\frac{1}{(2 \\pi)^{k / 2} \\text{det}(\\Sigma)^{1/2}} \\exp \\left\\{ -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right\\}$$\n",
    "\n",
    "where $\\mu$ is a vector of length $k$ and $\\Sigma$ is a $k \\times k$ symmetric, positive definite matrix.  Then $\\mathbb{E}(X) = \\mu$ and $\\mathbb{V}(X) = \\Sigma$.  Setting $\\mu = 0$ and $\\Sigma = I$ gives back the standard Normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 15.4**.  The following properties hold:\n",
    "\n",
    "1. If $Z \\sim N(0, 1)$ and $X = \\mu + \\Sigma^{1/2} Z$ then $X \\sim N(\\mu, \\Sigma)$.\n",
    "\n",
    "2. If $X \\sim N(\\mu, \\Sigma)$, then $\\Sigma^{-1/2}(X - \\mu) \\sim N(0, 1)$.\n",
    "\n",
    "3. If $X \\sim N(\\mu, \\Sigma)$ and $a$ is a vector with the same length as $X$, then $a^T X \\sim N(a^T \\mu, a^T \\Sigma a)$.\n",
    "\n",
    "4. Let $V = (X - \\mu)^T \\Sigma^{-1} (X - \\mu)$.  Then $V \\sim \\xi_k^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we partition a random Normal vector $X$ into two parts $X = (X_a, X_b)$.  We can similarly partition the mean $\\mu = (\\mu_a, \\mu_b)$ and the variance\n",
    "\n",
    "$$\\Sigma = \\begin{pmatrix} \n",
    "\\Sigma_{aa} & \\Sigma_{ab} \\\\\n",
    "\\Sigma_{ba} & \\Sigma_{bb}\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 15.5**.  Let $X \\sim N(\\mu, \\Sigma)$.  Then:\n",
    "\n",
    "1. The marginal distribution of $X_a$ is $X_a \\sim N(\\mu_a, \\Sigma_{aa})$.\n",
    "2. The conditional distribution of $X_b$ given $X_a = x_a$ is\n",
    "\n",
    "$$ X_b | X_a = x_a \\sim N(\\mu(x_a), \\Sigma(x_a))$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu(x_a) &= \\mu_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (x_a - \\mu_a) \\\\\n",
    "\\Sigma(x_a) &= \\Sigma_{bb} - \\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 15.6**.  Given a random sample of size $n$ from a $N(\\mu, \\Sigma)$, the log-likelihood (up to a constant not depending on $\\mu$ or $\\Sigma$) is given by\n",
    "\n",
    "$$\\ell(\\mu, \\Sigma) = -\\frac{n}{2} (\\overline{X} - \\mu)^T \\Sigma^{-1} (\\overline{X} - \\mu) - \\frac{n}{2} \\text{tr} \\left( \\Sigma^{-1}S \\right) - \\frac{n}{2} \\log \\text{det} \\left( \\Sigma \\right) $$\n",
    "\n",
    "The MLE is\n",
    "\n",
    "$$ \\hat{\\mu} = \\overline{X} \n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{\\Sigma} = \\left( \\frac{n - 1}{n} \\right) S$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5 Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof of Theorem 15.6**. Let the $i$-th random vector be $X^i$.  The log-likelihood is\n",
    "\n",
    "$$\\ell(\\mu, \\Sigma) = \\sum_{i = 1}^k f(X^i; \\mu, \\Sigma) \n",
    "= -\\frac{kn}{2} \\log ( 2\\pi ) - \\frac{n}{2} \\log \\text{det} \\left( \\Sigma \\right)\n",
    "- \\frac{1}{2} \\sum_{i=1}^k (X^i - \\mu)^T \\Sigma^{-1} (X^i - \\mu)\n",
    "$$\n",
    "\n",
    "Now,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^k (X^i - \\mu)^T \\Sigma^{-1} (X^i - \\mu) &=\n",
    "\\sum_{i=1}^k \\left[ (X^i - \\overline(X)) + (\\overline{X} - \\mu) \\right]^T \\Sigma^{-1} \\left[(X^i - \\overline{X}) + (\\overline{X} - \\mu) \\right] \\\\\n",
    "&= \\sum_{i=1}^k [(X^i - \\overline{X})^T\\Sigma^{-1}(X^i - \\overline{X})]\n",
    "+ n (\\overline{X} - \\mu)^T \\Sigma^{-1} (\\overline{X} - \\mu)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since $\\sum_i (X^i - \\overline{X}) \\Sigma^{-1} (\\overline{X} - \\mu) = 0$.  Also, $(X^i - \\mu)^T \\Sigma^T (X^i - \\mu)$ is a scalar, so\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^k (X^i - \\mu)^T \\Sigma^{-1} (X^i - \\mu) &=\n",
    "\\sum_{i=1}^k \\text{tr} \\left[ (X^i - \\mu)^T \\Sigma^{-1} (X^i - \\mu)\\right] \\\\\n",
    "&= \\sum_{i=1}^k \\text{tr} \\left[ \\Sigma^{-1} (X^i - \\mu) (X^i - \\mu)^T \\right] \\\\\n",
    "&= \\text{tr} \\left[ \\Sigma^{-1} \\sum_{i=1}^k (X^i - \\mu) (X^i - \\mu) ^T \\right] \\\\\n",
    "&= n \\; \\text{tr} \\left[ \\Sigma^{-1} S\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so the conclusion follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 15.6.1**.  Prove Theorem 15.1.\n",
    "\n",
    "Let $a$ be a vector of length $k$ and let $X$ be a random vector of the same length with mean $\\mu$ and variance $\\Sigma$.  Then $\\mathbb{E}(a^T X) = a^T\\mu$ and $\\mathbb{V}(a^T X) = a^T \\Sigma a$.  If $A$ is a matrix with $k$ columns then $\\mathbb{E}(AX) = A\\mu$ and $\\mathbb{V}(AX) = A \\Sigma A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "For the vector version of the theorem, we have:\n",
    "\n",
    "$$\\mathbb{E}(a^T X) = \\mathbb{E}\\left( \\sum_{i=1}^k a_i X_i \\right) = \\sum_{i=1}^k a_i \\mathbb{E}(X_i) = \\sum_{i=1}^k a_i \\mu_i = a^T \\mu $$\n",
    "\n",
    "$$\\mathbb{V}(a^T X) = \\mathbb{V}\\left( \\sum_{i=1}^k a_i X_i \\right) = \\sum_{i=1}^k \\sum_{j=1}^k a_i a_j \\text{Cov} (X_i, X_j) = \\sum_{i=1}^k a_i \\left( \\sum_{j=1}^k \\text{Cov}(X_i, X_j) a_j \\right) = \\sum_{i=1}^k a_i \\Big( \\Sigma a \\Big)_i = a^T \\Sigma a$$\n",
    "\n",
    "For the matrix version of the theorem, consider the $r$ rows of $A$ as vectors, separately, $a^1, \\dots, a^k$:\n",
    "\n",
    "$$ A = \\begin{pmatrix}\n",
    "\\cdots & a^1 & \\cdots \\\\\n",
    "\\cdots & a^2 & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\cdots & a^r & \\cdots\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ \\mathbb{E}(AX) = \\begin{pmatrix}\n",
    "\\mathbb{E}(a^1 X) \\\\\n",
    "\\mathbb{E}(a^2 X) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbb{E}(a^r X)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "a^1 \\mu \\\\\n",
    "a^2 \\mu \\\\\n",
    "\\vdots \\\\\n",
    "a^r \\mu\n",
    "\\end{pmatrix} = A\\mu\n",
    "$$\n",
    "\n",
    "Finally, looking at the $i$-th term of $AX$,\n",
    "\n",
    "$$(AX)_i = \\sum_{s=1}^k a_{is} X_s = a^i X$$\n",
    "\n",
    "so, by the vector version of the theorem, $\\mathbb{V}((AX)_i) = (a^i)^T \\Sigma a^i$.  Applying this to every element:\n",
    "\n",
    "$$\\mathbb{V}(AX) = \\begin{pmatrix}\n",
    "\\mathbb{V}((AX)_1) \\\\\n",
    "\\mathbb{V}((AX)_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbb{V}((AX)_r)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mathbb{V}(a^1 X) \\\\\n",
    "\\mathbb{V}(a^2 X) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbb{V}(a^r X)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "(a^1)^T \\Sigma a^1 \\\\\n",
    "(a^2)^T \\Sigma a^2 \\\\\n",
    "\\vdots \\\\\n",
    "(a^r)^T \\Sigma a^r\n",
    "\\end{pmatrix} = A \\Sigma A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 15.6.2**.  Find the Fisher information matrix for the MLE of a Multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
