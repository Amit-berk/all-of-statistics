{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Multivariate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review of notation from linear algebra:\n",
    "\n",
    "- If $x$ and $y$ are vectors, then $x^T y = \\sum_j x_j y_j$.\n",
    "- If $A$ is a matrix then $\\text{det}(A)$ denotes the determinant of $A$, $A^T$ denotes the transpose of A, and $A^{-1}$ denotes the inverse of $A$ (if the inverse exists).\n",
    "- The trace of a square matrix $A$, denoted by $\\text{tr}(A)$, is the sum of its diagonal elements.\n",
    "- The trace satisfies $\\text{tr}(AB) = \\text{tr}(BA)$ and $\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)$.\n",
    "- The trace satisfies $\\text{tr}(a) = a$ if $a$ is a scalar.\n",
    "- A matrix $\\Sigma$ is positive definite if $x^T \\Sigma x > 0$ for all non-zero vectors $x$.\n",
    "- If a matrix $\\Sigma$ is symmetric and positive definite, there exists a matrix $\\Sigma^{1/2}$, called the square root of $\\Sigma$, with the following properties:\n",
    "    - $\\Sigma^{1/2}$ is symmetric\n",
    "    - $\\Sigma = \\Sigma^{1/2} \\Sigma^{1/2}$\n",
    "    - $\\Sigma^{1/2} \\Sigma^{-1/2} = \\Sigma^{-1/2} \\Sigma^{1/2} = I$ where $\\Sigma^{-1/2} = (\\Sigma^{1/2})^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1 Random Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate models involve a random vector $X$ of the form\n",
    "\n",
    "$$X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_k \\end{pmatrix}$$\n",
    "\n",
    "The mean of a random vector $X$ is defined by\n",
    "\n",
    "$$\\mu \n",
    "= \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ mu_k \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\mathbb{E}(X_1) \\\\ \\vdots \\\\ \\mathbb{E}(X_k) \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The covariance matrix $\\Sigma$ is defined to be\n",
    "\n",
    "$$\\Sigma = \\mathbb{V}(X) = \\begin{pmatrix}\n",
    "\\mathbb{V}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_k) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\mathbb{V}(X_2) & \\cdots & \\text{Cov}(X_2, X_k) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_k, X_1) & \\text{Cov}(X_k, X_2) & \\cdots & \\mathbb{V}(X_k)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "This is also called the variance matrix or the variance-covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 15.1**.  Let $a$ be a vector of length $k$ and let $X$ be a random vector of the same length with mean $\\mu$ and variance $\\Sigma$.  Then $\\mathbb{E}(a^T X) = a^T\\mu$ and $\\mathbb{V}(a^T X) = a^T \\Sigma a$.  If $A$ is a matrix with $k$ columns then $\\mathbb{E}(AX) = A\\mu$ and $\\mathbb{V}(AX) = A \\Sigma A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have a random sample of $n$ vectors:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}X_{11} \\\\ X_{21} \\\\ \\vdots \\\\ X_{k1} \\end{pmatrix}, \\;\n",
    "\\begin{pmatrix}X_{21} \\\\ X_{22} \\\\ \\vdots \\\\ X_{k2} \\end{pmatrix}, \\;\n",
    "\\cdots , \\;\n",
    "\\begin{pmatrix}X_{1n} \\\\ X_{2n} \\\\ \\vdots \\\\ X_{kn} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The sample mean $\\overline{X}$ is a vector defined by\n",
    "\n",
    "$$\\overline{X} = \\begin{pmatrix} \\overline{X}_1 \\\\ \\vdots \\\\ \\overline{X}_k \\end{pmatrix}$$\n",
    "\n",
    "where $\\overline{X}_i = n^{-1} \\sum_{j = 1}^n X_{ij}$.  The sample variance matrix is\n",
    "\n",
    "$$ S = \\begin{pmatrix} \n",
    "s_{11} & s_{12} & \\cdots & s_{1k} \\\\\n",
    "s_{12} & s_{22} & \\cdots & s_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "s_{1k} & s_{2k} & \\cdots & s_{kk}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$s_{ab} = \\frac{1}{n - 1} \\sum_{j = 1}^n (X_{aj} - \\overline{X}_a) (X_{bj} - \\overline{X}_b)$$\n",
    "\n",
    "It follows that $\\mathbb{E}(\\overline{X}) = \\mu$ and $\\mathbb{E}(S) = \\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 Estimating the Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $n$ data points from a bivariate distribution\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} X_{11} \\\\ X_{21}\\end{pmatrix}, \\;\n",
    "\\begin{pmatrix} X_{12} \\\\ X_{22}\\end{pmatrix}, \\;\n",
    "\\cdots \\;\n",
    "\\begin{pmatrix} X_{1n} \\\\ X_{1n}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Recall that the correlation between $X_1$ and $X_2$ is\n",
    "\n",
    "$$\\rho = \\frac{\\mathbb{E}((X_1 - \\mu) (X_2 - \\mu_2))}{\\sigma_1 \\sigma_2}$$\n",
    "\n",
    "The sample correlation (the plug-in estimator) is\n",
    "\n",
    "$$\\hat{\\rho} = \\frac{\\sum_{i=1}^n (X_{1i} - \\overline{X}_1)(X_{2i} - \\overline{X}_2)}{s_1 s_2}$$\n",
    "\n",
    "We can construct a confidence interval for $\\rho$ by applying the delta method as usual.  However, it turns out that we get a more accurate confidence interval by first constructing a confidence interval for a function $\\theta = f(\\rho)$ and then applying the inverse function $f^{-1}$.  The method, due to Fisher, is as follows.  Define\n",
    "\n",
    "$$f(r) = \\frac{1}{2} \\left( \\log(1 + r) - \\log(1 - r)\\right) $$\n",
    "\n",
    "and let $\\theta = f(\\rho)$.  The inverse of $f$ is\n",
    "\n",
    "$$g(z) \\equiv f^{-1}(z) = \\frac{e^{2z} - 1}{e^{2z} + 1}$$\n",
    "\n",
    "Now do the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximate Confidence Interval for the Correlation**\n",
    "\n",
    "1. Compute\n",
    "\n",
    "$$\\hat{\\theta} = f(\\hat{\\rho}) = \\frac{1}{2} \\left( \\log(1 + \\hat{\\rho}) - \\log(1 - \\hat{\\rho})\\right) $$\n",
    "\n",
    "2. Compute the approximate standard error of $\\hat{\\theta}$ which can be shown to be\n",
    "\n",
    "$$\\hat{\\text{se}}(\\hat{\\theta}) = \\frac{1}{\\sqrt{n - 3}} $$\n",
    "\n",
    "3. An approximate $1 - \\alpha$ confidence interval for $\\theta = f(\\rho)$ is\n",
    "\n",
    "$$(a, b) \\equiv \\left(\\hat{\\theta} - \\frac{z_{\\alpha/2}}{\\sqrt{n - 3}}, \\; \\hat{\\theta} + \\frac{z_{\\alpha/2}}{\\sqrt{n - 3}} \\right)$$\n",
    "\n",
    "4. Apply the inverse transformation $f^{-1}(z)$ to get a confidence interval for $\\rho$:\n",
    "\n",
    "$$ \\left( \\frac{e^{2a} - 1}{e^{2a} + 1}, \\frac{e^{2b} - 1}{e^{2b} + 1} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
