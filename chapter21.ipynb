{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Nonparametric Curve Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter we discuss the nonparametric estimation of probability density functions and regression functions, which we refer to a **curve estimation**.\n",
    "\n",
    "In Chapter 8 we saw it is possible to consistently estimate a cumulative distribution function $F$ without making any assumptions about $F$.  If we want to estimate a probability density function $f(x)$ or a regression function $r(x) = \\mathbb{E}(Y | X = x)$ the situation is different.  We cannot estimate these functions consistently without making some smoothness assumptions.  Correspondingly, we will perform some sort of smoothing operation with the data.\n",
    "\n",
    "A simple example of a density estimator is a **histogram**.  To form a histogram estimator of a density $f$, we divide the  real line into disjoint sets called **bins**.  The histogram estimator is a piecewise constant function where the height of the function is proportional to the number of observations in each bin.  The number of bins is an example of a **smoothing parameter**.  If we smooth too much (large bins) we get a highly biased estimator while if smooth too little (small bins) we get a highly variable estimator.  Much of curve estimation is concerned with trying to optimally balance variance and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.1 The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $g$ denote an unknown function and let $\\hat{g}_n$ denote an estimator of $g$.  Bear in mind that $\\hat{g}_n(x)$ is a random function evaluated at a point $x$; $\\hat{g}_n$ is random because it depends on the data.  Indeed, we could be more explicit and write $\\hat{g}_n(x) = h_x(X_1, \\dots, X_n)$ to show that $\\hat{g}_n(x)$ is a function of the data $X_1, \\dots, X_n$ and that the function could be different for each $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a loss function, we will use the **integrated square error (ISE)**:\n",
    "\n",
    "$$ L(g, \\hat{g}_n) = \\int (g(u) - \\hat{g}_n(u))^2 du$$\n",
    "\n",
    "The **risk** or **mean integrated square error (MISE)** is:\n",
    "\n",
    "$$ R(g, \\hat{g}) = \\mathbb{E}\\left(L(g, \\hat{g}) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 21.1**.  The risk can be written as \n",
    "\n",
    "$$ R(g, \\hat{g}) = \\int b^2(x) dx + \\int v(x) dx $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ b(x) = \\mathbb{E}(\\hat{g}_n(x)) - g(x) $$\n",
    "\n",
    "is the bias of $\\hat{g}_n(x)$ at a fixed $x$ and\n",
    "\n",
    "$$ v(x) = \\mathbb{V}(\\hat{g}_n(x)) = \\mathbb{E}\\left( \\hat{g}_n(x) - \\mathbb{E}(\\hat{g}_n(x))^2\\right) $$\n",
    "\n",
    "is the variance of $\\hat{g}_n(x)$ at a fixed $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary,\n",
    "\n",
    "$$ \\text{RISK} = \\text{BIAS}^2 + \\text{VARIANCE} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data is over-smoothed, the bias term is large and the variance is small.  When the data are under-smoothed the opposite is true.  This is called the **bias-variance trade-off**.  Minimizing risk corresponds to balancing bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2 Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_1, \\dots, X_n$ be IID on $[0, 1]$ with density $f$.   The restriction on $[0, 1]$is not crucial; we can always rescale the data to be on this interval.  Let $m$ be an integer and define bins\n",
    "\n",
    "$$ B_1 = \\left[0, \\frac{1}{m} \\right), B_2 = \\left[\\frac{1}{m}, \\frac{2}{m} \\right), \\dots, B_m = \\left[\\frac{m - 1}{m}, 1 \\right] $$\n",
    "\n",
    "Define the **binwidth** $h = 1 / m$, let $v_j$ be the number of observations in $B_j$, let $\\hat{p}_j = v_j / n$ and let $p_j = \\int_{B_j} f(u) du$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **histogram estimator** is defined by \n",
    "\n",
    "$$\n",
    "\\hat{f}_n(x) = \\begin{cases}\n",
    "\\hat{p}_1 / h & x \\in B_1 \\\\\n",
    "\\hat{p}_2 / h & x \\in B_2 \\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "\\hat{p}_m / h & x \\in B_m\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which we can write more succinctly as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{f}_n(x) = \\sum_{j=1}^n \\frac{\\hat{p}_j}{h} I(x \\in B_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the motivation for this estimator, let $p_j = \\int_{B_j} f(u) du$ and note that, for $x \\in B_j$ and $h$ small,\n",
    "\n",
    "$$ \\hat{f}_n(x) = \\frac{\\hat{p}_j}{h} \\approx \\frac{p_j}{h} = \\frac{\\int_{B_j} f(u) du}{h} \\approx \\frac{f(x) h}{h} = f(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and the variance of $\\hat{f}_n(x)$ are given in the following Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.3**.  Consider fixed $x$ and fixed $m$, and let $B_j$ be the bin containing $x$.  Then,\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}(\\hat{f}_n(x)) = \\frac{p_j}{h} \n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{V}(\\hat{f}_n(x)) = \\frac{p_j (1 - p_j)}{nh^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the bias-variance tradeoff.  Consider some $x \\in B_j$.  For any other $u \\in B_j$,\n",
    "\n",
    "$$ f(u) \\approx f(x) + (u - x) f'(x) $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "p_j = \\int_{B_j} f(u) du &\\approx \\int_{B_j} (f(x) + (u - x) f'(x)) du \\\\\n",
    "&= f(x) h + h f'(x) \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the bias $b(x)$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b(x) &= \\mathbb{E}(\\hat{f}_n(x)) - f(x) = \\frac{p_j}{h} - f(x) \\\\\n",
    "&\\approx \\frac{f(x) h + h f'(x) \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)}{h} - f(x) \\\\\n",
    "&= f'(x) \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $\\overline{x}_j$ is the center of the bin, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int_{B_j} b^2(x) dx &= \\int_{B_j} (f'(x))^2 \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)^2 dx \\\\\n",
    "&\\approx (f'(\\overline{x}_j))^2 \\int_{B_j} \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)^2 dx \\\\\n",
    "&= (f'(\\overline{x}_j))^2 \\frac{h^3}{12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int_0^1 b^2(x) dx &= \\sum_{j=1}^m \\int_{B_j} b^2(x) dx \\approx \\sum_{j=1}^m (f'(\\overline{x}_j))^2 \\frac{h^3}{12} \\\\\n",
    "&= \\frac{h^2}{12} \\sum_{j=1}^m h(f'(\\overline{x}_j))^2 \\approx \\frac{h^2}{12} \\int_0^1 h(f'(\\overline{x}_j))^2 dx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that this increases as a function of $h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the variance.  For $h$ small, $1 - p_j \\approx 1$, so\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v(x) &\\approx \\frac{p_j}{nh^2}\\\\\n",
    "&= \\frac{f(x)h + h f'(x)\\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)}{nh^2} \\\\\n",
    "&\\approx \\frac{f(x)}{nh}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "when we keep the dominant term.  So\n",
    "\n",
    "$$\n",
    "\\int_0^1 v(x) dx \\approx \\frac{1}{nh}\n",
    "$$\n",
    "\n",
    "Note that this decreases with $h$.  Putting this all together, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.4**.  Suppose that $\\int (f'(u))^2 du < \\infty$.  Then\n",
    "\n",
    "$$ R(\\hat{f}_n, f) \\approx \\frac{h^2}{12} \\int (f'(u))^2 du + \\frac{1}{nh}$$\n",
    "\n",
    "The value $h^*$ that minimizes this expression is\n",
    "\n",
    "$$ h^* = \\frac{1}{n^{1/3}} \\left( \\frac{6}{\\int (f'(u))^2 du} \\right)^{1/3}$$\n",
    "\n",
    "With this choice of binwidth,\n",
    "\n",
    "$$ R(\\hat{f}_n, f) \\approx \\frac{C}{n^{2/3}} $$\n",
    "\n",
    "where $C = (3/4)^{2/3} \\left( \\int (f'(u))^2 du \\right)^{1/3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem 21.4 is quite revealing.  We see that with an optimally chosen bandwidth, the MISE decreases to 0 at rate $n^{-2/3}$.  By comparison, most parametric estimators converge at rate $n^{-1}$.  The formula for optimal binwidth $h^*$ is of theoretical interest but it is not useful in practice since it depends on the unknown function $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical way of choosing the binwidth is to estimate the risk function and minimize over $h$.  Recall that the loss function, which we now write as a function of $h$, is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h) &= \\int \\left( \\hat{f}_n(x) - f(x) \\right)^2 dx \\\\\n",
    "&= \\int \\hat{f}_n^2(x) dx - 2 \\int \\hat{f}_n(x) f(x) dx + \\int f^2(x) dx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last term does not depend on the binwidth $h$ so minimizing the risk is equivalent to minimizing the expected value of\n",
    "\n",
    "$$ J(h) = \\int \\hat{f}_n^2(x) dx - 2 \\int \\hat{f}_n(x) f(x) dx $$\n",
    "\n",
    "We shall refer to $\\mathbb{E}(J(h))$ as the risk, although it differs from the true risk by the constant term $\\int f^2(x) dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cross-validation estimator of  risk** is\n",
    "\n",
    "$$ \\hat{J}(h) = \\int \\left( \\hat{f}_n(x) \\right)^2 dx - \\frac{2}{n} \\sum_{i=1}^n \\hat{f}_{(-i)}(X_i)$$\n",
    "\n",
    "where $\\hat{f}_{(-i)}$ is the histogram estimator obtained after removing the $i$-th observation.  We refer to $\\hat{J}(h)$ as the cross-validation score or estimated risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.6**.  The cross-validation estimator is nearly unbiased:\n",
    "\n",
    "$$ \\mathbb{E}(\\hat{J}(x)) \\approx \\mathbb{E}(J(x)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we need to recompute the histogram $n$ times to compute $\\hat{J}(x)$.  Moreover, this has to be done for all values of $h$.  Fortunately, there is a shortcut formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.7**.  The following identity holds:\n",
    "\n",
    "$$ \\hat{J}(h) = \\frac{2}{(n - 1)h} + \\frac{n+1}{n-1} \\sum_{j=1}^m \\hat{p}_j^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want some sort of confidence set for $f$.  Suppose $\\hat{f}_n$ is a histogram with $m$ bins and binwidth $h = 1 / m$.  We cannot realistically make confidence statements about the fine details of true density $f$.  Instead, we make confidence statements about $f$ at the resolution of the histogram.  To this end, define\n",
    "\n",
    "$$ \\overline{f}(x) = \\frac{p_j}{h} \\quad \\text{for } x \\in B_j $$\n",
    "\n",
    "where $p_j = \\int_{B_j} f(u) du$ which is a \"histogramized\" version of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.9**.  Let $m = m(n)$ be the number of bins in the histogram $\\hat{f}_n$.  Assume that $m(n) \\rightarrow \\infty$ and $m(n) \\log m / n \\rightarrow 0$ as $n \\rightarrow \\infty$.  Define\n",
    "\n",
    "$$\n",
    "\\ell(x) = \\left( \\text{max} \\left\\{\\sqrt{\\hat{f}_n(x)} - c, 0\\right\\} \\right)^2\n",
    "\\quad \\text{and} \\quad\n",
    "u(x) = \\left(\\sqrt{\\hat{f}_n(x)} + c \\right)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "c = \\frac{z_{\\alpha / (2 m)}}{2} \\sqrt{\\frac{m}{n}}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P} \\left( \\ell(x) \\leq \\overline{f}(x) \\leq u(x) \\text{ for all } x \\right) \\geq 1 - \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  Here is an outline of the proof.  A rigorous proof requires fairly sophisticated tools.  From the central limit theorem, $\\hat{p}_j \\approx N\\left(p_j, p_j (1 - p_j) / n\\right)$.  By the delta method, $\\sqrt{\\hat{p}_j} \\approx N\\left(\\sqrt{p_j}, 1 / (4n)\\right)$.  Moreover, it can be shown that the $\\sqrt{\\hat{p}_j}$'s are approximately independent.  Therefore,\n",
    "\n",
    "$$ 2 \\sqrt{n} \\left( \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right) \\approx Z_j $$\n",
    "\n",
    "where $Z_1, \\dots, Z_m \\sim N(0, 1)$.  Let $A$ be the event that $\\ell(x) \\leq \\overline{f}(x) \\leq u(x)$ for all $x$.  So,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= \\left\\{ \\ell(x) \\leq \\overline{f}(x) \\leq u(x) \\text{ for all } x \\right\\} \\\\\n",
    "&= \\left\\{ \\sqrt{\\ell(x)} \\leq \\sqrt{\\overline{f}(x)} \\leq \\sqrt{u(x)} \\text{ for all } x \\right\\} \\\\\n",
    "&= \\left\\{ \\sqrt{\\hat{f}_n(x)} - c \\leq \\sqrt{\\overline{f}(x)} \\leq \\sqrt{\\hat{f}_n(x)} + c \\text{ for all } x \\right\\} \\\\\n",
    "&= \\left\\{ \\max_x \\left| \\sqrt{\\hat{f}_n(x)} - \\sqrt{\\overline{f}(x)} \\right| \\leq c \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(A^c) &= \\mathbb{P} \\left( \\max_x \\left| \\sqrt{\\hat{f}_n(x)} - \\sqrt{\\overline{f}(x)} \\right| > c\\right)\n",
    "= \\mathbb{P} \\left( \\max_j \\left| \\sqrt{\\frac{\\hat{p}_j}{h}} - \\sqrt{\\frac{p_j}{h}} \\right| > c\\right) \\\\\n",
    "&= \\mathbb{P} \\left( \\max_j \\left| \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right| > c \\sqrt{h} \\right)\n",
    "= \\mathbb{P} \\left( \\max_j 2 \\sqrt{n} \\left| \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right| > 2 c \\sqrt{hn} \\right) \\\\\n",
    "&= \\mathbb{P} \\left( \\max_j 2 \\sqrt{n} \\left| \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right| > z_{\\alpha / (2m)} \\right) \\\\\n",
    "&\\approx \\mathbb{P} \\left( \\max_j \\left| Z_j \\right| > z_{\\alpha / (2m)} \\right)\n",
    " \\leq \\sum_{j=1}^m \\mathbb{P} \\left( \\max_j \\left| Z_j \\right| > z_{\\alpha / (2m)} \\right) \\\\\n",
    "&= \\sum_{j=1}^m \\frac{\\alpha}{m} = \\alpha\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.3 Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms are discontinuous.  **Kernel density estimators** are smoother and they converge faster to the true density than histograms.\n",
    "\n",
    "Let $X_1, \\dots, X_n$ denote the observed data, a sample from $f$.  In this chapter, a **kernel** is defined to be any smooth function $K$ such that $K(x) \\geq 0$, $\\int K(x) dx = 1$, $\\int x K(x) dx = 0$ and $\\sigma_K^2 \\equiv \\int x^2 K(x) dx > 0$.  Two examples of kernels are the **Epanechnikov kernel**\n",
    "\n",
    "$$ K(x) = \\begin{cases}\n",
    "\\frac{3}{4} \\left(1 - x^2 / 5 \\right) / \\sqrt{5} & \\text{if } |x| < \\sqrt{5} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "and the Gaussian (Normal) kernel $K(x) = (2\\pi)^{-1/2} e^{-x^2/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a kernel $K$ and a positive number $h$, called the **bandwidth**, the **kernel density estimator** is defined to be\n",
    "\n",
    "$$ \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K \\left( \\frac{x - X_i}{h} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel density estimator effectively puts a smoothed out lump of mass $1 / n$ over each data point $X_i$.  The bandwidth $h$ controls the amount of smoothing.  When $h$ is close to 0, $\\hat{f}_n$ consists of a set of spikes, one at each data point.  The height of the spikes tends to infinity as $h \\rightarrow 0$.  When $h \\rightarrow 0$, $\\hat{f}_n$ tends to a uniform density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a kernel density estimator, we need to choose a kernel $K$ and a bandwidth $h$.  It can be shown theoretically and empirically that the choice of $K$ is not crucial.  However, the choice of bandwidth $h$ is very important.  As with the histogram, we can make a theoretical statement about how the risk of the estimator depends on the bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.13**.  Under weak assumptions on $f$ and $K$,\n",
    "\n",
    "$$ R(f, \\hat{f}_n) \\approx \\frac{1}{4} \\sigma_K^4 h^4 \\int \\left(f''(x)\\right)^2 dx + \\frac{\\int K^2(x) dx}{nh} $$\n",
    "\n",
    "where $\\sigma_K^2 = \\int x^2 K(x) dx$.  The optimal bandwidth is\n",
    "\n",
    "$$ h^* = \\frac{c_1^{-2/5} c_2^{1/5} c_3^{-1/5}}{n^{1/5}} $$\n",
    "\n",
    "where $c_1 = \\int x^2 K(x) dx$, $c_2 = \\int K(x)^2 dx$ and $c_3 = \\int \\left( f''(x) \\right)^2 dx$.  With this choice of bandwidth,\n",
    "\n",
    "$$ R(f, \\hat{f}_n) \\approx \\frac{c_4}{n^{4/5}} $$\n",
    "\n",
    "for some constant $c_4 > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
