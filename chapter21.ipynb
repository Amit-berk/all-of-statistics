{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Nonparametric Curve Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter we discuss the nonparametric estimation of probability density functions and regression functions, which we refer to a **curve estimation**.\n",
    "\n",
    "In Chapter 8 we saw it is possible to consistently estimate a cumulative distribution function $F$ without making any assumptions about $F$.  If we want to estimate a probability density function $f(x)$ or a regression function $r(x) = \\mathbb{E}(Y | X = x)$ the situation is different.  We cannot estimate these functions consistently without making some smoothness assumptions.  Correspondingly, we will perform some sort of smoothing operation with the data.\n",
    "\n",
    "A simple example of a density estimator is a **histogram**.  To form a histogram estimator of a density $f$, we divide the  real line into disjoint sets called **bins**.  The histogram estimator is a piecewise constant function where the height of the function is proportional to the number of observations in each bin.  The number of bins is an example of a **smoothing parameter**.  If we smooth too much (large bins) we get a highly biased estimator while if smooth too little (small bins) we get a highly variable estimator.  Much of curve estimation is concerned with trying to optimally balance variance and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.1 The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $g$ denote an unknown function and let $\\hat{g}_n$ denote an estimator of $g$.  Bear in mind that $\\hat{g}_n(x)$ is a random function evaluated at a point $x$; $\\hat{g}_n$ is random because it depends on the data.  Indeed, we could be more explicit and write $\\hat{g}_n(x) = h_x(X_1, \\dots, X_n)$ to show that $\\hat{g}_n(x)$ is a function of the data $X_1, \\dots, X_n$ and that the function could be different for each $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a loss function, we will use the **integrated square error (ISE)**:\n",
    "\n",
    "$$ L(g, \\hat{g}_n) = \\int (g(u) - \\hat{g}_n(u))^2 du$$\n",
    "\n",
    "The **risk** or **mean integrated square error (MISE)** is:\n",
    "\n",
    "$$ R(g, \\hat{g}) = \\mathbb{E}\\left(L(g, \\hat{g}) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 21.1**.  The risk can be written as \n",
    "\n",
    "$$ R(g, \\hat{g}) = \\int b^2(x) dx + \\int v(x) dx $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ b(x) = \\mathbb{E}(\\hat{g}_n(x)) - g(x) $$\n",
    "\n",
    "is the bias of $\\hat{g}_n(x)$ at a fixed $x$ and\n",
    "\n",
    "$$ v(x) = \\mathbb{V}(\\hat{g}_n(x)) = \\mathbb{E}\\left( \\hat{g}_n(x) - \\mathbb{E}(\\hat{g}_n(x))^2\\right) $$\n",
    "\n",
    "is the variance of $\\hat{g}_n(x)$ at a fixed $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary,\n",
    "\n",
    "$$ \\text{RISK} = \\text{BIAS}^2 + \\text{VARIANCE} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data is over-smoothed, the bias term is large and the variance is small.  When the data are under-smoothed the opposite is true.  This is called the **bias-variance trade-off**.  Minimizing risk corresponds to balancing bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2 Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_1, \\dots, X_n$ be IID on $[0, 1]$ with density $f$.   The restriction on $[0, 1]$is not crucial; we can always rescale the data to be on this interval.  Let $m$ be an integer and define bins\n",
    "\n",
    "$$ B_1 = \\left[0, \\frac{1}{m} \\right), B_2 = \\left[\\frac{1}{m}, \\frac{2}{m} \\right), \\dots, B_m = \\left[\\frac{m - 1}{m}, 1 \\right] $$\n",
    "\n",
    "Define the **binwidth** $h = 1 / m$, let $v_j$ be the number of observations in $B_j$, let $\\hat{p}_j = v_j / n$ and let $p_j = \\int_{B_j} f(u) du$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **histogram estimator** is defined by \n",
    "\n",
    "$$\n",
    "\\hat{f}_n(x) = \\begin{cases}\n",
    "\\hat{p}_1 / h & x \\in B_1 \\\\\n",
    "\\hat{p}_2 / h & x \\in B_2 \\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "\\hat{p}_m / h & x \\in B_m\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which we can write more succinctly as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{f}_n(x) = \\sum_{j=1}^n \\frac{\\hat{p}_j}{h} I(x \\in B_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the motivation for this estimator, let $p_j = \\int_{B_j} f(u) du$ and note that, for $x \\in B_j$ and $h$ small,\n",
    "\n",
    "$$ \\hat{f}_n(x) = \\frac{\\hat{p}_j}{h} \\approx \\frac{p_j}{h} = \\frac{\\int_{B_j} f(u) du}{h} \\approx \\frac{f(x) h}{h} = f(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and the variance of $\\hat{f}_n(x)$ are given in the following Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.3**.  Consider fixed $x$ and fixed $m$, and let $B_j$ be the bin containing $x$.  Then,\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}(\\hat{f}_n(x)) = \\frac{p_j}{h} \n",
    "\\quad \\text{and} \\quad\n",
    "\\mathbb{V}(\\hat{f}_n(x)) = \\frac{p_j (1 - p_j)}{nh^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the bias-variance tradeoff.  Consider some $x \\in B_j$.  For any other $u \\in B_j$,\n",
    "\n",
    "$$ f(u) \\approx f(x) + (u - x) f'(x) $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "p_j = \\int_{B_j} f(u) du &\\approx \\int_{B_j} (f(x) + (u - x) f'(x)) du \\\\\n",
    "&= f(x) h + h f'(x) \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the bias $b(x)$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b(x) &= \\mathbb{E}(\\hat{f}_n(x)) - f(x) = \\frac{p_j}{h} - f(x) \\\\\n",
    "&\\approx \\frac{f(x) h + h f'(x) \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)}{h} - f(x) \\\\\n",
    "&= f'(x) \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $\\overline{x}_j$ is the center of the bin, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int_{B_j} b^2(x) dx &= \\int_{B_j} (f'(x))^2 \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)^2 dx \\\\\n",
    "&\\approx (f'(\\overline{x}_j))^2 \\int_{B_j} \\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)^2 dx \\\\\n",
    "&= (f'(\\overline{x}_j))^2 \\frac{h^3}{12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int_0^1 b^2(x) dx &= \\sum_{j=1}^m \\int_{B_j} b^2(x) dx \\approx \\sum_{j=1}^m (f'(\\overline{x}_j))^2 \\frac{h^3}{12} \\\\\n",
    "&= \\frac{h^2}{12} \\sum_{j=1}^m h(f'(\\overline{x}_j))^2 \\approx \\frac{h^2}{12} \\int_0^1 h(f'(\\overline{x}_j))^2 dx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that this increases as a function of $h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the variance.  For $h$ small, $1 - p_j \\approx 1$, so\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v(x) &\\approx \\frac{p_j}{nh^2}\\\\\n",
    "&= \\frac{f(x)h + h f'(x)\\left(h \\left(j - \\frac{1}{2} \\right) - x \\right)}{nh^2} \\\\\n",
    "&\\approx \\frac{f(x)}{nh}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "when we keep the dominant term.  So\n",
    "\n",
    "$$\n",
    "\\int_0^1 v(x) dx \\approx \\frac{1}{nh}\n",
    "$$\n",
    "\n",
    "Note that this decreases with $h$.  Putting this all together, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.4**.  Suppose that $\\int (f'(u))^2 du < \\infty$.  Then\n",
    "\n",
    "$$ R(\\hat{f}_n, f) \\approx \\frac{h^2}{12} \\int (f'(u))^2 du + \\frac{1}{nh}$$\n",
    "\n",
    "The value $h^*$ that minimizes this expression is\n",
    "\n",
    "$$ h^* = \\frac{1}{n^{1/3}} \\left( \\frac{6}{\\int (f'(u))^2 du} \\right)^{1/3}$$\n",
    "\n",
    "With this choice of binwidth,\n",
    "\n",
    "$$ R(\\hat{f}_n, f) \\approx \\frac{C}{n^{2/3}} $$\n",
    "\n",
    "where $C = (3/4)^{2/3} \\left( \\int (f'(u))^2 du \\right)^{1/3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem 21.4 is quite revealing.  We see that with an optimally chosen bandwidth, the MISE decreases to 0 at rate $n^{-2/3}$.  By comparison, most parametric estimators converge at rate $n^{-1}$.  The formula for optimal binwidth $h^*$ is of theoretical interest but it is not useful in practice since it depends on the unknown function $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical way of choosing the binwidth is to estimate the risk function and minimize over $h$.  Recall that the loss function, which we now write as a function of $h$, is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h) &= \\int \\left( \\hat{f}_n(x) - f(x) \\right)^2 dx \\\\\n",
    "&= \\int \\hat{f}_n^2(x) dx - 2 \\int \\hat{f}_n(x) f(x) dx + \\int f^2(x) dx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last term does not depend on the binwidth $h$ so minimizing the risk is equivalent to minimizing the expected value of\n",
    "\n",
    "$$ J(h) = \\int \\hat{f}_n^2(x) dx - 2 \\int \\hat{f}_n(x) f(x) dx $$\n",
    "\n",
    "We shall refer to $\\mathbb{E}(J(h))$ as the risk, although it differs from the true risk by the constant term $\\int f^2(x) dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cross-validation estimator of  risk** is\n",
    "\n",
    "$$ \\hat{J}(h) = \\int \\left( \\hat{f}_n(x) \\right)^2 dx - \\frac{2}{n} \\sum_{i=1}^n \\hat{f}_{(-i)}(X_i)$$\n",
    "\n",
    "where $\\hat{f}_{(-i)}$ is the histogram estimator obtained after removing the $i$-th observation.  We refer to $\\hat{J}(h)$ as the cross-validation score or estimated risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.6**.  The cross-validation estimator is nearly unbiased:\n",
    "\n",
    "$$ \\mathbb{E}(\\hat{J}(x)) \\approx \\mathbb{E}(J(x)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we need to recompute the histogram $n$ times to compute $\\hat{J}(x)$.  Moreover, this has to be done for all values of $h$.  Fortunately, there is a shortcut formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.7**.  The following identity holds:\n",
    "\n",
    "$$ \\hat{J}(h) = \\frac{2}{(n - 1)h} + \\frac{n+1}{n-1} \\sum_{j=1}^m \\hat{p}_j^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want some sort of confidence set for $f$.  Suppose $\\hat{f}_n$ is a histogram with $m$ bins and binwidth $h = 1 / m$.  We cannot realistically make confidence statements about the fine details of true density $f$.  Instead, we make confidence statements about $f$ at the resolution of the histogram.  To this end, define\n",
    "\n",
    "$$ \\overline{f}(x) = \\frac{p_j}{h} \\quad \\text{for } x \\in B_j $$\n",
    "\n",
    "where $p_j = \\int_{B_j} f(u) du$ which is a \"histogramized\" version of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.9**.  Let $m = m(n)$ be the number of bins in the histogram $\\hat{f}_n$.  Assume that $m(n) \\rightarrow \\infty$ and $m(n) \\log m / n \\rightarrow 0$ as $n \\rightarrow \\infty$.  Define\n",
    "\n",
    "$$\n",
    "\\ell(x) = \\left( \\text{max} \\left\\{\\sqrt{\\hat{f}_n(x)} - c, 0\\right\\} \\right)^2\n",
    "\\quad \\text{and} \\quad\n",
    "u(x) = \\left(\\sqrt{\\hat{f}_n(x)} + c \\right)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "c = \\frac{z_{\\alpha / (2 m)}}{2} \\sqrt{\\frac{m}{n}}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P} \\left( \\ell(x) \\leq \\overline{f}(x) \\leq u(x) \\text{ for all } x \\right) \\geq 1 - \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  Here is an outline of the proof.  A rigorous proof requires fairly sophisticated tools.  From the central limit theorem, $\\hat{p}_j \\approx N\\left(p_j, p_j (1 - p_j) / n\\right)$.  By the delta method, $\\sqrt{\\hat{p}_j} \\approx N\\left(\\sqrt{p_j}, 1 / (4n)\\right)$.  Moreover, it can be shown that the $\\sqrt{\\hat{p}_j}$'s are approximately independent.  Therefore,\n",
    "\n",
    "$$ 2 \\sqrt{n} \\left( \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right) \\approx Z_j $$\n",
    "\n",
    "where $Z_1, \\dots, Z_m \\sim N(0, 1)$.  Let $A$ be the event that $\\ell(x) \\leq \\overline{f}(x) \\leq u(x)$ for all $x$.  So,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= \\left\\{ \\ell(x) \\leq \\overline{f}(x) \\leq u(x) \\text{ for all } x \\right\\} \\\\\n",
    "&= \\left\\{ \\sqrt{\\ell(x)} \\leq \\sqrt{\\overline{f}(x)} \\leq \\sqrt{u(x)} \\text{ for all } x \\right\\} \\\\\n",
    "&= \\left\\{ \\sqrt{\\hat{f}_n(x)} - c \\leq \\sqrt{\\overline{f}(x)} \\leq \\sqrt{\\hat{f}_n(x)} + c \\text{ for all } x \\right\\} \\\\\n",
    "&= \\left\\{ \\max_x \\left| \\sqrt{\\hat{f}_n(x)} - \\sqrt{\\overline{f}(x)} \\right| \\leq c \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(A^c) &= \\mathbb{P} \\left( \\max_x \\left| \\sqrt{\\hat{f}_n(x)} - \\sqrt{\\overline{f}(x)} \\right| > c\\right)\n",
    "= \\mathbb{P} \\left( \\max_j \\left| \\sqrt{\\frac{\\hat{p}_j}{h}} - \\sqrt{\\frac{p_j}{h}} \\right| > c\\right) \\\\\n",
    "&= \\mathbb{P} \\left( \\max_j \\left| \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right| > c \\sqrt{h} \\right)\n",
    "= \\mathbb{P} \\left( \\max_j 2 \\sqrt{n} \\left| \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right| > 2 c \\sqrt{hn} \\right) \\\\\n",
    "&= \\mathbb{P} \\left( \\max_j 2 \\sqrt{n} \\left| \\sqrt{\\hat{p}_j} - \\sqrt{p_j} \\right| > z_{\\alpha / (2m)} \\right) \\\\\n",
    "&\\approx \\mathbb{P} \\left( \\max_j \\left| Z_j \\right| > z_{\\alpha / (2m)} \\right)\n",
    " \\leq \\sum_{j=1}^m \\mathbb{P} \\left( \\max_j \\left| Z_j \\right| > z_{\\alpha / (2m)} \\right) \\\\\n",
    "&= \\sum_{j=1}^m \\frac{\\alpha}{m} = \\alpha\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.3 Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms are discontinuous.  **Kernel density estimators** are smoother and they converge faster to the true density than histograms.\n",
    "\n",
    "Let $X_1, \\dots, X_n$ denote the observed data, a sample from $f$.  In this chapter, a **kernel** is defined to be any smooth function $K$ such that $K(x) \\geq 0$, $\\int K(x) dx = 1$, $\\int x K(x) dx = 0$ and $\\sigma_K^2 \\equiv \\int x^2 K(x) dx > 0$.  Two examples of kernels are the **Epanechnikov kernel**\n",
    "\n",
    "$$ K(x) = \\begin{cases}\n",
    "\\frac{3}{4} \\left(1 - x^2 / 5 \\right) / \\sqrt{5} & \\text{if } |x| < \\sqrt{5} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "and the Gaussian (Normal) kernel $K(x) = (2\\pi)^{-1/2} e^{-x^2/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a kernel $K$ and a positive number $h$, called the **bandwidth**, the **kernel density estimator** is defined to be\n",
    "\n",
    "$$ \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K \\left( \\frac{x - X_i}{h} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel density estimator effectively puts a smoothed out lump of mass $1 / n$ over each data point $X_i$.  The bandwidth $h$ controls the amount of smoothing.  When $h$ is close to 0, $\\hat{f}_n$ consists of a set of spikes, one at each data point.  The height of the spikes tends to infinity as $h \\rightarrow 0$.  When $h \\rightarrow 0$, $\\hat{f}_n$ tends to a uniform density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a kernel density estimator, we need to choose a kernel $K$ and a bandwidth $h$.  It can be shown theoretically and empirically that the choice of $K$ is not crucial.  However, the choice of bandwidth $h$ is very important.  As with the histogram, we can make a theoretical statement about how the risk of the estimator depends on the bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.13**.  Under weak assumptions on $f$ and $K$,\n",
    "\n",
    "$$ R(f, \\hat{f}_n) \\approx \\frac{1}{4} \\sigma_K^4 h^4 \\int \\left(f''(x)\\right)^2 dx + \\frac{\\int K^2(x) dx}{nh} $$\n",
    "\n",
    "where $\\sigma_K^2 = \\int x^2 K(x) dx$.  The optimal bandwidth is\n",
    "\n",
    "$$ h^* = \\frac{c_1^{-2/5} c_2^{1/5} c_3^{-1/5}}{n^{1/5}} $$\n",
    "\n",
    "where $c_1 = \\int x^2 K(x) dx$, $c_2 = \\int K(x)^2 dx$ and $c_3 = \\int \\left( f''(x) \\right)^2 dx$.  With this choice of bandwidth,\n",
    "\n",
    "$$ R(f, \\hat{f}_n) \\approx \\frac{c_4}{n^{4/5}} $$\n",
    "\n",
    "for some constant $c_4 > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**.  Let\n",
    "\n",
    "$$ K_h(x, X) = \\frac{1}{h} K\\left( \\frac{x - X}{h} \\right)\n",
    "\\quad \\text{and} \\quad\n",
    "\\hat{f}_n(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x, X_i)\n",
    "$$\n",
    "\n",
    "Thus, $$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat{f}_n(x)] &= \\mathbb{E}[K_h(x, X)] \\\\\n",
    "\\mathbb{V}[\\hat{f}_n(x)] &= \\frac{1}{n} \\mathbb{V}[K_h(x, X)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[K_h(x, X)] &= \\int \\frac{1}{h} K\\left( \\frac{x - t}{h} \\right) f(t) dt \\\\\n",
    "&= \\int K(u) f(x - hu) du \\\\\n",
    "&= \\int K(u) \\left[ f(x) - hu f'(x) + \\frac{1}{2} h^2u^2 f''(x) + \\dots \\right] du \\\\\n",
    "&= f(x) + \\frac{1}{2} h^2 f''(x) \\int u^2 K(u) du + \\dots\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since $\\int K(u) du = 1$ and $\\int u K(u) du = 0$.  The bias is\n",
    "\n",
    "$$ \\mathbb{E}[K_h(x, X)] - f(x) \\approx \\frac{1}{2} \\sigma_K^2 h^2 f''(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By a similar calculation,\n",
    "\n",
    "$$ \\mathbb{V}[\\hat{f}_n(x)] \\approx \\frac{f(x) \\int K^2(u) du}{n h_n} $$\n",
    "\n",
    "The second result follows from integrating the bias squared plus variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that kernel estimators converge at rate $n^{-4/5}$ while histograms converge at rate $n^{-2/3}$.  It can be shown that, under weak assumptions, there does not exist a nonparametric estimator that converges faster than $n^{-4/5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for $h^*$ depends on the unknown density $f$ which makes the result of little practical use.  As with the histograms, we shall use cross-validation to find a bandwidth.  Thus, we estimate the risk (up to a constant) by\n",
    "\n",
    "$$ \\hat{J}(h) = \\int \\hat{f}^2(x) dx - 2 \\frac{1}{n} \\sum_{i=1}^n \\hat{f}_{-i}(X_i) $$\n",
    "\n",
    "where $\\hat{f}_{-i}$ is the kernel density estimator after omitting the $i$-th observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.14**.  For any $h > 0$,\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\hat{J}(h) \\right] = \\mathbb{E} \\left[ J(h) \\right] $$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$ \\hat{J}(h) \\approx \\frac{1}{hn^2}\\sum_{i, j} K^* \\left( \\frac{X_i - X_j}{h} \\right) + \\frac{2}{nh} K(0) $$\n",
    "\n",
    "where $K^*(x) = K^{(2)}(x) - 2 K(x)$ and $K^{(2)}(z) = \\int K(z - y) K(y) dy$.  In particular, if $K$ is a $N(0, 1)$ Gaussian kernel then $K^{(2)}(z)$ is the $N(0, 2)$ density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then choose the bandwidth $h_n$ that maximizes $\\hat{J}(h)$.  A justification for this method is given by the following remarkable theorem due to Stone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.15 (Stone's Theorem)**.  Suppose that $f$ is bounded.  Let $\\hat{f}_h$ denote the kernel estimator with bandwidth $h$ and let $h_n$ denote the bandwidth chosen by cross-validation.  Then,\n",
    "\n",
    "$$ \\frac{\\int \\left( f(x) - \\hat{f}_{h_n}(x)\\right)^2 dx}{\\inf_h \\int \\left( f(x) - \\hat{f}_h(x) \\right)^2 dx} \\xrightarrow{\\text{P}} 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct confidence bands, we use something similar to histograms although the details are more complicated.  The version described here is from Chaudhuri and Marron (1999)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence Band for Kernel Density Estimator**\n",
    "\n",
    "1. Choose an evenly spaced grid of points $\\mathcal{V} = \\{ v_1, \\dots, v_N \\}$.  For every $v \\in \\mathcal{V}$, define\n",
    "\n",
    "$$ Y_i(v) = \\frac{1}{h} K \\left( \\frac{v - X_i}{h} \\right) $$\n",
    "\n",
    "Note that $\\hat{f}_n(v) = \\overline{Y}_n(v)$, the average of the $Y_i(v)$'s.\n",
    "\n",
    "2. Define\n",
    "\n",
    "$$ \\text{se}(v) = \\frac{s(v)}{\\sqrt{n}} $$\n",
    "\n",
    "where $s^2(v) = (n - 1)^{-1} \\sum_{i=1}^n ( Y_i(v) - \\overline{Y}_n(v) )^2$.\n",
    "\n",
    "3. Compute the effective sample size\n",
    "\n",
    "$$ \\text{ESS}(v) = \\frac{\\sum_{i=1}^n K\\left( \\frac{v - X_i}{h} \\right)}{K(0)} $$\n",
    "\n",
    "4. Let $\\mathcal{V}^* = \\left\\{ v \\in \\mathcal{V} : \\text{ESS}(v) \\geq 5 \\right\\}$.  Now define the number of independent blocks $m$ by\n",
    "\n",
    "$$ \\frac{1}{m} = \\frac{\\overline{\\text{ESS}}}{n} $$\n",
    "\n",
    "where $\\overline{\\text{ESS}}$ is the average of $\\text{ESS}$ over $\\mathcal{V}^*$.\n",
    "\n",
    "5.  Let \n",
    "\n",
    "$$ q = \\Phi^{-1} \\left( \\frac{1 + (1 + \\alpha)^{1/m}}{2} \\right) $$\n",
    "\n",
    "and define\n",
    "\n",
    "$$ \\ell(v) = \\text{max} \\left\\{ \\hat{f}_n(v) - q \\cdot \\text{se}(v), 0 \\right\\}\n",
    "\\quad \\text{and} \\quad\n",
    "u(v) = \\hat{f}_n(v) + q \\cdot \\text{se}(v)$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ \\mathbb{P} \\Big\\{ \\ell(v) \\leq f(v) \\leq u(v) \\text{ for all } v \\Big\\} \\approx 1 - \\alpha $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that the data $X_i = \\{ X_{i1}, \\dots, X_{id} \\}$ are $d$-dimensional.  The kernel estimator can easily be generalized to $d$ dimensions.  Let $h = (h_1, \\dots, h_d)$ be a vector of bandwidths and define\n",
    "\n",
    "$$ \\hat{f}_n(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - X_i) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ K_h(x - X_i) = \\frac{1}{n \\prod_{j=1}^d h_j} \\left\\{ \\prod_{j=1}^d K \\left( \\frac{x_i - X_{ij}}{h_j} \\right) \\right\\} $$\n",
    "\n",
    "For simplicity, we might take $h_j = s_j h$ where $s_j$ is the standard deviation of the $j$-th variable.  There is now only a single bandwidth to choose.  Using calculations like the ones in the one-dimensional case, the risk is given by\n",
    "\n",
    "$$\n",
    "R(f, \\hat{f}_n) \n",
    "\\approx \\frac{1}{4} \\sigma_K^4 \\left[ \\sum_{j=1}^d h_j^4 \\int f_{jj}^2(x) dx + \\sum_{j \\neq k} h_j^2 h_k^2 \\int f_{jj}(x) f_{kk}(x) dx \\right] + \\frac{\\left( \\int K^2(x) dx\\right)^d}{n \\prod_{j=1}^d h_j}\n",
    "$$\n",
    "\n",
    "where $f_{jj}$ is the second partial derivative of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal bandwidth satisfies $h_i \\approx c_1 n^{-1/(4 + d)}$ leading to a risk of order $n^{-4/(4+d)}$.  From this fact, we see that the risk increases quickly with dimension, a problem usually called the **curse of dimensionality**.  To get a sense of how serious this problem is, consider the following table from Silverman (1986) which shows the sample size required to ensure a relative mean squared error less than 0.1 at 0 when the density is a multivariate normal and the optimal bandwidth is selected.\n",
    "\n",
    "| Dimension | Sample Size |\n",
    "|-----------|-------------|\n",
    "| 1         | 4           |\n",
    "| 2         | 19          |\n",
    "| 3         | 67          |\n",
    "| 4         | 223         |\n",
    "| 5         | 768         |\n",
    "| 6         | 2790        |\n",
    "| 7         | 10700       |\n",
    "| 8         | 43700       |\n",
    "| 9         | 187000      |\n",
    "| 10        | 842000      |\n",
    "\n",
    "This is bad news indeed.  it says that having 842,000 observations in a ten dimensional problem is really like having 4 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.4 Nonparametric Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider pairs of points $(x_1, Y_1), \\dots, (x_n, Y_n)$ related by\n",
    "\n",
    "$$ Y_i = r(x_i) + \\epsilon_i $$\n",
    "\n",
    "where $\\mathbb{E}(\\epsilon_i) = 0$ and we are treating the $x_i$'s as fixed.  In nonparametric regression, we want to estimate the regression function $r(x) = \\mathbb{E}(Y | X = x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many nonparametric regression estimators.  Most involve estimating $r(x)$ by taking some sort of weighted average of the $Y_i$'s, giving higher weight to those points near $x$.  In particular, the **Nadaraya-Watson kernel estimator** is defined by\n",
    "\n",
    "$$ \\hat{r}(x) = \\sum_{i=1}^n w_i(x) Y_i$$\n",
    "\n",
    "where the weights $w_i(x)$ are given by\n",
    "\n",
    "$$ w_i(x) = \\frac{K\\left(\\frac{x - x_i}{h}\\right)}{\\sum_{j=1}^n \\left(\\frac{x - x_j}{h}\\right) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form of this estimators comes from first estimating the joint density $f(x, y)$ using kernel density estimation and then inserting this into:\n",
    "\n",
    "$$ r(x) = \\mathbb{E}(Y | X = x) = \\int y f(y | x) dy = \\frac{\\int y f(x, y) dy}{\\int f(x, y) dy} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 21.19**.  Suppose that $\\mathbb{V}(\\epsilon_i) = \\sigma^2$.  The risk of the Nadaraya-Watson kernel estimator is\n",
    "\n",
    "$$ R(\\hat{r}_n, r) \\approx \\frac{h^4}{4} \n",
    "\\left( \\int x^2 K^2(x) dx\\right)^4\n",
    "\\int \\left( r''(x) + 2 r'(x) \\frac{f'(x)}{f(x)} \\right)^2 dx\n",
    "+ \\int \\frac{\\sigma^2 \\int K^2(x) dx}{nh f(x)} dx\n",
    "$$\n",
    "\n",
    "The optimal bandwidth decreases at rate $n^{-1/5}$ and with this choice the risk decreases at rate $n^{-4/5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, to choose the bandwidth $h$ we minimize the cross-validation score\n",
    "\n",
    "$$ \\hat{J}(h) = \\sum_{i=1}^n (Y_i - \\hat{r}_{-i}(x_i))^2$$ \n",
    "\n",
    "where $\\hat{r}_{-i}$ is the estimator we get by omitting the $i$-th variable.  An approximation to $\\hat{J}$ is given by\n",
    "\n",
    "$$ \\hat{J}(h) \\approx \\sum_{i=1}^n (Y_i - \\hat{r}(x_i))^2 \\left( 1 - \\frac{K(0)}{\\sum_{j=1}^n K \\left( \\frac{x_i - x_j}{h} \\right)} \\right)^{-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure for finding confidence bands is similar to that for density estimation.  However, we first need to estimate $\\sigma^2$.  Suppose that the $x_i$'s are ordered.  Assuming $r(x)$ is smooth, we have $r(x_{i+1}) - r(x) \\approx 0$ and hence\n",
    "\n",
    "$$Y_{i+1} - Y_i = \\left[ r(x_{i+1} + \\epsilon_{i+1} \\right] - \\left[ r(x_{i} + \\epsilon_{i} \\right] \\approx \\epsilon_{i+1} - \\epsilon_i $$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$ \\mathbb{V}(Y_{i+1} - Y_i) \\approx \\mathbb{V}(\\epsilon_{i+1} - \\epsilon{i}) = \\mathbb{V}(\\epsilon_{i+1}) + \\mathbb{V}(\\epsilon_i) = 2\\sigma^2$$\n",
    "\n",
    "We can thus use the average of the differences of consecutive $Y_i$'s to estimate $\\sigma^2$.  Define\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{2(n - 1)} \\sum_{i=1}^{n-1} (Y_{i+1} - Y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence Bands for Kernel Regression**\n",
    "\n",
    "Follow the same procedure as for kernel density estimation, except change the defition of the standard error $\\text{se}(v)$ to\n",
    "\n",
    "$$ \\text{se}(v) = \\hat{\\sigma} \\sqrt{\\sum_{i=1}^n w^2(x_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extension to multiple regressors $X = (X_1, \\dots, X_p)$ is straightforward.  As with kernel density estimation we just replace the kernel with a multivariate kernel.  However, the same caveats about the curse of dimensionality apply.\n",
    "\n",
    "In some cases, we may consider putting some restrictions on the regression function which will then reduce the curse of dimensionality.  For example, **additive regression** is based on the model\n",
    "\n",
    "$$ Y = \\sum_{j=1}^p r_j(X_j) + \\epsilon$$\n",
    "\n",
    "Now we only need to fit $p$ one-dimensional functions.  The model can be enriched by adding various interactions, for example\n",
    "\n",
    "$$ Y = \\sum_{j=1}^p r_j(X_j) + \\sum_{j < k} r_{jk}(X_j X_k) + \\epsilon$$\n",
    "\n",
    "Additive models are usually fit by an algorithm called **backfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backfitting**\n",
    "\n",
    "1. Initialize $r_1(x_1), \\dots, r_p(x_p)$.\n",
    "2. For $j = 1, \\dots, p$:\n",
    "  - Let $\\epsilon_i = Y_i - \\sum_{s \\neq j} r_s(x_i)$   \n",
    "  - Let $r_j$ be the function estimate obtained by regressing the $\\epsilon_i$'s on the $j$-th covariate.\n",
    "3.  If converged, stop.  Otherwise, go back to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive models have the advantage that they avoid the curse of dimensionality and they can be fit quickly but they have one disadvantage:  the model is not fully nonparametric.  In other words, the true regression function $r(x)$ may not be of the fitted form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.5  Appendix:  Confidence Sets and Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence bands we computed are for the smoothed function, rather than the density function or regression function.  For example, the confidence band for a kernel density estimate with bandwidth $h$ is a band for the function one gets by smoothing the true function with a kernel with the same bandwidth.  Getting a confidence band for the true function is complicated for reasons we now explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review the parametric case.  When estimating a scalar quantity $\\theta$ with an estimator $\\hat{\\theta}$, the usual confidence interval is of the form $\\hat{\\theta} \\pm z_{\\alpha / 2} s_n$, where $\\hat{\\theta}$ is the maximum likelihood estimator and $s_n = \\sqrt{\\mathbb{V}(\\hat{\\theta})}$ is the estimated standard error of the estimator.  Under standard regularity conditions, $\\hat{\\theta} \\approx N(\\theta, s_n)$ and\n",
    "\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\mathbb{P} \\left( \\hat{\\theta} - z_{\\alpha / 2} s_n \\leq \\theta \\leq \\hat{\\theta} + z_{\\alpha / 2} s_n \\right) = 1 - \\alpha $$\n",
    "\n",
    "But let's take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $b_n = \\mathbb{E}(\\hat{\\theta}_n) - \\theta$.  This is a bias term we usually ignore in large sample calculations, but let's keep track of it; $\\hat{\\theta} \\approx N(\\theta + b_n, s_n)$.  The coverage of the usual confidence interval is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}\\left(\\hat{\\theta} - z_{\\alpha / 2} s_n \\leq \\theta \\leq \\hat{\\theta} + z_{\\alpha / 2} s_n \\right)\n",
    "&= \\mathbb{P}\\left(- z_{\\alpha / 2}  \\leq \\frac{\\theta - \\hat{\\theta}}{s_n} \\leq z_{\\alpha / 2}\\right) \\\\\n",
    "&= \\mathbb{P}\\left(- z_{\\alpha / 2}  \\leq \\frac{N(b_n, s_n)}{s_n} \\leq z_{\\alpha / 2}\\right) \\\\\n",
    "&= \\mathbb{P}\\left(- z_{\\alpha / 2}  \\leq N\\left(\\frac{b_n}{s_n}, 1\\right) \\leq z_{\\alpha / 2}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In a well-behaved parametric model, $s_n$ is of size $n^{-1/2}$ and $b_n$ is of size $n^{-1}$.  Hence, $b_n / s_n \\rightarrow 0$ and the last probability statement becomes $\\mathbb{P}\\left(- z_{\\alpha / 2}  \\leq N\\left(0, 1\\right) \\leq z_{\\alpha / 2}\\right) = 1 - \\alpha$.  What makes parametric confidence intervals have the right coverage is the fact that $b_n / s_n \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation is more complicated for kernel methods.  Consider estimating a density $f(x)$ at a single point $x$ with a kernel density estimator.  Since $\\hat{f}(x)$ is a sum of iid random variables, the central limit theorem implies that\n",
    "\n",
    "$$ \\hat{f}(x) \\approx N \\left( f(x) + b_n(x), \\frac{c_2 f(x)}{nh} \\right) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ b_n(x) = \\frac{1}{2} h^2 f''(x) c_1 $$\n",
    "\n",
    "is the bias, $c_1 = \\int x^2 K(x) dx$, and $c_2 = \\int K^2(x) dx$.  The estimated standard error is\n",
    "\n",
    "$$ s_n(x) = \\left\\{ \\frac{c_2 \\hat{f}(x)}{nh} \\right\\}^{1/2} $$\n",
    "\n",
    "Suppose we use the usual interval $\\hat{f}(x) \\pm z_{\\alpha/2} s_n(x)$.  Arguing as before, the coverage is approximately\n",
    "\n",
    "$$ \\mathbb{P}\\left(-z_{\\alpha/2} \\leq N\\left(\\frac{b_n(x)}{s_n(x)}, 1\\right) \\leq z_{\\alpha/2} \\right) $$\n",
    "\n",
    "The optimal bandwidth is of the form $h^* = cn^{-1/5}$ for some constant $c$.  if we plug $h = cn^{-1/5}$ into the definitions of $b_n(x)$ and $s_n(x)$ we see that $b_n(x) / s_n(x)$ does not tend to 0.  Thus, the confidence interval will have coverage less than $1 - \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 21.7.1**.  Let $X_1, \\dots, X_n \\sim f$ and let $\\hat{f}_n$ be the kernel density estimator using the boxcar kernel:\n",
    "\n",
    "$$ K(x) = \\begin{cases}\n",
    "1 & \\text{if } -\\frac{1}{2} < x < \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**(a)**  Show that\n",
    "\n",
    "$$\\mathbb{E}(\\hat{f}(x)) = \\frac{1}{h} \\int_{x-(h/2)}^{x+(h/2)} f(y) dy$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mathbb{V}(\\hat{f}(x)) = \\frac{1}{nh^2} \\left[ \\int_{x-(h/2)}^{x+(h/2)} f(y) dy  - \\left( \\int_{x-(h/2)}^{x+(h/2)} f(y) dy \\right)^2\\right]$$\n",
    "\n",
    "**(b)** Show that if $h \\rightarrow 0$ and $nh \\rightarrow \\infty$ as $n \\rightarrow \\infty$ then $f_n(x) \\xrightarrow{\\text{P}} f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
